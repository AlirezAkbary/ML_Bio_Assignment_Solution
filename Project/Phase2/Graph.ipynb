{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6bqYFVjcQ1P",
        "colab_type": "text"
      },
      "source": [
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "فاز دوم پروژه درس یادگیری ماشین برای بیوانفورماتیک\n",
        "</font></h1><p></p>\n",
        "علیرضا اکبری\n",
        "\n",
        "۹۵۱۰۵۳۷۹\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT1SjS0Occ8C",
        "colab_type": "text"
      },
      "source": [
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "۱.\n",
        "</font></h1><p></p>\n",
        "نقاط قوت: در این مسئله از چالش‌هایی که در فاز قبلی پروژه نیز با آن روبرو بودیم، استخراج و بدست آوردن فیچرهای مناسب و معنایی است که در حل مسئله کارایی لازم را داشتند. در فاز قبلی با \n",
        "feature engineering\n",
        "سعی کردیم فیچرها را بسازیم . اما در این کار هیچ تضمینی وجود ندارد که لزوما فیچرهای مناسب و معنایی ساخته شوند و همچنین تعداد آن‌ها برای رگرشن و دسته‌بندی کافی باشد.\n",
        "\n",
        "اما در \n",
        "DeepDTA\n",
        "این کار، یعنی استخراج فیچرهای مناسب، به خود مدل سپرده شده است تا با توجه به \n",
        "loss\n",
        "مناسب‌ترین \n",
        "representation\n",
        "ای که می‌توان برای دارو و پروتئین متصور بود پیدا شود. یعنی به مدل خود دارو و پروتئین را می‌دهیم و نمایش خام آن‌ها را با یک امبدینگ تبدیل به نمایش جدید می‌کنیم و حال از این نمایش دنبال فیچرهای مناسب برای دارو و پروتئین می‌گردیم.\n",
        " یعنی در دو مسیر مجزا به وسیله شبکه عصبی فیچر مناسب هر کدام را پیدا می‌کنیم. و حالا با این فیچرهای استخراج شده، مسئله رگرشن را با یک شبکه عصبی \n",
        "dense\n",
        "حل می‌کنیم.\n",
        "\n",
        "نقطه قوت دیگر این است که همان‌طور که در مقاله گفته شده است، جنس این مسئله، یک مسئله پیوسته است. یعنی مقدار \n",
        "affinity\n",
        "یک عدد پیوسته است که پیش‌بینی آن به صورت یک عدد اطلاعات بیشتری برای ما می‌تواند داشته باشد تا این کار صرفا دسته‌بندی انجام دهید. در نتیجه این مدل یک عدد را پیش‌بینی کرده است به جای دسته مناسب.\n",
        "\n",
        "از نقاط ضعف می‌توان به مدلی که قرار است فیچرهای دارو و پروتئین را استخراج کند اشاره کنیم. در \n",
        "DeepDTA\n",
        "از چند لایه کانولوشن یک‌بعدی استفاده شده است که از نمایش امبدینگ آن‌ها فیچر استخراج کند. می‌دانیم که کانولوشن در استخراج فیچرهای محلی بیشترین قابلیت را دارد. حال آنکه ورودی‌های ما که دارو و پروتئین هستند ساختار خاص‌تری دارند که لزوما ویژگی‌های محلی فیچرهای کافی‌ای نخواهند بود. یک دارو یک ساختار شیمیایی دارد که که در یک ساختار شیمیایی علاوه بر خواص محلی، ساختار کلی دارو که به شکل یک گراف قابل نمایش است مهم‌تر خواهد بود. همچنین پروتئین یک توالی است که می‌دانیم لایه مناسب برای بررسی یک داده باتوالی، لایه \n",
        "rnn\n",
        "مناسب‌تر است.\n",
        "\n",
        "نکته دیگر این است که توالی‌های دارو و پروتئین طول‌های مختلفی در داده‌ها دارند که در این مقاله تصمیم گرفته شده است که به یک طول ثابت اصلاح شوند. این کار با توجه به آنکه منجر می‌شود بخشی از دارو یا پروتئین حذف شود، می‌تواند تاثیر به سزایی در استخراج فیچر داشته باشد. \n",
        "\n",
        "همچنین در مورد دارو علاوه بر آنکه ساختار کلی آن در نظر گرفته نمی‌شود، علاوه بر آن، ویژگی‌های شیمیایی اتم‌ها و ... که در مولکول قرار دارد می‌تواند به مدل داده شود که اینجا اینگونه نیست.\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefUkou2pvyI",
        "colab_type": "text"
      },
      "source": [
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "۲.\n",
        "</font></h1><p></p>\n",
        "همانطور که گفتیم که یک دارو یک ساختار شیمیایی دارد که به صورت گراف می‌توان آنرا نمایش داد. لایه مناسبی که می‌تواند یک ساختار گراف را یاد بگیرید\n",
        "graph neural network\n",
        "ها هستند که نشان داده شده است نتایج بسیار خوبی روی یادگیری انواع گراف‌ها مانند \n",
        "syntax tree\n",
        "یا مولکول‌های شیمیایی کسب می‌کنند. در نتیجه در مدل پیشنهادی می‌خواهیم در بخش دارو، کانولوشن را با لایه‌های \n",
        "gnn\n",
        "جایگزین گنیم. \n",
        "یک \n",
        "gnn\n",
        "را به طور کلی می‌توان حالت کلی‌تری از \n",
        "rnn \n",
        "در نظر گرفت که هر \n",
        "node\n",
        " یک سری فیچر دارد و در هر مرحله پیامی را از همسایه‌های خود دریافت می‌کند و با \n",
        " aggregate\n",
        " کردن پیام‌های جدید و با توجه به \n",
        " state\n",
        "  قبلی خود\n",
        " ،\n",
        "  state \n",
        "جدید خود را آپدیت می‌کند. با تکرار این کار در مراحل بالا ، به صورت \n",
        "iterative\n",
        "، \n",
        "نودها به مرور زمان از تمام نودهای موجود در گراف خبردار می‌شوند و در نتیجه رو به سمت یادگیری کل ساختار گراف می‌آورند.\n",
        " در جست‌وجوهایی که انجام شد، دو مقاله در همین زمینه با عنوان \n",
        "GraphDTA\n",
        "و\n",
        "DeepS\n",
        "یافت شد که ایده مشابه را برای یادگیری ساختار دارو به کار گرفته‌اند. برای این بخش مقاله \n",
        "GraphDTA\n",
        "را مطالعه و استفاده کردیم. در این مقاله از ۴ \n",
        "variant\n",
        "مختلف \n",
        "gnn\n",
        "ها با نام‌های\n",
        "Graph Convolution Network \n",
        "و\n",
        "Graph Attention Network \n",
        "و\n",
        "Graph Isomorphism Network \n",
        "و\n",
        "GAT-GCN \n",
        " استفاده شده است که می‌توان گفت منشا اصلی تفاوت \n",
        " آن‌ها در نوع ارسال پیام‌ها میان نودها و آپدیت فیچرها می‌باشد. \n",
        " با بررسی نتایج آورده شده در این مقاله، فهمیدیم که \n",
        " GIN\n",
        " بهترین نتیجه را در تسک رگرشن بدست آورده است. \n",
        " ویژگی اصلی\n",
        "در \n",
        "GIN\n",
        "این است که فیچرهای موجود در یک نود به صورت زیر آپدیت می‌شوند:\n",
        "\\begin{equation*}\n",
        "X_i^{(t+1)} = MLP((1 + \\epsilon) X_i^{(t)} + \\sum_{j \\in B(i)} X_j^{(t)})\n",
        "\\end{equation*}\n",
        "که \n",
        "B(i)\n",
        "نمایانگر همسایه‌های نود \n",
        "i\n",
        "ام و \n",
        "MLP\n",
        "نمایانگر شبکه\n",
        "Multilayer Perceptron \n",
        "می‌باشد. در مدل از ۵ لایه\n",
        "GIN\n",
        "استفاده می‌کنیم. \n",
        "همچنین برای آنکه در \n",
        "gnn\n",
        "ها مسیر \n",
        "backprop\n",
        "می‌تواند بسیار عمیق شود، از \n",
        "batch normalization\n",
        "میان لایه‌های \n",
        "gin\n",
        "استفاده شده است. در انتهای لایه‌های گرافی، نیاز داریم با معیاری فیچرهای را میان تمام نودها \n",
        "aggregate\n",
        "کنیم تا به یک سری فیچر برای کل گراف برسیم. اینجا فیچرهای نودها را جمع می‌کنیم تا به یک فیچر خلاصه برای کل گراف برسیم.\n",
        "(global add pool)\n",
        ".\n",
        "\n",
        "برای پروتئین‌ها چون یک توالی است، بسیار مهم است که امبدینگ مناسبی برای آن‌ها ایجاد شده باشد. برای این منظور وزن‌های امبدینگ استفاده شده در پای‌تورچ را یادگیری‌پذیر قرار دادیم که وزن‌های مناسب را یاد بگیرد. همچنین امبدینگ‌های خاص‌تری مانند \n",
        "ProtVec\n",
        "وجود دارد که اگر استفاده شاید بتواند به مدل کمک کند.\n",
        "\n",
        "همچنین برای پروتئین تصمیم گرفتیم که از شبکه \n",
        "LSTM \n",
        "برای یادگیری توالی پروتئین استفاده کنیم. این کار را در ابتدا بر روی \n",
        "DeepDTA\n",
        "انجام دادیم \n",
        "(بدون آنکه برای دارو کار خاصی انجام دهیم)\n",
        "و در خروجی بهبودی حاصل نشد و حتی به خروجی \n",
        "DeepDTA\n",
        "نرسیدیم. (البته این موضوع به تعداد \n",
        "epoch\n",
        "هم بسیار می‌تواند وابسته باشد. اما ددر تعداد \n",
        "epoch \n",
        "برابر \n",
        "DeepDTA\n",
        "بهتر عمل کرد\n",
        ")\n",
        ". سپس یک بار \n",
        "LSTM\n",
        "را بر روی مدل مبتنی بر گراف قرار دادیم و تا \n",
        "epoch\n",
        "ای که یاد گرفتیم، بهبود خاصی در نتیجه دیده نمی‌شد.\n",
        "این مشاهده توسط سه مقاله\n",
        "DeepDTA , GraphDTA, DeepGS\n",
        "هم به نوعی مورد تایید می‌باشند چرا که در هیچکدام از این سه مقاله، برای یادگیری پروتئین‌ها از \n",
        "rnn\n",
        "استفاده نشده است و در هر ۳ از \n",
        "cnn\n",
        "استفاده شده است و این مشاهده ما را تایید می‌کند.\n",
        "در نتیجه تغییرات در پروتئین تنها به یاد گرفتن وزن‌های امبدینگ محدود شد و از \n",
        "cnn\n",
        "استفاده کردیم که به نظر گزینه بهتری از \n",
        "LSTM\n",
        "در اینجا می‌باشد.\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j0nLkYkTBQX",
        "colab_type": "text"
      },
      "source": [
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "۳.\n",
        "</font></h1><p></p>\n",
        "باید ورودی دارو که به نمایش\n",
        "SMILES\n",
        "است را به شکل گراف دربیاوریم. برای این منظور از پکیج\n",
        "rdkit\n",
        "استفاده می‌شود که به وسیله آن می‌توان\n",
        "smile\n",
        "گرفت و مولکول متناظر آنرا دریافت کرد.\n",
        "همچنین در مقاله گفته شده است که برای هر اتم ۵ نوع کلی فیچر ساخته می‌شود : نشان هر اتم، تعداد اتم‌های مجاور، تعداد هیدروژن‌های مجاور، ارزش ضمنی اتم\n",
        "(implicit value) \n",
        "و \n",
        "aromatic\n",
        "بودن یا نبودن ساختار اتم.\n",
        "حال نیاز داریم که دیتا رو به فرمت مطابق با پکیج \n",
        "torch-geometric\n",
        "که برای \n",
        "gnn\n",
        "ها دولوپ شده است دربیاوریم. در این پکیج از سه مفهوم \n",
        "Data, TestbedDatset, DataLoader\n",
        "برای دیتا استفاده می‌کنیم که هر کدام با توجه به آنکه دیتا گرافی را باید هندل کنند، تفاوت‌های بنیادی با مفاهیم معادل در حالت عادی پای‌تورچ دارند. به طور کلی می‌بایست فیچرهای اتم‌ها را در \n",
        "Data.x\n",
        "،\n",
        "یال‌ها را در \n",
        "Data.edge_index\n",
        "و \n",
        "affinity\n",
        "را در \n",
        "Data.y\n",
        "قرار دهیم. همچنین برای آنکه به پروتئین‌ها هم در دیتاست بتوانیم دسترسی داشته باشیم، ، یک \n",
        "attribute\n",
        "دیگر به نام \n",
        "target\n",
        "خودمان به \n",
        "Data\n",
        "اضافه می‌کنیم.\n",
        "\n",
        "برای پروتئین‌ها پیش‌پردازش خیلی خاصی نداریم چرا که مدل همان \n",
        "CNN\n",
        "است و تنها لازم است حروف را به یک سری عدد تبدیل کنیم. و در ادامه در امبدینگ، هر عدد به یک بردار ۱۲۸ بعدی تبدیل می‌شود. تنها این نکته باقی می‌ماند که برای پروتئین‌ها طول ثابت ۱۰۰۰ را در نظر گرفته‌ایم. اگر طول یک پروتئین کمتر باشد با صفر\n",
        "pad\n",
        "می‌شود و اگر بیشتر باشد\n",
        "truncate\n",
        "می‌شود.\n",
        "در \n",
        "preprocess\n",
        "از گیت‌هاب \n",
        "GraphDTA\n",
        "کمک گرفته شد.\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL2hugyqgR5-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "\n",
        "</font></h1><p></p>\n",
        "ادامه سوال‌ها در میان سلول‌ها آمده است.\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNW61nWEqUVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ec978513-ad5f-49c2-88ec-a1e8fd7fbb09"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lStetdWfqbb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/data\" ."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj-GnDY0gXXe",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "\n",
        "</font></h1><p></p>\n",
        "دستورات نصب از دو لینک\n",
        "<a href=\"https://towardsdatascience.com/conda-google-colab-75f7c867a522\">اینجا\n",
        "</a>\n",
        "و\n",
        "<a href=\"https://github.com/thinng/GraphDTA\">اینجا\n",
        "</a>\n",
        "برداشته شده است\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo4-XvqlZuSN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67c155db-fcb0-46aa-ad37-42607d130cc7"
      },
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX\n",
        "conda install --channel defaults conda python=3.6 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n",
            "installing: ca-certificates-2018.03.07-0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: libgcc-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: ncurses-6.1-hf484d3e_0 ...\n",
            "installing: openssl-1.0.2o-h20670df_0 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: xz-5.2.4-h14c3975_4 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: sqlite-3.23.1-he433501_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: certifi-2018.4.16-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pysocks-1.6.8-py36_0 ...\n",
            "installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: cffi-1.11.5-py36h9745a5d_0 ...\n",
            "installing: setuptools-39.2.0-py36_0 ...\n",
            "installing: cryptography-2.2.2-py36h14c3975_0 ...\n",
            "installing: wheel-0.31.1-py36_0 ...\n",
            "installing: pip-10.0.1-py36_0 ...\n",
            "installing: pyopenssl-18.0.0-py36_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: conda-4.5.4-py36_0 ...\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - conda\n",
            "    - python=3.6\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ld_impl_linux-64-2.33.1    |       h53a641e_7         645 KB\n",
            "    openssl-1.1.1g             |       h7b6447c_0         3.8 MB\n",
            "    conda-4.8.4                |           py36_0         3.0 MB\n",
            "    setuptools-49.6.0          |           py36_0         927 KB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    cffi-1.14.1                |   py36he30daa8_0         227 KB\n",
            "    cryptography-2.9.2         |   py36h1ba5d50_0         626 KB\n",
            "    pysocks-1.7.1              |           py36_0          30 KB\n",
            "    tqdm-4.48.2                |             py_0          63 KB\n",
            "    urllib3-1.25.10            |             py_0          93 KB\n",
            "    libedit-3.1.20191231       |       h14c3975_1         121 KB\n",
            "    certifi-2020.6.20          |           py36_0         160 KB\n",
            "    pycosat-0.6.3              |   py36h7b6447c_0         107 KB\n",
            "    pip-20.2.2                 |           py36_0         2.0 MB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    readline-8.0               |       h7b6447c_0         428 KB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    ca-certificates-2020.6.24  |                0         133 KB\n",
            "    idna-2.10                  |             py_0          56 KB\n",
            "    six-1.15.0                 |             py_0          13 KB\n",
            "    yaml-0.2.5                 |       h7b6447c_0          87 KB\n",
            "    pyopenssl-19.1.0           |             py_1          47 KB\n",
            "    brotlipy-0.7.0             |py36h7b6447c_1000         348 KB\n",
            "    zlib-1.2.11                |       h7b6447c_3         120 KB\n",
            "    tk-8.6.10                  |       hbc83047_0         3.2 MB\n",
            "    requests-2.24.0            |             py_0          54 KB\n",
            "    ruamel_yaml-0.15.87        |   py36h7b6447c_1         256 KB\n",
            "    python-3.6.10              |       h7579374_2        33.9 MB\n",
            "    conda-package-handling-1.6.1|   py36h7b6447c_0         886 KB\n",
            "    chardet-3.0.4              |        py36_1003         197 KB\n",
            "    wheel-0.34.2               |           py36_0          49 KB\n",
            "    libffi-3.3                 |       he6710b0_2          54 KB\n",
            "    ncurses-6.2                |       he6710b0_1         1.1 MB\n",
            "    libstdcxx-ng-9.1.0         |       hdf63c60_0         4.0 MB\n",
            "    sqlite-3.33.0              |       h62c20be_0         2.0 MB\n",
            "    pycparser-2.20             |             py_2          94 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        67.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:          0.1-main               \n",
            "    brotlipy:               0.7.0-py36h7b6447c_1000\n",
            "    conda-package-handling: 1.6.1-py36h7b6447c_0   \n",
            "    ld_impl_linux-64:       2.33.1-h53a641e_7      \n",
            "    tqdm:                   4.48.2-py_0            \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates:        2018.03.07-0            --> 2020.6.24-0            \n",
            "    certifi:                2018.4.16-py36_0        --> 2020.6.20-py36_0       \n",
            "    cffi:                   1.11.5-py36h9745a5d_0   --> 1.14.1-py36he30daa8_0  \n",
            "    chardet:                3.0.4-py36h0f667ec_1    --> 3.0.4-py36_1003        \n",
            "    conda:                  4.5.4-py36_0            --> 4.8.4-py36_0           \n",
            "    cryptography:           2.2.2-py36h14c3975_0    --> 2.9.2-py36h1ba5d50_0   \n",
            "    idna:                   2.6-py36h82fb2a8_1      --> 2.10-py_0              \n",
            "    libedit:                3.1.20170329-h6b74fdf_2 --> 3.1.20191231-h14c3975_1\n",
            "    libffi:                 3.2.1-hd88cf55_4        --> 3.3-he6710b0_2         \n",
            "    libgcc-ng:              7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0       \n",
            "    libstdcxx-ng:           7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0       \n",
            "    ncurses:                6.1-hf484d3e_0          --> 6.2-he6710b0_1         \n",
            "    openssl:                1.0.2o-h20670df_0       --> 1.1.1g-h7b6447c_0      \n",
            "    pip:                    10.0.1-py36_0           --> 20.2.2-py36_0          \n",
            "    pycosat:                0.6.3-py36h0a5515d_0    --> 0.6.3-py36h7b6447c_0   \n",
            "    pycparser:              2.18-py36hf9f622e_1     --> 2.20-py_2              \n",
            "    pyopenssl:              18.0.0-py36_0           --> 19.1.0-py_1            \n",
            "    pysocks:                1.6.8-py36_0            --> 1.7.1-py36_0           \n",
            "    python:                 3.6.5-hc3d631a_2        --> 3.6.10-h7579374_2      \n",
            "    readline:               7.0-ha6073c6_4          --> 8.0-h7b6447c_0         \n",
            "    requests:               2.18.4-py36he2e5f8d_1   --> 2.24.0-py_0            \n",
            "    ruamel_yaml:            0.15.37-py36h14c3975_2  --> 0.15.87-py36h7b6447c_1 \n",
            "    setuptools:             39.2.0-py36_0           --> 49.6.0-py36_0          \n",
            "    six:                    1.11.0-py36h372c433_1   --> 1.15.0-py_0            \n",
            "    sqlite:                 3.23.1-he433501_0       --> 3.33.0-h62c20be_0      \n",
            "    tk:                     8.6.7-hc745277_3        --> 8.6.10-hbc83047_0      \n",
            "    urllib3:                1.22-py36hbe7ace6_0     --> 1.25.10-py_0           \n",
            "    wheel:                  0.31.1-py36_0           --> 0.34.2-py36_0          \n",
            "    xz:                     5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0       \n",
            "    yaml:                   0.1.7-had09818_2        --> 0.2.5-h7b6447c_0       \n",
            "    zlib:                   1.2.11-ha838bed_2       --> 1.2.11-h7b6447c_3      \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  asn1crypto-0.24.0-py36_0\n",
            "  conda-env-2.6.0-h36134e3_1\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-08-22 17:26:33--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\n",
            "--2020-08-22 17:26:33--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58468498 (56M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 47.6M 1s\n",
            "    50K .......... .......... .......... .......... ..........  0% 14.2M 3s\n",
            "   100K .......... .......... .......... .......... ..........  0% 14.2M 3s\n",
            "   150K .......... .......... .......... .......... ..........  0% 14.5M 3s\n",
            "   200K .......... .......... .......... .......... ..........  0%  128M 3s\n",
            "   250K .......... .......... .......... .......... ..........  0%  161M 2s\n",
            "   300K .......... .......... .......... .......... ..........  0% 35.3M 2s\n",
            "   350K .......... .......... .......... .......... ..........  0% 38.1M 2s\n",
            "   400K .......... .......... .......... .......... ..........  0%  209M 2s\n",
            "   450K .......... .......... .......... .......... ..........  0%  140M 2s\n",
            "   500K .......... .......... .......... .......... ..........  0%  133M 2s\n",
            "   550K .......... .......... .......... .......... ..........  1%  129M 2s\n",
            "   600K .......... .......... .......... .......... ..........  1%  163M 1s\n",
            "   650K .......... .......... .......... .......... ..........  1%  129M 1s\n",
            "   700K .......... .......... .......... .......... ..........  1%  150M 1s\n",
            "   750K .......... .......... .......... .......... ..........  1%  145M 1s\n",
            "   800K .......... .......... .......... .......... ..........  1%  148M 1s\n",
            "   850K .......... .......... .......... .......... ..........  1%  159M 1s\n",
            "   900K .......... .......... .......... .......... ..........  1%  160M 1s\n",
            "   950K .......... .......... .......... .......... ..........  1%  117M 1s\n",
            "  1000K .......... .......... .......... .......... ..........  1%  153M 1s\n",
            "  1050K .......... .......... .......... .......... ..........  1%  135M 1s\n",
            "  1100K .......... .......... .......... .......... ..........  2%  163M 1s\n",
            "  1150K .......... .......... .......... .......... ..........  2%  114M 1s\n",
            "  1200K .......... .......... .......... .......... ..........  2%  134M 1s\n",
            "  1250K .......... .......... .......... .......... ..........  2%  117M 1s\n",
            "  1300K .......... .......... .......... .......... ..........  2%  123M 1s\n",
            "  1350K .......... .......... .......... .......... ..........  2%  111M 1s\n",
            "  1400K .......... .......... .......... .......... ..........  2%  125M 1s\n",
            "  1450K .......... .......... .......... .......... ..........  2%  126M 1s\n",
            "  1500K .......... .......... .......... .......... ..........  2%  117M 1s\n",
            "  1550K .......... .......... .......... .......... ..........  2%  116M 1s\n",
            "  1600K .......... .......... .......... .......... ..........  2%  125M 1s\n",
            "  1650K .......... .......... .......... .......... ..........  2%  141M 1s\n",
            "  1700K .......... .......... .......... .......... ..........  3%  131M 1s\n",
            "  1750K .......... .......... .......... .......... ..........  3%  145M 1s\n",
            "  1800K .......... .......... .......... .......... ..........  3%  164M 1s\n",
            "  1850K .......... .......... .......... .......... ..........  3%  149M 1s\n",
            "  1900K .......... .......... .......... .......... ..........  3%  125M 1s\n",
            "  1950K .......... .......... .......... .......... ..........  3%  126M 1s\n",
            "  2000K .......... .......... .......... .......... ..........  3%  145M 1s\n",
            "  2050K .......... .......... .......... .......... ..........  3%  139M 1s\n",
            "  2100K .......... .......... .......... .......... ..........  3%  132M 1s\n",
            "  2150K .......... .......... .......... .......... ..........  3%  140M 1s\n",
            "  2200K .......... .......... .......... .......... ..........  3%  141M 1s\n",
            "  2250K .......... .......... .......... .......... ..........  4%  135M 1s\n",
            "  2300K .......... .......... .......... .......... ..........  4%  150M 1s\n",
            "  2350K .......... .......... .......... .......... ..........  4%  137M 1s\n",
            "  2400K .......... .......... .......... .......... ..........  4%  165M 1s\n",
            "  2450K .......... .......... .......... .......... ..........  4%  155M 1s\n",
            "  2500K .......... .......... .......... .......... ..........  4%  154M 1s\n",
            "  2550K .......... .......... .......... .......... ..........  4%  131M 1s\n",
            "  2600K .......... .......... .......... .......... ..........  4%  124M 1s\n",
            "  2650K .......... .......... .......... .......... ..........  4%  156M 1s\n",
            "  2700K .......... .......... .......... .......... ..........  4%  113M 1s\n",
            "  2750K .......... .......... .......... .......... ..........  4%  113M 1s\n",
            "  2800K .......... .......... .......... .......... ..........  4%  145M 1s\n",
            "  2850K .......... .......... .......... .......... ..........  5%  147M 1s\n",
            "  2900K .......... .......... .......... .......... ..........  5%  134M 1s\n",
            "  2950K .......... .......... .......... .......... ..........  5%  118M 1s\n",
            "  3000K .......... .......... .......... .......... ..........  5%  131M 1s\n",
            "  3050K .......... .......... .......... .......... ..........  5%  144M 1s\n",
            "  3100K .......... .......... .......... .......... ..........  5%  127M 1s\n",
            "  3150K .......... .......... .......... .......... ..........  5%  122M 1s\n",
            "  3200K .......... .......... .......... .......... ..........  5%  164M 1s\n",
            "  3250K .......... .......... .......... .......... ..........  5%  150M 1s\n",
            "  3300K .......... .......... .......... .......... ..........  5%  154M 1s\n",
            "  3350K .......... .......... .......... .......... ..........  5%  119M 1s\n",
            "  3400K .......... .......... .......... .......... ..........  6%  138M 1s\n",
            "  3450K .......... .......... .......... .......... ..........  6%  113M 1s\n",
            "  3500K .......... .......... .......... .......... ..........  6%  133M 1s\n",
            "  3550K .......... .......... .......... .......... ..........  6%  128M 1s\n",
            "  3600K .......... .......... .......... .......... ..........  6%  167M 1s\n",
            "  3650K .......... .......... .......... .......... ..........  6%  171M 1s\n",
            "  3700K .......... .......... .......... .......... ..........  6%  136M 1s\n",
            "  3750K .......... .......... .......... .......... ..........  6%  127M 1s\n",
            "  3800K .......... .......... .......... .......... ..........  6%  154M 1s\n",
            "  3850K .......... .......... .......... .......... ..........  6%  163M 1s\n",
            "  3900K .......... .......... .......... .......... ..........  6%  162M 1s\n",
            "  3950K .......... .......... .......... .......... ..........  7%  139M 1s\n",
            "  4000K .......... .......... .......... .......... ..........  7%  125M 1s\n",
            "  4050K .......... .......... .......... .......... ..........  7%  122M 1s\n",
            "  4100K .......... .......... .......... .......... ..........  7%  139M 1s\n",
            "  4150K .......... .......... .......... .......... ..........  7%  118M 1s\n",
            "  4200K .......... .......... .......... .......... ..........  7%  160M 1s\n",
            "  4250K .......... .......... .......... .......... ..........  7%  153M 1s\n",
            "  4300K .......... .......... .......... .......... ..........  7%  165M 1s\n",
            "  4350K .......... .......... .......... .......... ..........  7%  112M 1s\n",
            "  4400K .......... .......... .......... .......... ..........  7%  127M 1s\n",
            "  4450K .......... .......... .......... .......... ..........  7%  146M 1s\n",
            "  4500K .......... .......... .......... .......... ..........  7%  156M 1s\n",
            "  4550K .......... .......... .......... .......... ..........  8%  118M 1s\n",
            "  4600K .......... .......... .......... .......... ..........  8%  156M 1s\n",
            "  4650K .......... .......... .......... .......... ..........  8%  140M 1s\n",
            "  4700K .......... .......... .......... .......... ..........  8%  145M 1s\n",
            "  4750K .......... .......... .......... .......... ..........  8%  132M 1s\n",
            "  4800K .......... .......... .......... .......... ..........  8%  157M 0s\n",
            "  4850K .......... .......... .......... .......... ..........  8%  157M 0s\n",
            "  4900K .......... .......... .......... .......... ..........  8%  151M 0s\n",
            "  4950K .......... .......... .......... .......... ..........  8%  112M 0s\n",
            "  5000K .......... .......... .......... .......... ..........  8%  134M 0s\n",
            "  5050K .......... .......... .......... .......... ..........  8%  120M 0s\n",
            "  5100K .......... .......... .......... .......... ..........  9%  147M 0s\n",
            "  5150K .......... .......... .......... .......... ..........  9%  151M 0s\n",
            "  5200K .......... .......... .......... .......... ..........  9%  151M 0s\n",
            "  5250K .......... .......... .......... .......... ..........  9%  157M 0s\n",
            "  5300K .......... .......... .......... .......... ..........  9%  149M 0s\n",
            "  5350K .......... .......... .......... .......... ..........  9%  106M 0s\n",
            "  5400K .......... .......... .......... .......... ..........  9%  125M 0s\n",
            "  5450K .......... .......... .......... .......... ..........  9%  137M 0s\n",
            "  5500K .......... .......... .......... .......... ..........  9%  128M 0s\n",
            "  5550K .......... .......... .......... .......... ..........  9%  106M 0s\n",
            "  5600K .......... .......... .......... .......... ..........  9%  168M 0s\n",
            "  5650K .......... .......... .......... .......... ..........  9%  159M 0s\n",
            "  5700K .......... .......... .......... .......... .......... 10%  166M 0s\n",
            "  5750K .......... .......... .......... .......... .......... 10%  121M 0s\n",
            "  5800K .......... .......... .......... .......... .......... 10%  160M 0s\n",
            "  5850K .......... .......... .......... .......... .......... 10%  178M 0s\n",
            "  5900K .......... .......... .......... .......... .......... 10%  171M 0s\n",
            "  5950K .......... .......... .......... .......... .......... 10%  123M 0s\n",
            "  6000K .......... .......... .......... .......... .......... 10%  138M 0s\n",
            "  6050K .......... .......... .......... .......... .......... 10%  171M 0s\n",
            "  6100K .......... .......... .......... .......... .......... 10%  131M 0s\n",
            "  6150K .......... .......... .......... .......... .......... 10%  140M 0s\n",
            "  6200K .......... .......... .......... .......... .......... 10%  136M 0s\n",
            "  6250K .......... .......... .......... .......... .......... 11%  142M 0s\n",
            "  6300K .......... .......... .......... .......... .......... 11%  164M 0s\n",
            "  6350K .......... .......... .......... .......... .......... 11%  126M 0s\n",
            "  6400K .......... .......... .......... .......... .......... 11%  157M 0s\n",
            "  6450K .......... .......... .......... .......... .......... 11%  150M 0s\n",
            "  6500K .......... .......... .......... .......... .......... 11%  119M 0s\n",
            "  6550K .......... .......... .......... .......... .......... 11%  113M 0s\n",
            "  6600K .......... .......... .......... .......... .......... 11%  130M 0s\n",
            "  6650K .......... .......... .......... .......... .......... 11%  166M 0s\n",
            "  6700K .......... .......... .......... .......... .......... 11%  149M 0s\n",
            "  6750K .......... .......... .......... .......... .......... 11%  135M 0s\n",
            "  6800K .......... .......... .......... .......... .......... 11%  163M 0s\n",
            "  6850K .......... .......... .......... .......... .......... 12%  158M 0s\n",
            "  6900K .......... .......... .......... .......... .......... 12%  121M 0s\n",
            "  6950K .......... .......... .......... .......... .......... 12%  117M 0s\n",
            "  7000K .......... .......... .......... .......... .......... 12%  138M 0s\n",
            "  7050K .......... .......... .......... .......... .......... 12%  141M 0s\n",
            "  7100K .......... .......... .......... .......... .......... 12%  153M 0s\n",
            "  7150K .......... .......... .......... .......... .......... 12%  125M 0s\n",
            "  7200K .......... .......... .......... .......... .......... 12%  165M 0s\n",
            "  7250K .......... .......... .......... .......... .......... 12%  167M 0s\n",
            "  7300K .......... .......... .......... .......... .......... 12%  156M 0s\n",
            "  7350K .......... .......... .......... .......... .......... 12%  145M 0s\n",
            "  7400K .......... .......... .......... .......... .......... 13%  145M 0s\n",
            "  7450K .......... .......... .......... .......... .......... 13%  135M 0s\n",
            "  7500K .......... .......... .......... .......... .......... 13%  119M 0s\n",
            "  7550K .......... .......... .......... .......... .......... 13%  120M 0s\n",
            "  7600K .......... .......... .......... .......... .......... 13%  137M 0s\n",
            "  7650K .......... .......... .......... .......... .......... 13%  142M 0s\n",
            "  7700K .......... .......... .......... .......... .......... 13%  160M 0s\n",
            "  7750K .......... .......... .......... .......... .......... 13%  149M 0s\n",
            "  7800K .......... .......... .......... .......... .......... 13%  146M 0s\n",
            "  7850K .......... .......... .......... .......... .......... 13%  128M 0s\n",
            "  7900K .......... .......... .......... .......... .......... 13%  171M 0s\n",
            "  7950K .......... .......... .......... .......... .......... 14%  138M 0s\n",
            "  8000K .......... .......... .......... .......... .......... 14%  131M 0s\n",
            "  8050K .......... .......... .......... .......... .......... 14%  132M 0s\n",
            "  8100K .......... .......... .......... .......... .......... 14%  161M 0s\n",
            "  8150K .......... .......... .......... .......... .......... 14%  139M 0s\n",
            "  8200K .......... .......... .......... .......... .......... 14%  171M 0s\n",
            "  8250K .......... .......... .......... .......... .......... 14%  154M 0s\n",
            "  8300K .......... .......... .......... .......... .......... 14%  103M 0s\n",
            "  8350K .......... .......... .......... .......... .......... 14%  121M 0s\n",
            "  8400K .......... .......... .......... .......... .......... 14%  142M 0s\n",
            "  8450K .......... .......... .......... .......... .......... 14%  145M 0s\n",
            "  8500K .......... .......... .......... .......... .......... 14%  136M 0s\n",
            "  8550K .......... .......... .......... .......... .......... 15%  141M 0s\n",
            "  8600K .......... .......... .......... .......... .......... 15%  173M 0s\n",
            "  8650K .......... .......... .......... .......... .......... 15%  162M 0s\n",
            "  8700K .......... .......... .......... .......... .......... 15%  165M 0s\n",
            "  8750K .......... .......... .......... .......... .......... 15%  120M 0s\n",
            "  8800K .......... .......... .......... .......... .......... 15%  160M 0s\n",
            "  8850K .......... .......... .......... .......... .......... 15%  163M 0s\n",
            "  8900K .......... .......... .......... .......... .......... 15%  172M 0s\n",
            "  8950K .......... .......... .......... .......... .......... 15%  143M 0s\n",
            "  9000K .......... .......... .......... .......... .......... 15%  163M 0s\n",
            "  9050K .......... .......... .......... .......... .......... 15%  161M 0s\n",
            "  9100K .......... .......... .......... .......... .......... 16%  171M 0s\n",
            "  9150K .......... .......... .......... .......... .......... 16%  148M 0s\n",
            "  9200K .......... .......... .......... .......... .......... 16%  164M 0s\n",
            "  9250K .......... .......... .......... .......... .......... 16%  167M 0s\n",
            "  9300K .......... .......... .......... .......... .......... 16%  174M 0s\n",
            "  9350K .......... .......... .......... .......... .......... 16%  129M 0s\n",
            "  9400K .......... .......... .......... .......... .......... 16%  141M 0s\n",
            "  9450K .......... .......... .......... .......... .......... 16%  133M 0s\n",
            "  9500K .......... .......... .......... .......... .......... 16%  127M 0s\n",
            "  9550K .......... .......... .......... .......... .......... 16%  105M 0s\n",
            "  9600K .......... .......... .......... .......... .......... 16%  140M 0s\n",
            "  9650K .......... .......... .......... .......... .......... 16%  168M 0s\n",
            "  9700K .......... .......... .......... .......... .......... 17%  113M 0s\n",
            "  9750K .......... .......... .......... .......... .......... 17%  108M 0s\n",
            "  9800K .......... .......... .......... .......... .......... 17%  165M 0s\n",
            "  9850K .......... .......... .......... .......... .......... 17%  133M 0s\n",
            "  9900K .......... .......... .......... .......... .......... 17%  152M 0s\n",
            "  9950K .......... .......... .......... .......... .......... 17%  136M 0s\n",
            " 10000K .......... .......... .......... .......... .......... 17%  161M 0s\n",
            " 10050K .......... .......... .......... .......... .......... 17%  147M 0s\n",
            " 10100K .......... .......... .......... .......... .......... 17%  138M 0s\n",
            " 10150K .......... .......... .......... .......... .......... 17%  136M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 17%  165M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 18%  169M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 18%  163M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 18%  129M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 18%  158M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 18%  121M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 18%  112M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 18%  107M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 18%  125M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 18%  154M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 18%  150M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 18%  143M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 19%  149M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 19%  133M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 19%  132M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 19%  132M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 19%  159M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 19%  151M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 19%  151M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 19%  129M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 19%  159M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 19%  165M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 19%  145M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 19%  106M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 20%  159M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 20%  162M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 20%  142M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 20%  122M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 20%  153M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 20%  168M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 20%  135M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 20% 89.8M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 20%  160M 0s\n",
            " 11850K .......... .......... .......... .......... .......... 20%  153M 0s\n",
            " 11900K .......... .......... .......... .......... .......... 20%  149M 0s\n",
            " 11950K .......... .......... .......... .......... .......... 21%  123M 0s\n",
            " 12000K .......... .......... .......... .......... .......... 21%  140M 0s\n",
            " 12050K .......... .......... .......... .......... .......... 21%  143M 0s\n",
            " 12100K .......... .......... .......... .......... .......... 21%  142M 0s\n",
            " 12150K .......... .......... .......... .......... .......... 21%  130M 0s\n",
            " 12200K .......... .......... .......... .......... .......... 21%  135M 0s\n",
            " 12250K .......... .......... .......... .......... .......... 21%  140M 0s\n",
            " 12300K .......... .......... .......... .......... .......... 21%  121M 0s\n",
            " 12350K .......... .......... .......... .......... .......... 21%  134M 0s\n",
            " 12400K .......... .......... .......... .......... .......... 21%  157M 0s\n",
            " 12450K .......... .......... .......... .......... .......... 21%  142M 0s\n",
            " 12500K .......... .......... .......... .......... .......... 21%  158M 0s\n",
            " 12550K .......... .......... .......... .......... .......... 22%  112M 0s\n",
            " 12600K .......... .......... .......... .......... .......... 22%  125M 0s\n",
            " 12650K .......... .......... .......... .......... .......... 22%  160M 0s\n",
            " 12700K .......... .......... .......... .......... .......... 22%  145M 0s\n",
            " 12750K .......... .......... .......... .......... .......... 22%  125M 0s\n",
            " 12800K .......... .......... .......... .......... .......... 22%  148M 0s\n",
            " 12850K .......... .......... .......... .......... .......... 22%  125M 0s\n",
            " 12900K .......... .......... .......... .......... .......... 22%  130M 0s\n",
            " 12950K .......... .......... .......... .......... .......... 22%  137M 0s\n",
            " 13000K .......... .......... .......... .......... .......... 22%  150M 0s\n",
            " 13050K .......... .......... .......... .......... .......... 22%  160M 0s\n",
            " 13100K .......... .......... .......... .......... .......... 23%  163M 0s\n",
            " 13150K .......... .......... .......... .......... .......... 23% 97.3M 0s\n",
            " 13200K .......... .......... .......... .......... .......... 23%  145M 0s\n",
            " 13250K .......... .......... .......... .......... .......... 23%  158M 0s\n",
            " 13300K .......... .......... .......... .......... .......... 23%  151M 0s\n",
            " 13350K .......... .......... .......... .......... .......... 23%  120M 0s\n",
            " 13400K .......... .......... .......... .......... .......... 23%  134M 0s\n",
            " 13450K .......... .......... .......... .......... .......... 23%  150M 0s\n",
            " 13500K .......... .......... .......... .......... .......... 23%  166M 0s\n",
            " 13550K .......... .......... .......... .......... .......... 23%  125M 0s\n",
            " 13600K .......... .......... .......... .......... .......... 23%  151M 0s\n",
            " 13650K .......... .......... .......... .......... .......... 23%  164M 0s\n",
            " 13700K .......... .......... .......... .......... .......... 24%  148M 0s\n",
            " 13750K .......... .......... .......... .......... .......... 24%  109M 0s\n",
            " 13800K .......... .......... .......... .......... .......... 24%  123M 0s\n",
            " 13850K .......... .......... .......... .......... .......... 24%  155M 0s\n",
            " 13900K .......... .......... .......... .......... .......... 24%  167M 0s\n",
            " 13950K .......... .......... .......... .......... .......... 24%  121M 0s\n",
            " 14000K .......... .......... .......... .......... .......... 24%  129M 0s\n",
            " 14050K .......... .......... .......... .......... .......... 24%  155M 0s\n",
            " 14100K .......... .......... .......... .......... .......... 24%  159M 0s\n",
            " 14150K .......... .......... .......... .......... .......... 24%  128M 0s\n",
            " 14200K .......... .......... .......... .......... .......... 24%  141M 0s\n",
            " 14250K .......... .......... .......... .......... .......... 25%  158M 0s\n",
            " 14300K .......... .......... .......... .......... .......... 25%  124M 0s\n",
            " 14350K .......... .......... .......... .......... .......... 25%  147M 0s\n",
            " 14400K .......... .......... .......... .......... .......... 25%  135M 0s\n",
            " 14450K .......... .......... .......... .......... .......... 25%  159M 0s\n",
            " 14500K .......... .......... .......... .......... .......... 25%  166M 0s\n",
            " 14550K .......... .......... .......... .......... .......... 25%  121M 0s\n",
            " 14600K .......... .......... .......... .......... .......... 25%  155M 0s\n",
            " 14650K .......... .......... .......... .......... .......... 25%  166M 0s\n",
            " 14700K .......... .......... .......... .......... .......... 25%  155M 0s\n",
            " 14750K .......... .......... .......... .......... .......... 25%  139M 0s\n",
            " 14800K .......... .......... .......... .......... .......... 26%  127M 0s\n",
            " 14850K .......... .......... .......... .......... .......... 26%  149M 0s\n",
            " 14900K .......... .......... .......... .......... .......... 26%  152M 0s\n",
            " 14950K .......... .......... .......... .......... .......... 26%  109M 0s\n",
            " 15000K .......... .......... .......... .......... .......... 26%  134M 0s\n",
            " 15050K .......... .......... .......... .......... .......... 26%  146M 0s\n",
            " 15100K .......... .......... .......... .......... .......... 26%  163M 0s\n",
            " 15150K .......... .......... .......... .......... .......... 26%  104M 0s\n",
            " 15200K .......... .......... .......... .......... .......... 26%  152M 0s\n",
            " 15250K .......... .......... .......... .......... .......... 26%  156M 0s\n",
            " 15300K .......... .......... .......... .......... .......... 26%  135M 0s\n",
            " 15350K .......... .......... .......... .......... .......... 26%  140M 0s\n",
            " 15400K .......... .......... .......... .......... .......... 27%  140M 0s\n",
            " 15450K .......... .......... .......... .......... .......... 27%  145M 0s\n",
            " 15500K .......... .......... .......... .......... .......... 27%  175M 0s\n",
            " 15550K .......... .......... .......... .......... .......... 27%  140M 0s\n",
            " 15600K .......... .......... .......... .......... .......... 27%  159M 0s\n",
            " 15650K .......... .......... .......... .......... .......... 27%  170M 0s\n",
            " 15700K .......... .......... .......... .......... .......... 27%  174M 0s\n",
            " 15750K .......... .......... .......... .......... .......... 27%  151M 0s\n",
            " 15800K .......... .......... .......... .......... .......... 27%  154M 0s\n",
            " 15850K .......... .......... .......... .......... .......... 27%  174M 0s\n",
            " 15900K .......... .......... .......... .......... .......... 27%  170M 0s\n",
            " 15950K .......... .......... .......... .......... .......... 28%  115M 0s\n",
            " 16000K .......... .......... .......... .......... .......... 28%  138M 0s\n",
            " 16050K .......... .......... .......... .......... .......... 28%  138M 0s\n",
            " 16100K .......... .......... .......... .......... .......... 28%  138M 0s\n",
            " 16150K .......... .......... .......... .......... .......... 28%  143M 0s\n",
            " 16200K .......... .......... .......... .......... .......... 28%  147M 0s\n",
            " 16250K .......... .......... .......... .......... .......... 28%  154M 0s\n",
            " 16300K .......... .......... .......... .......... .......... 28%  134M 0s\n",
            " 16350K .......... .......... .......... .......... .......... 28% 98.4M 0s\n",
            " 16400K .......... .......... .......... .......... .......... 28%  169M 0s\n",
            " 16450K .......... .......... .......... .......... .......... 28%  171M 0s\n",
            " 16500K .......... .......... .......... .......... .......... 28%  157M 0s\n",
            " 16550K .......... .......... .......... .......... .......... 29%  115M 0s\n",
            " 16600K .......... .......... .......... .......... .......... 29%  143M 0s\n",
            " 16650K .......... .......... .......... .......... .......... 29%  161M 0s\n",
            " 16700K .......... .......... .......... .......... .......... 29%  165M 0s\n",
            " 16750K .......... .......... .......... .......... .......... 29%  131M 0s\n",
            " 16800K .......... .......... .......... .......... .......... 29%  140M 0s\n",
            " 16850K .......... .......... .......... .......... .......... 29%  173M 0s\n",
            " 16900K .......... .......... .......... .......... .......... 29%  173M 0s\n",
            " 16950K .......... .......... .......... .......... .......... 29%  116M 0s\n",
            " 17000K .......... .......... .......... .......... .......... 29%  138M 0s\n",
            " 17050K .......... .......... .......... .......... .......... 29%  158M 0s\n",
            " 17100K .......... .......... .......... .......... .......... 30%  164M 0s\n",
            " 17150K .......... .......... .......... .......... .......... 30%  125M 0s\n",
            " 17200K .......... .......... .......... .......... .......... 30%  161M 0s\n",
            " 17250K .......... .......... .......... .......... .......... 30%  164M 0s\n",
            " 17300K .......... .......... .......... .......... .......... 30%  147M 0s\n",
            " 17350K .......... .......... .......... .......... .......... 30%  138M 0s\n",
            " 17400K .......... .......... .......... .......... .......... 30%  153M 0s\n",
            " 17450K .......... .......... .......... .......... .......... 30%  136M 0s\n",
            " 17500K .......... .......... .......... .......... .......... 30%  156M 0s\n",
            " 17550K .......... .......... .......... .......... .......... 30%  136M 0s\n",
            " 17600K .......... .......... .......... .......... .......... 30%  167M 0s\n",
            " 17650K .......... .......... .......... .......... .......... 30%  167M 0s\n",
            " 17700K .......... .......... .......... .......... .......... 31%  160M 0s\n",
            " 17750K .......... .......... .......... .......... .......... 31%  121M 0s\n",
            " 17800K .......... .......... .......... .......... .......... 31%  129M 0s\n",
            " 17850K .......... .......... .......... .......... .......... 31%  161M 0s\n",
            " 17900K .......... .......... .......... .......... .......... 31%  139M 0s\n",
            " 17950K .......... .......... .......... .......... .......... 31%  114M 0s\n",
            " 18000K .......... .......... .......... .......... .......... 31%  159M 0s\n",
            " 18050K .......... .......... .......... .......... .......... 31%  174M 0s\n",
            " 18100K .......... .......... .......... .......... .......... 31%  166M 0s\n",
            " 18150K .......... .......... .......... .......... .......... 31%  126M 0s\n",
            " 18200K .......... .......... .......... .......... .......... 31%  174M 0s\n",
            " 18250K .......... .......... .......... .......... .......... 32%  153M 0s\n",
            " 18300K .......... .......... .......... .......... .......... 32%  127M 0s\n",
            " 18350K .......... .......... .......... .......... .......... 32%  122M 0s\n",
            " 18400K .......... .......... .......... .......... .......... 32%  127M 0s\n",
            " 18450K .......... .......... .......... .......... .......... 32%  131M 0s\n",
            " 18500K .......... .......... .......... .......... .......... 32%  170M 0s\n",
            " 18550K .......... .......... .......... .......... .......... 32%  144M 0s\n",
            " 18600K .......... .......... .......... .......... .......... 32%  149M 0s\n",
            " 18650K .......... .......... .......... .......... .......... 32%  132M 0s\n",
            " 18700K .......... .......... .......... .......... .......... 32%  146M 0s\n",
            " 18750K .......... .......... .......... .......... .......... 32%  117M 0s\n",
            " 18800K .......... .......... .......... .......... .......... 33%  129M 0s\n",
            " 18850K .......... .......... .......... .......... .......... 33%  144M 0s\n",
            " 18900K .......... .......... .......... .......... .......... 33%  165M 0s\n",
            " 18950K .......... .......... .......... .......... .......... 33%  137M 0s\n",
            " 19000K .......... .......... .......... .......... .......... 33%  156M 0s\n",
            " 19050K .......... .......... .......... .......... .......... 33%  165M 0s\n",
            " 19100K .......... .......... .......... .......... .......... 33%  151M 0s\n",
            " 19150K .......... .......... .......... .......... .......... 33%  101M 0s\n",
            " 19200K .......... .......... .......... .......... .......... 33%  133M 0s\n",
            " 19250K .......... .......... .......... .......... .......... 33%  139M 0s\n",
            " 19300K .......... .......... .......... .......... .......... 33%  124M 0s\n",
            " 19350K .......... .......... .......... .......... .......... 33%  122M 0s\n",
            " 19400K .......... .......... .......... .......... .......... 34%  155M 0s\n",
            " 19450K .......... .......... .......... .......... .......... 34%  145M 0s\n",
            " 19500K .......... .......... .......... .......... .......... 34%  169M 0s\n",
            " 19550K .......... .......... .......... .......... .......... 34%  127M 0s\n",
            " 19600K .......... .......... .......... .......... .......... 34%  161M 0s\n",
            " 19650K .......... .......... .......... .......... .......... 34%  130M 0s\n",
            " 19700K .......... .......... .......... .......... .......... 34%  125M 0s\n",
            " 19750K .......... .......... .......... .......... .......... 34%  123M 0s\n",
            " 19800K .......... .......... .......... .......... .......... 34%  124M 0s\n",
            " 19850K .......... .......... .......... .......... .......... 34%  128M 0s\n",
            " 19900K .......... .......... .......... .......... .......... 34%  131M 0s\n",
            " 19950K .......... .......... .......... .......... .......... 35%  104M 0s\n",
            " 20000K .......... .......... .......... .......... .......... 35%  137M 0s\n",
            " 20050K .......... .......... .......... .......... .......... 35%  137M 0s\n",
            " 20100K .......... .......... .......... .......... .......... 35%  139M 0s\n",
            " 20150K .......... .......... .......... .......... .......... 35%  140M 0s\n",
            " 20200K .......... .......... .......... .......... .......... 35%  136M 0s\n",
            " 20250K .......... .......... .......... .......... .......... 35%  130M 0s\n",
            " 20300K .......... .......... .......... .......... .......... 35%  157M 0s\n",
            " 20350K .......... .......... .......... .......... .......... 35%  112M 0s\n",
            " 20400K .......... .......... .......... .......... .......... 35%  127M 0s\n",
            " 20450K .......... .......... .......... .......... .......... 35%  161M 0s\n",
            " 20500K .......... .......... .......... .......... .......... 35%  126M 0s\n",
            " 20550K .......... .......... .......... .......... .......... 36%  146M 0s\n",
            " 20600K .......... .......... .......... .......... .......... 36%  141M 0s\n",
            " 20650K .......... .......... .......... .......... .......... 36%  147M 0s\n",
            " 20700K .......... .......... .......... .......... .......... 36%  159M 0s\n",
            " 20750K .......... .......... .......... .......... .......... 36%  122M 0s\n",
            " 20800K .......... .......... .......... .......... .......... 36%  130M 0s\n",
            " 20850K .......... .......... .......... .......... .......... 36%  131M 0s\n",
            " 20900K .......... .......... .......... .......... .......... 36%  162M 0s\n",
            " 20950K .......... .......... .......... .......... .......... 36%  143M 0s\n",
            " 21000K .......... .......... .......... .......... .......... 36%  157M 0s\n",
            " 21050K .......... .......... .......... .......... .......... 36%  165M 0s\n",
            " 21100K .......... .......... .......... .......... .......... 37%  156M 0s\n",
            " 21150K .......... .......... .......... .......... .......... 37%  150M 0s\n",
            " 21200K .......... .......... .......... .......... .......... 37%  131M 0s\n",
            " 21250K .......... .......... .......... .......... .......... 37%  124M 0s\n",
            " 21300K .......... .......... .......... .......... .......... 37%  148M 0s\n",
            " 21350K .......... .......... .......... .......... .......... 37%  106M 0s\n",
            " 21400K .......... .......... .......... .......... .......... 37%  137M 0s\n",
            " 21450K .......... .......... .......... .......... .......... 37%  168M 0s\n",
            " 21500K .......... .......... .......... .......... .......... 37%  131M 0s\n",
            " 21550K .......... .......... .......... .......... .......... 37%  126M 0s\n",
            " 21600K .......... .......... .......... .......... .......... 37%  158M 0s\n",
            " 21650K .......... .......... .......... .......... .......... 38%  166M 0s\n",
            " 21700K .......... .......... .......... .......... .......... 38%  156M 0s\n",
            " 21750K .......... .......... .......... .......... .......... 38%  138M 0s\n",
            " 21800K .......... .......... .......... .......... .......... 38%  164M 0s\n",
            " 21850K .......... .......... .......... .......... .......... 38%  144M 0s\n",
            " 21900K .......... .......... .......... .......... .......... 38%  149M 0s\n",
            " 21950K .......... .......... .......... .......... .......... 38%  143M 0s\n",
            " 22000K .......... .......... .......... .......... .......... 38%  151M 0s\n",
            " 22050K .......... .......... .......... .......... .......... 38%  128M 0s\n",
            " 22100K .......... .......... .......... .......... .......... 38%  117M 0s\n",
            " 22150K .......... .......... .......... .......... .......... 38%  111M 0s\n",
            " 22200K .......... .......... .......... .......... .......... 38%  137M 0s\n",
            " 22250K .......... .......... .......... .......... .......... 39%  127M 0s\n",
            " 22300K .......... .......... .......... .......... .......... 39%  133M 0s\n",
            " 22350K .......... .......... .......... .......... .......... 39%  105M 0s\n",
            " 22400K .......... .......... .......... .......... .......... 39%  169M 0s\n",
            " 22450K .......... .......... .......... .......... .......... 39%  154M 0s\n",
            " 22500K .......... .......... .......... .......... .......... 39%  136M 0s\n",
            " 22550K .......... .......... .......... .......... .......... 39%  124M 0s\n",
            " 22600K .......... .......... .......... .......... .......... 39%  153M 0s\n",
            " 22650K .......... .......... .......... .......... .......... 39%  150M 0s\n",
            " 22700K .......... .......... .......... .......... .......... 39%  171M 0s\n",
            " 22750K .......... .......... .......... .......... .......... 39%  112M 0s\n",
            " 22800K .......... .......... .......... .......... .......... 40%  122M 0s\n",
            " 22850K .......... .......... .......... .......... .......... 40%  132M 0s\n",
            " 22900K .......... .......... .......... .......... .......... 40%  151M 0s\n",
            " 22950K .......... .......... .......... .......... .......... 40%  138M 0s\n",
            " 23000K .......... .......... .......... .......... .......... 40%  150M 0s\n",
            " 23050K .......... .......... .......... .......... .......... 40%  140M 0s\n",
            " 23100K .......... .......... .......... .......... .......... 40%  156M 0s\n",
            " 23150K .......... .......... .......... .......... .......... 40%  139M 0s\n",
            " 23200K .......... .......... .......... .......... .......... 40%  147M 0s\n",
            " 23250K .......... .......... .......... .......... .......... 40%  127M 0s\n",
            " 23300K .......... .......... .......... .......... .......... 40%  132M 0s\n",
            " 23350K .......... .......... .......... .......... .......... 40%  109M 0s\n",
            " 23400K .......... .......... .......... .......... .......... 41%  143M 0s\n",
            " 23450K .......... .......... .......... .......... .......... 41%  130M 0s\n",
            " 23500K .......... .......... .......... .......... .......... 41%  141M 0s\n",
            " 23550K .......... .......... .......... .......... .......... 41%  129M 0s\n",
            " 23600K .......... .......... .......... .......... .......... 41%  151M 0s\n",
            " 23650K .......... .......... .......... .......... .......... 41%  157M 0s\n",
            " 23700K .......... .......... .......... .......... .......... 41%  140M 0s\n",
            " 23750K .......... .......... .......... .......... .......... 41%  115M 0s\n",
            " 23800K .......... .......... .......... .......... .......... 41%  155M 0s\n",
            " 23850K .......... .......... .......... .......... .......... 41%  153M 0s\n",
            " 23900K .......... .......... .......... .......... .......... 41%  155M 0s\n",
            " 23950K .......... .......... .......... .......... .......... 42%  130M 0s\n",
            " 24000K .......... .......... .......... .......... .......... 42%  155M 0s\n",
            " 24050K .......... .......... .......... .......... .......... 42%  155M 0s\n",
            " 24100K .......... .......... .......... .......... .......... 42%  126M 0s\n",
            " 24150K .......... .......... .......... .......... .......... 42%  127M 0s\n",
            " 24200K .......... .......... .......... .......... .......... 42%  137M 0s\n",
            " 24250K .......... .......... .......... .......... .......... 42%  155M 0s\n",
            " 24300K .......... .......... .......... .......... .......... 42%  165M 0s\n",
            " 24350K .......... .......... .......... .......... .......... 42%  119M 0s\n",
            " 24400K .......... .......... .......... .......... .......... 42%  144M 0s\n",
            " 24450K .......... .......... .......... .......... .......... 42%  131M 0s\n",
            " 24500K .......... .......... .......... .......... .......... 42%  156M 0s\n",
            " 24550K .......... .......... .......... .......... .......... 43%  128M 0s\n",
            " 24600K .......... .......... .......... .......... .......... 43%  150M 0s\n",
            " 24650K .......... .......... .......... .......... .......... 43%  145M 0s\n",
            " 24700K .......... .......... .......... .......... .......... 43%  140M 0s\n",
            " 24750K .......... .......... .......... .......... .......... 43%  130M 0s\n",
            " 24800K .......... .......... .......... .......... .......... 43%  154M 0s\n",
            " 24850K .......... .......... .......... .......... .......... 43%  147M 0s\n",
            " 24900K .......... .......... .......... .......... .......... 43%  159M 0s\n",
            " 24950K .......... .......... .......... .......... .......... 43%  120M 0s\n",
            " 25000K .......... .......... .......... .......... .......... 43% 98.7M 0s\n",
            " 25050K .......... .......... .......... .......... .......... 43%  136M 0s\n",
            " 25100K .......... .......... .......... .......... .......... 44%  148M 0s\n",
            " 25150K .......... .......... .......... .......... .......... 44%  135M 0s\n",
            " 25200K .......... .......... .......... .......... .......... 44%  131M 0s\n",
            " 25250K .......... .......... .......... .......... .......... 44%  170M 0s\n",
            " 25300K .......... .......... .......... .......... .......... 44%  147M 0s\n",
            " 25350K .......... .......... .......... .......... .......... 44%  120M 0s\n",
            " 25400K .......... .......... .......... .......... .......... 44%  143M 0s\n",
            " 25450K .......... .......... .......... .......... .......... 44%  143M 0s\n",
            " 25500K .......... .......... .......... .......... .......... 44%  143M 0s\n",
            " 25550K .......... .......... .......... .......... .......... 44%  147M 0s\n",
            " 25600K .......... .......... .......... .......... .......... 44%  142M 0s\n",
            " 25650K .......... .......... .......... .......... .......... 45%  145M 0s\n",
            " 25700K .......... .......... .......... .......... .......... 45%  142M 0s\n",
            " 25750K .......... .......... .......... .......... .......... 45%  142M 0s\n",
            " 25800K .......... .......... .......... .......... .......... 45%  135M 0s\n",
            " 25850K .......... .......... .......... .......... .......... 45%  122M 0s\n",
            " 25900K .......... .......... .......... .......... .......... 45%  134M 0s\n",
            " 25950K .......... .......... .......... .......... .......... 45%  121M 0s\n",
            " 26000K .......... .......... .......... .......... .......... 45%  167M 0s\n",
            " 26050K .......... .......... .......... .......... .......... 45%  147M 0s\n",
            " 26100K .......... .......... .......... .......... .......... 45%  145M 0s\n",
            " 26150K .......... .......... .......... .......... .......... 45%  133M 0s\n",
            " 26200K .......... .......... .......... .......... .......... 45%  158M 0s\n",
            " 26250K .......... .......... .......... .......... .......... 46%  144M 0s\n",
            " 26300K .......... .......... .......... .......... .......... 46%  120M 0s\n",
            " 26350K .......... .......... .......... .......... .......... 46%  121M 0s\n",
            " 26400K .......... .......... .......... .......... .......... 46%  154M 0s\n",
            " 26450K .......... .......... .......... .......... .......... 46%  141M 0s\n",
            " 26500K .......... .......... .......... .......... .......... 46%  124M 0s\n",
            " 26550K .......... .......... .......... .......... .......... 46%  101M 0s\n",
            " 26600K .......... .......... .......... .......... .......... 46%  137M 0s\n",
            " 26650K .......... .......... .......... .......... .......... 46%  150M 0s\n",
            " 26700K .......... .......... .......... .......... .......... 46%  132M 0s\n",
            " 26750K .......... .......... .......... .......... .......... 46% 97.7M 0s\n",
            " 26800K .......... .......... .......... .......... .......... 47%  156M 0s\n",
            " 26850K .......... .......... .......... .......... .......... 47%  147M 0s\n",
            " 26900K .......... .......... .......... .......... .......... 47%  128M 0s\n",
            " 26950K .......... .......... .......... .......... .......... 47%  140M 0s\n",
            " 27000K .......... .......... .......... .......... .......... 47%  125M 0s\n",
            " 27050K .......... .......... .......... .......... .......... 47%  159M 0s\n",
            " 27100K .......... .......... .......... .......... .......... 47%  163M 0s\n",
            " 27150K .......... .......... .......... .......... .......... 47%  126M 0s\n",
            " 27200K .......... .......... .......... .......... .......... 47%  152M 0s\n",
            " 27250K .......... .......... .......... .......... .......... 47%  129M 0s\n",
            " 27300K .......... .......... .......... .......... .......... 47%  131M 0s\n",
            " 27350K .......... .......... .......... .......... .......... 47%  123M 0s\n",
            " 27400K .......... .......... .......... .......... .......... 48%  146M 0s\n",
            " 27450K .......... .......... .......... .......... .......... 48%  130M 0s\n",
            " 27500K .......... .......... .......... .......... .......... 48%  157M 0s\n",
            " 27550K .......... .......... .......... .......... .......... 48%  135M 0s\n",
            " 27600K .......... .......... .......... .......... .......... 48%  150M 0s\n",
            " 27650K .......... .......... .......... .......... .......... 48%  161M 0s\n",
            " 27700K .......... .......... .......... .......... .......... 48%  141M 0s\n",
            " 27750K .......... .......... .......... .......... .......... 48%  139M 0s\n",
            " 27800K .......... .......... .......... .......... .......... 48%  140M 0s\n",
            " 27850K .......... .......... .......... .......... .......... 48%  151M 0s\n",
            " 27900K .......... .......... .......... .......... .......... 48%  154M 0s\n",
            " 27950K .......... .......... .......... .......... .......... 49%  131M 0s\n",
            " 28000K .......... .......... .......... .......... .......... 49%  150M 0s\n",
            " 28050K .......... .......... .......... .......... .......... 49%  151M 0s\n",
            " 28100K .......... .......... .......... .......... .......... 49%  165M 0s\n",
            " 28150K .......... .......... .......... .......... .......... 49%  137M 0s\n",
            " 28200K .......... .......... .......... .......... .......... 49%  136M 0s\n",
            " 28250K .......... .......... .......... .......... .......... 49%  147M 0s\n",
            " 28300K .......... .......... .......... .......... .......... 49%  144M 0s\n",
            " 28350K .......... .......... .......... .......... .......... 49%  125M 0s\n",
            " 28400K .......... .......... .......... .......... .......... 49%  162M 0s\n",
            " 28450K .......... .......... .......... .......... .......... 49%  153M 0s\n",
            " 28500K .......... .......... .......... .......... .......... 50%  124M 0s\n",
            " 28550K .......... .......... .......... .......... .......... 50%  139M 0s\n",
            " 28600K .......... .......... .......... .......... .......... 50%  138M 0s\n",
            " 28650K .......... .......... .......... .......... .......... 50%  130M 0s\n",
            " 28700K .......... .......... .......... .......... .......... 50%  125M 0s\n",
            " 28750K .......... .......... .......... .......... .......... 50%  138M 0s\n",
            " 28800K .......... .......... .......... .......... .......... 50%  155M 0s\n",
            " 28850K .......... .......... .......... .......... .......... 50%  151M 0s\n",
            " 28900K .......... .......... .......... .......... .......... 50%  125M 0s\n",
            " 28950K .......... .......... .......... .......... .......... 50%  140M 0s\n",
            " 29000K .......... .......... .......... .......... .......... 50%  134M 0s\n",
            " 29050K .......... .......... .......... .......... .......... 50%  133M 0s\n",
            " 29100K .......... .......... .......... .......... .......... 51%  167M 0s\n",
            " 29150K .......... .......... .......... .......... .......... 51%  121M 0s\n",
            " 29200K .......... .......... .......... .......... .......... 51%  153M 0s\n",
            " 29250K .......... .......... .......... .......... .......... 51%  145M 0s\n",
            " 29300K .......... .......... .......... .......... .......... 51%  143M 0s\n",
            " 29350K .......... .......... .......... .......... .......... 51%  120M 0s\n",
            " 29400K .......... .......... .......... .......... .......... 51%  131M 0s\n",
            " 29450K .......... .......... .......... .......... .......... 51%  132M 0s\n",
            " 29500K .......... .......... .......... .......... .......... 51%  150M 0s\n",
            " 29550K .......... .......... .......... .......... .......... 51%  140M 0s\n",
            " 29600K .......... .......... .......... .......... .......... 51%  147M 0s\n",
            " 29650K .......... .......... .......... .......... .......... 52%  145M 0s\n",
            " 29700K .......... .......... .......... .......... .......... 52%  159M 0s\n",
            " 29750K .......... .......... .......... .......... .......... 52%  116M 0s\n",
            " 29800K .......... .......... .......... .......... .......... 52%  139M 0s\n",
            " 29850K .......... .......... .......... .......... .......... 52%  161M 0s\n",
            " 29900K .......... .......... .......... .......... .......... 52%  143M 0s\n",
            " 29950K .......... .......... .......... .......... .......... 52%  130M 0s\n",
            " 30000K .......... .......... .......... .......... .......... 52%  130M 0s\n",
            " 30050K .......... .......... .......... .......... .......... 52%  148M 0s\n",
            " 30100K .......... .......... .......... .......... .......... 52%  144M 0s\n",
            " 30150K .......... .......... .......... .......... .......... 52%  113M 0s\n",
            " 30200K .......... .......... .......... .......... .......... 52%  159M 0s\n",
            " 30250K .......... .......... .......... .......... .......... 53%  143M 0s\n",
            " 30300K .......... .......... .......... .......... .......... 53%  130M 0s\n",
            " 30350K .......... .......... .......... .......... .......... 53%  136M 0s\n",
            " 30400K .......... .......... .......... .......... .......... 53%  142M 0s\n",
            " 30450K .......... .......... .......... .......... .......... 53%  112M 0s\n",
            " 30500K .......... .......... .......... .......... .......... 53%  147M 0s\n",
            " 30550K .......... .......... .......... .......... .......... 53%  140M 0s\n",
            " 30600K .......... .......... .......... .......... .......... 53%  141M 0s\n",
            " 30650K .......... .......... .......... .......... .......... 53%  168M 0s\n",
            " 30700K .......... .......... .......... .......... .......... 53%  145M 0s\n",
            " 30750K .......... .......... .......... .......... .......... 53%  124M 0s\n",
            " 30800K .......... .......... .......... .......... .......... 54%  150M 0s\n",
            " 30850K .......... .......... .......... .......... .......... 54%  129M 0s\n",
            " 30900K .......... .......... .......... .......... .......... 54%  151M 0s\n",
            " 30950K .......... .......... .......... .......... .......... 54%  126M 0s\n",
            " 31000K .......... .......... .......... .......... .......... 54%  163M 0s\n",
            " 31050K .......... .......... .......... .......... .......... 54%  149M 0s\n",
            " 31100K .......... .......... .......... .......... .......... 54%  124M 0s\n",
            " 31150K .......... .......... .......... .......... .......... 54%  134M 0s\n",
            " 31200K .......... .......... .......... .......... .......... 54%  148M 0s\n",
            " 31250K .......... .......... .......... .......... .......... 54%  128M 0s\n",
            " 31300K .......... .......... .......... .......... .......... 54%  149M 0s\n",
            " 31350K .......... .......... .......... .......... .......... 54%  141M 0s\n",
            " 31400K .......... .......... .......... .......... .......... 55%  130M 0s\n",
            " 31450K .......... .......... .......... .......... .......... 55%  149M 0s\n",
            " 31500K .......... .......... .......... .......... .......... 55%  153M 0s\n",
            " 31550K .......... .......... .......... .......... .......... 55%  124M 0s\n",
            " 31600K .......... .......... .......... .......... .......... 55%  129M 0s\n",
            " 31650K .......... .......... .......... .......... .......... 55%  152M 0s\n",
            " 31700K .......... .......... .......... .......... .......... 55%  155M 0s\n",
            " 31750K .......... .......... .......... .......... .......... 55%  112M 0s\n",
            " 31800K .......... .......... .......... .......... .......... 55%  183M 0s\n",
            " 31850K .......... .......... .......... .......... .......... 55%  146M 0s\n",
            " 31900K .......... .......... .......... .......... .......... 55%  153M 0s\n",
            " 31950K .......... .......... .......... .......... .......... 56%  125M 0s\n",
            " 32000K .......... .......... .......... .......... .......... 56%  154M 0s\n",
            " 32050K .......... .......... .......... .......... .......... 56%  151M 0s\n",
            " 32100K .......... .......... .......... .......... .......... 56%  136M 0s\n",
            " 32150K .......... .......... .......... .......... .......... 56%  123M 0s\n",
            " 32200K .......... .......... .......... .......... .......... 56%  139M 0s\n",
            " 32250K .......... .......... .......... .......... .......... 56%  150M 0s\n",
            " 32300K .......... .......... .......... .......... .......... 56%  160M 0s\n",
            " 32350K .......... .......... .......... .......... .......... 56%  128M 0s\n",
            " 32400K .......... .......... .......... .......... .......... 56%  153M 0s\n",
            " 32450K .......... .......... .......... .......... .......... 56%  145M 0s\n",
            " 32500K .......... .......... .......... .......... .......... 57%  126M 0s\n",
            " 32550K .......... .......... .......... .......... .......... 57%  120M 0s\n",
            " 32600K .......... .......... .......... .......... .......... 57%  122M 0s\n",
            " 32650K .......... .......... .......... .......... .......... 57%  156M 0s\n",
            " 32700K .......... .......... .......... .......... .......... 57%  150M 0s\n",
            " 32750K .......... .......... .......... .......... .......... 57%  130M 0s\n",
            " 32800K .......... .......... .......... .......... .......... 57%  136M 0s\n",
            " 32850K .......... .......... .......... .......... .......... 57%  141M 0s\n",
            " 32900K .......... .......... .......... .......... .......... 57%  136M 0s\n",
            " 32950K .......... .......... .......... .......... .......... 57%  137M 0s\n",
            " 33000K .......... .......... .......... .......... .......... 57%  164M 0s\n",
            " 33050K .......... .......... .......... .......... .......... 57%  128M 0s\n",
            " 33100K .......... .......... .......... .......... .......... 58%  137M 0s\n",
            " 33150K .......... .......... .......... .......... .......... 58%  114M 0s\n",
            " 33200K .......... .......... .......... .......... .......... 58%  144M 0s\n",
            " 33250K .......... .......... .......... .......... .......... 58%  150M 0s\n",
            " 33300K .......... .......... .......... .......... .......... 58%  140M 0s\n",
            " 33350K .......... .......... .......... .......... .......... 58%  149M 0s\n",
            " 33400K .......... .......... .......... .......... .......... 58%  149M 0s\n",
            " 33450K .......... .......... .......... .......... .......... 58%  139M 0s\n",
            " 33500K .......... .......... .......... .......... .......... 58%  161M 0s\n",
            " 33550K .......... .......... .......... .......... .......... 58%  125M 0s\n",
            " 33600K .......... .......... .......... .......... .......... 58%  150M 0s\n",
            " 33650K .......... .......... .......... .......... .......... 59%  152M 0s\n",
            " 33700K .......... .......... .......... .......... .......... 59%  160M 0s\n",
            " 33750K .......... .......... .......... .......... .......... 59%  119M 0s\n",
            " 33800K .......... .......... .......... .......... .......... 59%  155M 0s\n",
            " 33850K .......... .......... .......... .......... .......... 59%  155M 0s\n",
            " 33900K .......... .......... .......... .......... .......... 59%  145M 0s\n",
            " 33950K .......... .......... .......... .......... .......... 59%  136M 0s\n",
            " 34000K .......... .......... .......... .......... .......... 59%  162M 0s\n",
            " 34050K .......... .......... .......... .......... .......... 59%  128M 0s\n",
            " 34100K .......... .......... .......... .......... .......... 59%  165M 0s\n",
            " 34150K .......... .......... .......... .......... .......... 59%  123M 0s\n",
            " 34200K .......... .......... .......... .......... .......... 59%  153M 0s\n",
            " 34250K .......... .......... .......... .......... .......... 60%  134M 0s\n",
            " 34300K .......... .......... .......... .......... .......... 60%  145M 0s\n",
            " 34350K .......... .......... .......... .......... .......... 60%  134M 0s\n",
            " 34400K .......... .......... .......... .......... .......... 60%  146M 0s\n",
            " 34450K .......... .......... .......... .......... .......... 60%  129M 0s\n",
            " 34500K .......... .......... .......... .......... .......... 60%  156M 0s\n",
            " 34550K .......... .......... .......... .......... .......... 60%  130M 0s\n",
            " 34600K .......... .......... .......... .......... .......... 60%  133M 0s\n",
            " 34650K .......... .......... .......... .......... .......... 60%  138M 0s\n",
            " 34700K .......... .......... .......... .......... .......... 60%  159M 0s\n",
            " 34750K .......... .......... .......... .......... .......... 60%  116M 0s\n",
            " 34800K .......... .......... .......... .......... .......... 61%  144M 0s\n",
            " 34850K .......... .......... .......... .......... .......... 61%  164M 0s\n",
            " 34900K .......... .......... .......... .......... .......... 61%  157M 0s\n",
            " 34950K .......... .......... .......... .......... .......... 61%  124M 0s\n",
            " 35000K .......... .......... .......... .......... .......... 61%  163M 0s\n",
            " 35050K .......... .......... .......... .......... .......... 61%  144M 0s\n",
            " 35100K .......... .......... .......... .......... .......... 61%  145M 0s\n",
            " 35150K .......... .......... .......... .......... .......... 61%  127M 0s\n",
            " 35200K .......... .......... .......... .......... .......... 61%  153M 0s\n",
            " 35250K .......... .......... .......... .......... .......... 61%  145M 0s\n",
            " 35300K .......... .......... .......... .......... .......... 61%  133M 0s\n",
            " 35350K .......... .......... .......... .......... .......... 61%  127M 0s\n",
            " 35400K .......... .......... .......... .......... .......... 62%  159M 0s\n",
            " 35450K .......... .......... .......... .......... .......... 62%  141M 0s\n",
            " 35500K .......... .......... .......... .......... .......... 62%  156M 0s\n",
            " 35550K .......... .......... .......... .......... .......... 62%  126M 0s\n",
            " 35600K .......... .......... .......... .......... .......... 62%  110M 0s\n",
            " 35650K .......... .......... .......... .......... .......... 62%  158M 0s\n",
            " 35700K .......... .......... .......... .......... .......... 62%  159M 0s\n",
            " 35750K .......... .......... .......... .......... .......... 62%  144M 0s\n",
            " 35800K .......... .......... .......... .......... .......... 62%  140M 0s\n",
            " 35850K .......... .......... .......... .......... .......... 62%  147M 0s\n",
            " 35900K .......... .......... .......... .......... .......... 62%  155M 0s\n",
            " 35950K .......... .......... .......... .......... .......... 63%  106M 0s\n",
            " 36000K .......... .......... .......... .......... .......... 63%  150M 0s\n",
            " 36050K .......... .......... .......... .......... .......... 63%  149M 0s\n",
            " 36100K .......... .......... .......... .......... .......... 63%  142M 0s\n",
            " 36150K .......... .......... .......... .......... .......... 63%  145M 0s\n",
            " 36200K .......... .......... .......... .......... .......... 63%  155M 0s\n",
            " 36250K .......... .......... .......... .......... .......... 63%  169M 0s\n",
            " 36300K .......... .......... .......... .......... .......... 63%  115M 0s\n",
            " 36350K .......... .......... .......... .......... .......... 63%  124M 0s\n",
            " 36400K .......... .......... .......... .......... .......... 63%  139M 0s\n",
            " 36450K .......... .......... .......... .......... .......... 63%  145M 0s\n",
            " 36500K .......... .......... .......... .......... .......... 64%  157M 0s\n",
            " 36550K .......... .......... .......... .......... .......... 64%  111M 0s\n",
            " 36600K .......... .......... .......... .......... .......... 64%  144M 0s\n",
            " 36650K .......... .......... .......... .......... .......... 64%  148M 0s\n",
            " 36700K .......... .......... .......... .......... .......... 64%  162M 0s\n",
            " 36750K .......... .......... .......... .......... .......... 64%  131M 0s\n",
            " 36800K .......... .......... .......... .......... .......... 64%  149M 0s\n",
            " 36850K .......... .......... .......... .......... .......... 64%  148M 0s\n",
            " 36900K .......... .......... .......... .......... .......... 64%  158M 0s\n",
            " 36950K .......... .......... .......... .......... .......... 64%  117M 0s\n",
            " 37000K .......... .......... .......... .......... .......... 64%  132M 0s\n",
            " 37050K .......... .......... .......... .......... .......... 64%  133M 0s\n",
            " 37100K .......... .......... .......... .......... .......... 65%  165M 0s\n",
            " 37150K .......... .......... .......... .......... .......... 65%  119M 0s\n",
            " 37200K .......... .......... .......... .......... .......... 65%  142M 0s\n",
            " 37250K .......... .......... .......... .......... .......... 65%  115M 0s\n",
            " 37300K .......... .......... .......... .......... .......... 65%  133M 0s\n",
            " 37350K .......... .......... .......... .......... .......... 65%  106M 0s\n",
            " 37400K .......... .......... .......... .......... .......... 65%  139M 0s\n",
            " 37450K .......... .......... .......... .......... .......... 65%  149M 0s\n",
            " 37500K .......... .......... .......... .......... .......... 65%  130M 0s\n",
            " 37550K .......... .......... .......... .......... .......... 65%  117M 0s\n",
            " 37600K .......... .......... .......... .......... .......... 65%  150M 0s\n",
            " 37650K .......... .......... .......... .......... .......... 66%  163M 0s\n",
            " 37700K .......... .......... .......... .......... .......... 66%  152M 0s\n",
            " 37750K .......... .......... .......... .......... .......... 66%  116M 0s\n",
            " 37800K .......... .......... .......... .......... .......... 66%  153M 0s\n",
            " 37850K .......... .......... .......... .......... .......... 66%  145M 0s\n",
            " 37900K .......... .......... .......... .......... .......... 66%  152M 0s\n",
            " 37950K .......... .......... .......... .......... .......... 66%  147M 0s\n",
            " 38000K .......... .......... .......... .......... .......... 66%  141M 0s\n",
            " 38050K .......... .......... .......... .......... .......... 66%  135M 0s\n",
            " 38100K .......... .......... .......... .......... .......... 66%  150M 0s\n",
            " 38150K .......... .......... .......... .......... .......... 66%  134M 0s\n",
            " 38200K .......... .......... .......... .......... .......... 66%  147M 0s\n",
            " 38250K .......... .......... .......... .......... .......... 67%  134M 0s\n",
            " 38300K .......... .......... .......... .......... .......... 67%  163M 0s\n",
            " 38350K .......... .......... .......... .......... .......... 67%  115M 0s\n",
            " 38400K .......... .......... .......... .......... .......... 67%  156M 0s\n",
            " 38450K .......... .......... .......... .......... .......... 67%  163M 0s\n",
            " 38500K .......... .......... .......... .......... .......... 67%  138M 0s\n",
            " 38550K .......... .......... .......... .......... .......... 67%  141M 0s\n",
            " 38600K .......... .......... .......... .......... .......... 67%  160M 0s\n",
            " 38650K .......... .......... .......... .......... .......... 67%  138M 0s\n",
            " 38700K .......... .......... .......... .......... .......... 67%  152M 0s\n",
            " 38750K .......... .......... .......... .......... .......... 67%  116M 0s\n",
            " 38800K .......... .......... .......... .......... .......... 68%  155M 0s\n",
            " 38850K .......... .......... .......... .......... .......... 68%  152M 0s\n",
            " 38900K .......... .......... .......... .......... .......... 68%  159M 0s\n",
            " 38950K .......... .......... .......... .......... .......... 68%  146M 0s\n",
            " 39000K .......... .......... .......... .......... .......... 68%  147M 0s\n",
            " 39050K .......... .......... .......... .......... .......... 68%  144M 0s\n",
            " 39100K .......... .......... .......... .......... .......... 68%  153M 0s\n",
            " 39150K .......... .......... .......... .......... .......... 68%  126M 0s\n",
            " 39200K .......... .......... .......... .......... .......... 68%  150M 0s\n",
            " 39250K .......... .......... .......... .......... .......... 68%  127M 0s\n",
            " 39300K .......... .......... .......... .......... .......... 68%  138M 0s\n",
            " 39350K .......... .......... .......... .......... .......... 69%  121M 0s\n",
            " 39400K .......... .......... .......... .......... .......... 69%  162M 0s\n",
            " 39450K .......... .......... .......... .......... .......... 69%  136M 0s\n",
            " 39500K .......... .......... .......... .......... .......... 69%  119M 0s\n",
            " 39550K .......... .......... .......... .......... .......... 69%  132M 0s\n",
            " 39600K .......... .......... .......... .......... .......... 69%  163M 0s\n",
            " 39650K .......... .......... .......... .......... .......... 69%  149M 0s\n",
            " 39700K .......... .......... .......... .......... .......... 69%  156M 0s\n",
            " 39750K .......... .......... .......... .......... .......... 69%  124M 0s\n",
            " 39800K .......... .......... .......... .......... .......... 69%  131M 0s\n",
            " 39850K .......... .......... .......... .......... .......... 69%  159M 0s\n",
            " 39900K .......... .......... .......... .......... .......... 69%  161M 0s\n",
            " 39950K .......... .......... .......... .......... .......... 70%  122M 0s\n",
            " 40000K .......... .......... .......... .......... .......... 70%  150M 0s\n",
            " 40050K .......... .......... .......... .......... .......... 70%  144M 0s\n",
            " 40100K .......... .......... .......... .......... .......... 70%  164M 0s\n",
            " 40150K .......... .......... .......... .......... .......... 70%  121M 0s\n",
            " 40200K .......... .......... .......... .......... .......... 70%  125M 0s\n",
            " 40250K .......... .......... .......... .......... .......... 70%  153M 0s\n",
            " 40300K .......... .......... .......... .......... .......... 70%  144M 0s\n",
            " 40350K .......... .......... .......... .......... .......... 70%  124M 0s\n",
            " 40400K .......... .......... .......... .......... .......... 70%  161M 0s\n",
            " 40450K .......... .......... .......... .......... .......... 70%  143M 0s\n",
            " 40500K .......... .......... .......... .......... .......... 71%  133M 0s\n",
            " 40550K .......... .......... .......... .......... .......... 71%  144M 0s\n",
            " 40600K .......... .......... .......... .......... .......... 71%  162M 0s\n",
            " 40650K .......... .......... .......... .......... .......... 71%  134M 0s\n",
            " 40700K .......... .......... .......... .......... .......... 71%  128M 0s\n",
            " 40750K .......... .......... .......... .......... .......... 71%  129M 0s\n",
            " 40800K .......... .......... .......... .......... .......... 71%  143M 0s\n",
            " 40850K .......... .......... .......... .......... .......... 71%  167M 0s\n",
            " 40900K .......... .......... .......... .......... .......... 71%  161M 0s\n",
            " 40950K .......... .......... .......... .......... .......... 71%  121M 0s\n",
            " 41000K .......... .......... .......... .......... .......... 71%  130M 0s\n",
            " 41050K .......... .......... .......... .......... .......... 71%  158M 0s\n",
            " 41100K .......... .......... .......... .......... .......... 72%  150M 0s\n",
            " 41150K .......... .......... .......... .......... .......... 72%  120M 0s\n",
            " 41200K .......... .......... .......... .......... .......... 72%  114M 0s\n",
            " 41250K .......... .......... .......... .......... .......... 72%  160M 0s\n",
            " 41300K .......... .......... .......... .......... .......... 72%  152M 0s\n",
            " 41350K .......... .......... .......... .......... .......... 72%  121M 0s\n",
            " 41400K .......... .......... .......... .......... .......... 72%  150M 0s\n",
            " 41450K .......... .......... .......... .......... .......... 72%  139M 0s\n",
            " 41500K .......... .......... .......... .......... .......... 72%  135M 0s\n",
            " 41550K .......... .......... .......... .......... .......... 72%  102M 0s\n",
            " 41600K .......... .......... .......... .......... .......... 72%  156M 0s\n",
            " 41650K .......... .......... .......... .......... .......... 73%  138M 0s\n",
            " 41700K .......... .......... .......... .......... .......... 73%  150M 0s\n",
            " 41750K .......... .......... .......... .......... .......... 73%  112M 0s\n",
            " 41800K .......... .......... .......... .......... .......... 73%  149M 0s\n",
            " 41850K .......... .......... .......... .......... .......... 73%  149M 0s\n",
            " 41900K .......... .......... .......... .......... .......... 73%  134M 0s\n",
            " 41950K .......... .......... .......... .......... .......... 73%  140M 0s\n",
            " 42000K .......... .......... .......... .......... .......... 73%  135M 0s\n",
            " 42050K .......... .......... .......... .......... .......... 73%  134M 0s\n",
            " 42100K .......... .......... .......... .......... .......... 73%  150M 0s\n",
            " 42150K .......... .......... .......... .......... .......... 73%  118M 0s\n",
            " 42200K .......... .......... .......... .......... .......... 73%  144M 0s\n",
            " 42250K .......... .......... .......... .......... .......... 74%  154M 0s\n",
            " 42300K .......... .......... .......... .......... .......... 74%  155M 0s\n",
            " 42350K .......... .......... .......... .......... .......... 74%  126M 0s\n",
            " 42400K .......... .......... .......... .......... .......... 74%  149M 0s\n",
            " 42450K .......... .......... .......... .......... .......... 74%  114M 0s\n",
            " 42500K .......... .......... .......... .......... .......... 74%  161M 0s\n",
            " 42550K .......... .......... .......... .......... .......... 74%  121M 0s\n",
            " 42600K .......... .......... .......... .......... .......... 74%  141M 0s\n",
            " 42650K .......... .......... .......... .......... .......... 74%  170M 0s\n",
            " 42700K .......... .......... .......... .......... .......... 74%  165M 0s\n",
            " 42750K .......... .......... .......... .......... .......... 74%  119M 0s\n",
            " 42800K .......... .......... .......... .......... .......... 75%  153M 0s\n",
            " 42850K .......... .......... .......... .......... .......... 75%  139M 0s\n",
            " 42900K .......... .......... .......... .......... .......... 75%  152M 0s\n",
            " 42950K .......... .......... .......... .......... .......... 75%  136M 0s\n",
            " 43000K .......... .......... .......... .......... .......... 75%  151M 0s\n",
            " 43050K .......... .......... .......... .......... .......... 75%  144M 0s\n",
            " 43100K .......... .......... .......... .......... .......... 75%  145M 0s\n",
            " 43150K .......... .......... .......... .......... .......... 75%  131M 0s\n",
            " 43200K .......... .......... .......... .......... .......... 75%  139M 0s\n",
            " 43250K .......... .......... .......... .......... .......... 75%  150M 0s\n",
            " 43300K .......... .......... .......... .......... .......... 75%  148M 0s\n",
            " 43350K .......... .......... .......... .......... .......... 76%  123M 0s\n",
            " 43400K .......... .......... .......... .......... .......... 76%  159M 0s\n",
            " 43450K .......... .......... .......... .......... .......... 76%  128M 0s\n",
            " 43500K .......... .......... .......... .......... .......... 76%  131M 0s\n",
            " 43550K .......... .......... .......... .......... .......... 76%  146M 0s\n",
            " 43600K .......... .......... .......... .......... .......... 76%  150M 0s\n",
            " 43650K .......... .......... .......... .......... .......... 76%  141M 0s\n",
            " 43700K .......... .......... .......... .......... .......... 76%  145M 0s\n",
            " 43750K .......... .......... .......... .......... .......... 76%  117M 0s\n",
            " 43800K .......... .......... .......... .......... .......... 76%  151M 0s\n",
            " 43850K .......... .......... .......... .......... .......... 76%  148M 0s\n",
            " 43900K .......... .......... .......... .......... .......... 76%  138M 0s\n",
            " 43950K .......... .......... .......... .......... .......... 77%  127M 0s\n",
            " 44000K .......... .......... .......... .......... .......... 77%  148M 0s\n",
            " 44050K .......... .......... .......... .......... .......... 77%  132M 0s\n",
            " 44100K .......... .......... .......... .......... .......... 77%  142M 0s\n",
            " 44150K .......... .......... .......... .......... .......... 77%  142M 0s\n",
            " 44200K .......... .......... .......... .......... .......... 77%  162M 0s\n",
            " 44250K .......... .......... .......... .......... .......... 77%  150M 0s\n",
            " 44300K .......... .......... .......... .......... .......... 77%  143M 0s\n",
            " 44350K .......... .......... .......... .......... .......... 77%  127M 0s\n",
            " 44400K .......... .......... .......... .......... .......... 77%  175M 0s\n",
            " 44450K .......... .......... .......... .......... .......... 77%  149M 0s\n",
            " 44500K .......... .......... .......... .......... .......... 78%  135M 0s\n",
            " 44550K .......... .......... .......... .......... .......... 78%  144M 0s\n",
            " 44600K .......... .......... .......... .......... .......... 78%  164M 0s\n",
            " 44650K .......... .......... .......... .......... .......... 78%  144M 0s\n",
            " 44700K .......... .......... .......... .......... .......... 78%  146M 0s\n",
            " 44750K .......... .......... .......... .......... .......... 78%  112M 0s\n",
            " 44800K .......... .......... .......... .......... .......... 78%  131M 0s\n",
            " 44850K .......... .......... .......... .......... .......... 78%  158M 0s\n",
            " 44900K .......... .......... .......... .......... .......... 78%  162M 0s\n",
            " 44950K .......... .......... .......... .......... .......... 78%  127M 0s\n",
            " 45000K .......... .......... .......... .......... .......... 78%  124M 0s\n",
            " 45050K .......... .......... .......... .......... .......... 78%  157M 0s\n",
            " 45100K .......... .......... .......... .......... .......... 79%  124M 0s\n",
            " 45150K .......... .......... .......... .......... .......... 79%  123M 0s\n",
            " 45200K .......... .......... .......... .......... .......... 79%  152M 0s\n",
            " 45250K .......... .......... .......... .......... .......... 79%  154M 0s\n",
            " 45300K .......... .......... .......... .......... .......... 79%  145M 0s\n",
            " 45350K .......... .......... .......... .......... .......... 79%  139M 0s\n",
            " 45400K .......... .......... .......... .......... .......... 79%  151M 0s\n",
            " 45450K .......... .......... .......... .......... .......... 79%  132M 0s\n",
            " 45500K .......... .......... .......... .......... .......... 79%  141M 0s\n",
            " 45550K .......... .......... .......... .......... .......... 79%  132M 0s\n",
            " 45600K .......... .......... .......... .......... .......... 79%  143M 0s\n",
            " 45650K .......... .......... .......... .......... .......... 80%  159M 0s\n",
            " 45700K .......... .......... .......... .......... .......... 80%  122M 0s\n",
            " 45750K .......... .......... .......... .......... .......... 80%  128M 0s\n",
            " 45800K .......... .......... .......... .......... .......... 80%  153M 0s\n",
            " 45850K .......... .......... .......... .......... .......... 80%  145M 0s\n",
            " 45900K .......... .......... .......... .......... .......... 80%  121M 0s\n",
            " 45950K .......... .......... .......... .......... .......... 80%  115M 0s\n",
            " 46000K .......... .......... .......... .......... .......... 80%  164M 0s\n",
            " 46050K .......... .......... .......... .......... .......... 80%  126M 0s\n",
            " 46100K .......... .......... .......... .......... .......... 80%  129M 0s\n",
            " 46150K .......... .......... .......... .......... .......... 80%  134M 0s\n",
            " 46200K .......... .......... .......... .......... .......... 81%  132M 0s\n",
            " 46250K .......... .......... .......... .......... .......... 81%  166M 0s\n",
            " 46300K .......... .......... .......... .......... .......... 81%  147M 0s\n",
            " 46350K .......... .......... .......... .......... .......... 81%  118M 0s\n",
            " 46400K .......... .......... .......... .......... .......... 81%  135M 0s\n",
            " 46450K .......... .......... .......... .......... .......... 81%  157M 0s\n",
            " 46500K .......... .......... .......... .......... .......... 81%  156M 0s\n",
            " 46550K .......... .......... .......... .......... .......... 81%  114M 0s\n",
            " 46600K .......... .......... .......... .......... .......... 81%  147M 0s\n",
            " 46650K .......... .......... .......... .......... .......... 81%  140M 0s\n",
            " 46700K .......... .......... .......... .......... .......... 81%  167M 0s\n",
            " 46750K .......... .......... .......... .......... .......... 81%  137M 0s\n",
            " 46800K .......... .......... .......... .......... .......... 82%  157M 0s\n",
            " 46850K .......... .......... .......... .......... .......... 82%  144M 0s\n",
            " 46900K .......... .......... .......... .......... .......... 82%  141M 0s\n",
            " 46950K .......... .......... .......... .......... .......... 82%  136M 0s\n",
            " 47000K .......... .......... .......... .......... .......... 82%  156M 0s\n",
            " 47050K .......... .......... .......... .......... .......... 82%  136M 0s\n",
            " 47100K .......... .......... .......... .......... .......... 82%  145M 0s\n",
            " 47150K .......... .......... .......... .......... .......... 82%  112M 0s\n",
            " 47200K .......... .......... .......... .......... .......... 82%  141M 0s\n",
            " 47250K .......... .......... .......... .......... .......... 82%  165M 0s\n",
            " 47300K .......... .......... .......... .......... .......... 82%  161M 0s\n",
            " 47350K .......... .......... .......... .......... .......... 83%  122M 0s\n",
            " 47400K .......... .......... .......... .......... .......... 83%  154M 0s\n",
            " 47450K .......... .......... .......... .......... .......... 83%  164M 0s\n",
            " 47500K .......... .......... .......... .......... .......... 83%  160M 0s\n",
            " 47550K .......... .......... .......... .......... .......... 83%  127M 0s\n",
            " 47600K .......... .......... .......... .......... .......... 83%  140M 0s\n",
            " 47650K .......... .......... .......... .......... .......... 83%  152M 0s\n",
            " 47700K .......... .......... .......... .......... .......... 83%  148M 0s\n",
            " 47750K .......... .......... .......... .......... .......... 83%  137M 0s\n",
            " 47800K .......... .......... .......... .......... .......... 83%  153M 0s\n",
            " 47850K .......... .......... .......... .......... .......... 83%  140M 0s\n",
            " 47900K .......... .......... .......... .......... .......... 83%  148M 0s\n",
            " 47950K .......... .......... .......... .......... .......... 84%  129M 0s\n",
            " 48000K .......... .......... .......... .......... .......... 84%  158M 0s\n",
            " 48050K .......... .......... .......... .......... .......... 84%  134M 0s\n",
            " 48100K .......... .......... .......... .......... .......... 84%  141M 0s\n",
            " 48150K .......... .......... .......... .......... .......... 84%  148M 0s\n",
            " 48200K .......... .......... .......... .......... .......... 84%  128M 0s\n",
            " 48250K .......... .......... .......... .......... .......... 84%  156M 0s\n",
            " 48300K .......... .......... .......... .......... .......... 84%  158M 0s\n",
            " 48350K .......... .......... .......... .......... .......... 84% 69.1M 0s\n",
            " 48400K .......... .......... .......... .......... .......... 84%  145M 0s\n",
            " 48450K .......... .......... .......... .......... .......... 84%  162M 0s\n",
            " 48500K .......... .......... .......... .......... .......... 85%  152M 0s\n",
            " 48550K .......... .......... .......... .......... .......... 85% 53.6M 0s\n",
            " 48600K .......... .......... .......... .......... .......... 85%  157M 0s\n",
            " 48650K .......... .......... .......... .......... .......... 85%  183M 0s\n",
            " 48700K .......... .......... .......... .......... .......... 85%  173M 0s\n",
            " 48750K .......... .......... .......... .......... .......... 85%  111M 0s\n",
            " 48800K .......... .......... .......... .......... .......... 85%  159M 0s\n",
            " 48850K .......... .......... .......... .......... .......... 85%  146M 0s\n",
            " 48900K .......... .......... .......... .......... .......... 85%  162M 0s\n",
            " 48950K .......... .......... .......... .......... .......... 85%  126M 0s\n",
            " 49000K .......... .......... .......... .......... .......... 85%  185M 0s\n",
            " 49050K .......... .......... .......... .......... .......... 85%  171M 0s\n",
            " 49100K .......... .......... .......... .......... .......... 86%  156M 0s\n",
            " 49150K .......... .......... .......... .......... .......... 86%  135M 0s\n",
            " 49200K .......... .......... .......... .......... .......... 86%  171M 0s\n",
            " 49250K .......... .......... .......... .......... .......... 86%  175M 0s\n",
            " 49300K .......... .......... .......... .......... .......... 86%  161M 0s\n",
            " 49350K .......... .......... .......... .......... .......... 86%  145M 0s\n",
            " 49400K .......... .......... .......... .......... .......... 86%  186M 0s\n",
            " 49450K .......... .......... .......... .......... .......... 86%  169M 0s\n",
            " 49500K .......... .......... .......... .......... .......... 86%  139M 0s\n",
            " 49550K .......... .......... .......... .......... .......... 86%  128M 0s\n",
            " 49600K .......... .......... .......... .......... .......... 86%  152M 0s\n",
            " 49650K .......... .......... .......... .......... .......... 87%  168M 0s\n",
            " 49700K .......... .......... .......... .......... .......... 87%  183M 0s\n",
            " 49750K .......... .......... .......... .......... .......... 87%  139M 0s\n",
            " 49800K .......... .......... .......... .......... .......... 87%  124M 0s\n",
            " 49850K .......... .......... .......... .......... .......... 87%  138M 0s\n",
            " 49900K .......... .......... .......... .......... .......... 87%  147M 0s\n",
            " 49950K .......... .......... .......... .......... .......... 87%  127M 0s\n",
            " 50000K .......... .......... .......... .......... .......... 87%  153M 0s\n",
            " 50050K .......... .......... .......... .......... .......... 87%  141M 0s\n",
            " 50100K .......... .......... .......... .......... .......... 87% 32.1M 0s\n",
            " 50150K .......... .......... .......... .......... .......... 87%  127M 0s\n",
            " 50200K .......... .......... .......... .......... .......... 88%  122M 0s\n",
            " 50250K .......... .......... .......... .......... .......... 88%  124M 0s\n",
            " 50300K .......... .......... .......... .......... .......... 88%  137M 0s\n",
            " 50350K .......... .......... .......... .......... .......... 88%  123M 0s\n",
            " 50400K .......... .......... .......... .......... .......... 88%  154M 0s\n",
            " 50450K .......... .......... .......... .......... .......... 88%  190M 0s\n",
            " 50500K .......... .......... .......... .......... .......... 88% 55.8M 0s\n",
            " 50550K .......... .......... .......... .......... .......... 88% 26.0M 0s\n",
            " 50600K .......... .......... .......... .......... .......... 88% 74.7M 0s\n",
            " 50650K .......... .......... .......... .......... .......... 88%  137M 0s\n",
            " 50700K .......... .......... .......... .......... .......... 88%  151M 0s\n",
            " 50750K .......... .......... .......... .......... .......... 88%  103M 0s\n",
            " 50800K .......... .......... .......... .......... .......... 89%  121M 0s\n",
            " 50850K .......... .......... .......... .......... .......... 89%  157M 0s\n",
            " 50900K .......... .......... .......... .......... .......... 89%  134M 0s\n",
            " 50950K .......... .......... .......... .......... .......... 89%  144M 0s\n",
            " 51000K .......... .......... .......... .......... .......... 89%  156M 0s\n",
            " 51050K .......... .......... .......... .......... .......... 89%  143M 0s\n",
            " 51100K .......... .......... .......... .......... .......... 89%  154M 0s\n",
            " 51150K .......... .......... .......... .......... .......... 89%  142M 0s\n",
            " 51200K .......... .......... .......... .......... .......... 89%  137M 0s\n",
            " 51250K .......... .......... .......... .......... .......... 89% 97.1M 0s\n",
            " 51300K .......... .......... .......... .......... .......... 89%  105M 0s\n",
            " 51350K .......... .......... .......... .......... .......... 90%  114M 0s\n",
            " 51400K .......... .......... .......... .......... .......... 90%  158M 0s\n",
            " 51450K .......... .......... .......... .......... .......... 90%  159M 0s\n",
            " 51500K .......... .......... .......... .......... .......... 90%  159M 0s\n",
            " 51550K .......... .......... .......... .......... .......... 90%  128M 0s\n",
            " 51600K .......... .......... .......... .......... .......... 90%  168M 0s\n",
            " 51650K .......... .......... .......... .......... .......... 90%  124M 0s\n",
            " 51700K .......... .......... .......... .......... .......... 90%  109M 0s\n",
            " 51750K .......... .......... .......... .......... .......... 90% 92.4M 0s\n",
            " 51800K .......... .......... .......... .......... .......... 90%  119M 0s\n",
            " 51850K .......... .......... .......... .......... .......... 90%  120M 0s\n",
            " 51900K .......... .......... .......... .......... .......... 90%  162M 0s\n",
            " 51950K .......... .......... .......... .......... .......... 91%  130M 0s\n",
            " 52000K .......... .......... .......... .......... .......... 91%  162M 0s\n",
            " 52050K .......... .......... .......... .......... .......... 91%  160M 0s\n",
            " 52100K .......... .......... .......... .......... .......... 91%  120M 0s\n",
            " 52150K .......... .......... .......... .......... .......... 91% 97.3M 0s\n",
            " 52200K .......... .......... .......... .......... .......... 91%  122M 0s\n",
            " 52250K .......... .......... .......... .......... .......... 91%  131M 0s\n",
            " 52300K .......... .......... .......... .......... .......... 91%  114M 0s\n",
            " 52350K .......... .......... .......... .......... .......... 91%  133M 0s\n",
            " 52400K .......... .......... .......... .......... .......... 91%  129M 0s\n",
            " 52450K .......... .......... .......... .......... .......... 91%  140M 0s\n",
            " 52500K .......... .......... .......... .......... .......... 92% 99.3M 0s\n",
            " 52550K .......... .......... .......... .......... .......... 92%  138M 0s\n",
            " 52600K .......... .......... .......... .......... .......... 92%  131M 0s\n",
            " 52650K .......... .......... .......... .......... .......... 92%  131M 0s\n",
            " 52700K .......... .......... .......... .......... .......... 92%  160M 0s\n",
            " 52750K .......... .......... .......... .......... .......... 92%  135M 0s\n",
            " 52800K .......... .......... .......... .......... .......... 92%  165M 0s\n",
            " 52850K .......... .......... .......... .......... .......... 92%  143M 0s\n",
            " 52900K .......... .......... .......... .......... .......... 92%  156M 0s\n",
            " 52950K .......... .......... .......... .......... .......... 92%  136M 0s\n",
            " 53000K .......... .......... .......... .......... .......... 92%  161M 0s\n",
            " 53050K .......... .......... .......... .......... .......... 92%  162M 0s\n",
            " 53100K .......... .......... .......... .......... .......... 93%  155M 0s\n",
            " 53150K .......... .......... .......... .......... .......... 93%  148M 0s\n",
            " 53200K .......... .......... .......... .......... .......... 93%  161M 0s\n",
            " 53250K .......... .......... .......... .......... .......... 93%  132M 0s\n",
            " 53300K .......... .......... .......... .......... .......... 93%  163M 0s\n",
            " 53350K .......... .......... .......... .......... .......... 93%  141M 0s\n",
            " 53400K .......... .......... .......... .......... .......... 93%  160M 0s\n",
            " 53450K .......... .......... .......... .......... .......... 93%  150M 0s\n",
            " 53500K .......... .......... .......... .......... .......... 93%  158M 0s\n",
            " 53550K .......... .......... .......... .......... .......... 93%  136M 0s\n",
            " 53600K .......... .......... .......... .......... .......... 93%  154M 0s\n",
            " 53650K .......... .......... .......... .......... .......... 94%  134M 0s\n",
            " 53700K .......... .......... .......... .......... .......... 94%  152M 0s\n",
            " 53750K .......... .......... .......... .......... .......... 94%  110M 0s\n",
            " 53800K .......... .......... .......... .......... .......... 94%  159M 0s\n",
            " 53850K .......... .......... .......... .......... .......... 94%  167M 0s\n",
            " 53900K .......... .......... .......... .......... .......... 94%  165M 0s\n",
            " 53950K .......... .......... .......... .......... .......... 94%  134M 0s\n",
            " 54000K .......... .......... .......... .......... .......... 94%  159M 0s\n",
            " 54050K .......... .......... .......... .......... .......... 94%  161M 0s\n",
            " 54100K .......... .......... .......... .......... .......... 94%  163M 0s\n",
            " 54150K .......... .......... .......... .......... .......... 94%  134M 0s\n",
            " 54200K .......... .......... .......... .......... .......... 95%  146M 0s\n",
            " 54250K .......... .......... .......... .......... .......... 95%  150M 0s\n",
            " 54300K .......... .......... .......... .......... .......... 95%  168M 0s\n",
            " 54350K .......... .......... .......... .......... .......... 95%  151M 0s\n",
            " 54400K .......... .......... .......... .......... .......... 95%  154M 0s\n",
            " 54450K .......... .......... .......... .......... .......... 95%  163M 0s\n",
            " 54500K .......... .......... .......... .......... .......... 95%  167M 0s\n",
            " 54550K .......... .......... .......... .......... .......... 95%  135M 0s\n",
            " 54600K .......... .......... .......... .......... .......... 95%  163M 0s\n",
            " 54650K .......... .......... .......... .......... .......... 95%  162M 0s\n",
            " 54700K .......... .......... .......... .......... .......... 95%  148M 0s\n",
            " 54750K .......... .......... .......... .......... .......... 95%  145M 0s\n",
            " 54800K .......... .......... .......... .......... .......... 96%  169M 0s\n",
            " 54850K .......... .......... .......... .......... .......... 96%  139M 0s\n",
            " 54900K .......... .......... .......... .......... .......... 96%  157M 0s\n",
            " 54950K .......... .......... .......... .......... .......... 96%  148M 0s\n",
            " 55000K .......... .......... .......... .......... .......... 96%  135M 0s\n",
            " 55050K .......... .......... .......... .......... .......... 96%  151M 0s\n",
            " 55100K .......... .......... .......... .......... .......... 96%  129M 0s\n",
            " 55150K .......... .......... .......... .......... .......... 96%  137M 0s\n",
            " 55200K .......... .......... .......... .......... .......... 96%  163M 0s\n",
            " 55250K .......... .......... .......... .......... .......... 96%  182M 0s\n",
            " 55300K .......... .......... .......... .......... .......... 96%  176M 0s\n",
            " 55350K .......... .......... .......... .......... .......... 97%  139M 0s\n",
            " 55400K .......... .......... .......... .......... .......... 97%  165M 0s\n",
            " 55450K .......... .......... .......... .......... .......... 97%  132M 0s\n",
            " 55500K .......... .......... .......... .......... .......... 97%  150M 0s\n",
            " 55550K .......... .......... .......... .......... .......... 97%  140M 0s\n",
            " 55600K .......... .......... .......... .......... .......... 97%  167M 0s\n",
            " 55650K .......... .......... .......... .......... .......... 97%  155M 0s\n",
            " 55700K .......... .......... .......... .......... .......... 97%  142M 0s\n",
            " 55750K .......... .......... .......... .......... .......... 97%  131M 0s\n",
            " 55800K .......... .......... .......... .......... .......... 97%  142M 0s\n",
            " 55850K .......... .......... .......... .......... .......... 97%  145M 0s\n",
            " 55900K .......... .......... .......... .......... .......... 97%  160M 0s\n",
            " 55950K .......... .......... .......... .......... .......... 98%  118M 0s\n",
            " 56000K .......... .......... .......... .......... .......... 98%  149M 0s\n",
            " 56050K .......... .......... .......... .......... .......... 98%  167M 0s\n",
            " 56100K .......... .......... .......... .......... .......... 98%  133M 0s\n",
            " 56150K .......... .......... .......... .......... .......... 98%  136M 0s\n",
            " 56200K .......... .......... .......... .......... .......... 98%  128M 0s\n",
            " 56250K .......... .......... .......... .......... .......... 98%  169M 0s\n",
            " 56300K .......... .......... .......... .......... .......... 98%  129M 0s\n",
            " 56350K .......... .......... .......... .......... .......... 98%  138M 0s\n",
            " 56400K .......... .......... .......... .......... .......... 98%  165M 0s\n",
            " 56450K .......... .......... .......... .......... .......... 98%  143M 0s\n",
            " 56500K .......... .......... .......... .......... .......... 99%  154M 0s\n",
            " 56550K .......... .......... .......... .......... .......... 99%  122M 0s\n",
            " 56600K .......... .......... .......... .......... .......... 99%  157M 0s\n",
            " 56650K .......... .......... .......... .......... .......... 99%  124M 0s\n",
            " 56700K .......... .......... .......... .......... .......... 99%  129M 0s\n",
            " 56750K .......... .......... .......... .......... .......... 99%  120M 0s\n",
            " 56800K .......... .......... .......... .......... .......... 99%  127M 0s\n",
            " 56850K .......... .......... .......... .......... .......... 99%  169M 0s\n",
            " 56900K .......... .......... .......... .......... .......... 99%  132M 0s\n",
            " 56950K .......... .......... .......... .......... .......... 99%  123M 0s\n",
            " 57000K .......... .......... .......... .......... .......... 99%  153M 0s\n",
            " 57050K .......... .......... .......... .......... ........  100%  128M=0.4s\n",
            "\n",
            "2020-08-22 17:26:33 (135 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498]\n",
            "\n",
            "Python 3.6.5 :: Anaconda, Inc.\n",
            "\rld_impl_linux-64-2.3 |  645 KB |            |   0% \rld_impl_linux-64-2.3 |  645 KB | ########9  |  89% \rld_impl_linux-64-2.3 |  645 KB | ########## | 100% \n",
            "\ropenssl-1.1.1g       |  3.8 MB |            |   0% \ropenssl-1.1.1g       |  3.8 MB | #######5   |  75% \ropenssl-1.1.1g       |  3.8 MB | #########7 |  98% \ropenssl-1.1.1g       |  3.8 MB | ########## | 100% \n",
            "\rconda-4.8.4          |  3.0 MB |            |   0% \rconda-4.8.4          |  3.0 MB | ########1  |  81% \rconda-4.8.4          |  3.0 MB | ########## | 100% \n",
            "\rsetuptools-49.6.0    |  927 KB |            |   0% \rsetuptools-49.6.0    |  927 KB | ########1  |  81% \rsetuptools-49.6.0    |  927 KB | ########## | 100% \n",
            "\rlibgcc-ng-9.1.0      |  8.1 MB |            |   0% \rlibgcc-ng-9.1.0      |  8.1 MB | #######5   |  75% \rlibgcc-ng-9.1.0      |  8.1 MB | #########8 |  98% \rlibgcc-ng-9.1.0      |  8.1 MB | ########## | 100% \n",
            "\rcffi-1.14.1          |  227 KB |            |   0% \rcffi-1.14.1          |  227 KB | ########## | 100% \n",
            "\rcryptography-2.9.2   |  626 KB |            |   0% \rcryptography-2.9.2   |  626 KB | ########9  |  89% \rcryptography-2.9.2   |  626 KB | ########## | 100% \n",
            "\rpysocks-1.7.1        |   30 KB |            |   0% \rpysocks-1.7.1        |   30 KB | ########## | 100% \n",
            "\rtqdm-4.48.2          |   63 KB |            |   0% \rtqdm-4.48.2          |   63 KB | ########## | 100% \n",
            "\rurllib3-1.25.10      |   93 KB |            |   0% \rurllib3-1.25.10      |   93 KB | ########## | 100% \n",
            "\rlibedit-3.1.20191231 |  121 KB |            |   0% \rlibedit-3.1.20191231 |  121 KB | ########## | 100% \n",
            "\rcertifi-2020.6.20    |  160 KB |            |   0% \rcertifi-2020.6.20    |  160 KB | ########## | 100% \n",
            "\rpycosat-0.6.3        |  107 KB |            |   0% \rpycosat-0.6.3        |  107 KB | ########## | 100% \n",
            "\rpip-20.2.2           |  2.0 MB |            |   0% \rpip-20.2.2           |  2.0 MB | #######7   |  78% \rpip-20.2.2           |  2.0 MB | #########3 |  93% \rpip-20.2.2           |  2.0 MB | ########## | 100% \n",
            "\r_libgcc_mutex-0.1    |    3 KB |            |   0% \r_libgcc_mutex-0.1    |    3 KB | ########## | 100% \n",
            "\rreadline-8.0         |  428 KB |            |   0% \rreadline-8.0         |  428 KB | ########## | 100% \n",
            "\rxz-5.2.5             |  438 KB |            |   0% \rxz-5.2.5             |  438 KB | #########6 |  96% \rxz-5.2.5             |  438 KB | ########## | 100% \n",
            "\rca-certificates-2020 |  133 KB |            |   0% \rca-certificates-2020 |  133 KB | ########## | 100% \n",
            "\ridna-2.10            |   56 KB |            |   0% \ridna-2.10            |   56 KB | ########## | 100% \n",
            "\rsix-1.15.0           |   13 KB |            |   0% \rsix-1.15.0           |   13 KB | ########## | 100% \n",
            "\ryaml-0.2.5           |   87 KB |            |   0% \ryaml-0.2.5           |   87 KB | ########## | 100% \n",
            "\rpyopenssl-19.1.0     |   47 KB |            |   0% \rpyopenssl-19.1.0     |   47 KB | ########## | 100% \n",
            "\rbrotlipy-0.7.0       |  348 KB |            |   0% \rbrotlipy-0.7.0       |  348 KB | ########## | 100% \n",
            "\rzlib-1.2.11          |  120 KB |            |   0% \rzlib-1.2.11          |  120 KB | ########## | 100% \n",
            "\rtk-8.6.10            |  3.2 MB |            |   0% \rtk-8.6.10            |  3.2 MB | #######7   |  78% \rtk-8.6.10            |  3.2 MB | ########## | 100% \n",
            "\rrequests-2.24.0      |   54 KB |            |   0% \rrequests-2.24.0      |   54 KB | ########## | 100% \n",
            "\rruamel_yaml-0.15.87  |  256 KB |            |   0% \rruamel_yaml-0.15.87  |  256 KB | ########## | 100% \n",
            "\rpython-3.6.10        | 33.9 MB |            |   0% \rpython-3.6.10        | 33.9 MB | #8         |  18% \rpython-3.6.10        | 33.9 MB | ####4      |  45% \rpython-3.6.10        | 33.9 MB | #######2   |  73% \rpython-3.6.10        | 33.9 MB | ########9  |  89% \rpython-3.6.10        | 33.9 MB | ########## | 100% \n",
            "\rconda-package-handli |  886 KB |            |   0% \rconda-package-handli |  886 KB | #########1 |  91% \rconda-package-handli |  886 KB | ########## | 100% \n",
            "\rchardet-3.0.4        |  197 KB |            |   0% \rchardet-3.0.4        |  197 KB | ########## | 100% \n",
            "\rwheel-0.34.2         |   49 KB |            |   0% \rwheel-0.34.2         |   49 KB | ########## | 100% \n",
            "\rlibffi-3.3           |   54 KB |            |   0% \rlibffi-3.3           |   54 KB | ########## | 100% \n",
            "\rncurses-6.2          |  1.1 MB |            |   0% \rncurses-6.2          |  1.1 MB | #######8   |  78% \rncurses-6.2          |  1.1 MB | #########8 |  98% \rncurses-6.2          |  1.1 MB | ########## | 100% \n",
            "\rlibstdcxx-ng-9.1.0   |  4.0 MB |            |   0% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #######5   |  75% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #########7 |  98% \rlibstdcxx-ng-9.1.0   |  4.0 MB | ########## | 100% \n",
            "\rsqlite-3.33.0        |  2.0 MB |            |   0% \rsqlite-3.33.0        |  2.0 MB | #######8   |  79% \rsqlite-3.33.0        |  2.0 MB | ########## | 100% \n",
            "\rpycparser-2.20       |   94 KB |            |   0% \rpycparser-2.20       |   94 KB | ########## | 100% \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc3IcSnEH8Uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.6/site-packages\"))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWTBcPFGIMMs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7e7311a-c51d-4c71-e7e7-4b721f64e21d"
      },
      "source": [
        "!time conda install -q -y -c conda-forge rdkit --yes\n",
        "!time conda install pytorch torchvision cudatoolkit -c pytorch --yes\n",
        "!time conda install ipykernel --yes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.72.0               |   py36h9de70de_0         339 KB  conda-forge\n",
            "    boost-cpp-1.72.0           |       h7b93d67_2        16.3 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_3         398 KB  conda-forge\n",
            "    ca-certificates-2020.6.20  |       hecda079_0         145 KB  conda-forge\n",
            "    cairo-1.16.0               |    h3fc0475_1005         1.5 MB  conda-forge\n",
            "    certifi-2020.6.20          |   py36h9f0ad1d_0         151 KB  conda-forge\n",
            "    conda-4.8.4                |   py36h9f0ad1d_2         3.1 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h1056068_1002         365 KB  conda-forge\n",
            "    freetype-2.10.2            |       he06d7ca_0         905 KB  conda-forge\n",
            "    glib-2.65.0                |       h3eb4bd4_0         2.9 MB\n",
            "    icu-67.1                   |       he1b5a44_0        12.9 MB  conda-forge\n",
            "    jpeg-9d                    |       h516909a_0         266 KB  conda-forge\n",
            "    lcms2-2.11                 |       hbd6801e_0         431 KB  conda-forge\n",
            "    libblas-3.8.0              |      17_openblas          11 KB  conda-forge\n",
            "    libcblas-3.8.0             |      17_openblas          11 KB  conda-forge\n",
            "    libgfortran-ng-7.5.0       |      hdf63c60_15         1.3 MB  conda-forge\n",
            "    libiconv-1.16              |       h516909a_0         1.4 MB  conda-forge\n",
            "    liblapack-3.8.0            |      17_openblas          11 KB  conda-forge\n",
            "    libopenblas-0.3.10         |pthreads_hb3c22a3_4         7.8 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_2         359 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc7e4089_6         668 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libwebp-base-1.1.0         |       h516909a_3         845 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       h68273f3_2         1.3 MB  conda-forge\n",
            "    lz4-c-1.9.2                |       he1b5a44_3         203 KB  conda-forge\n",
            "    numpy-1.19.1               |   py36h3849536_2         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1g             |       h516909a_1         2.1 MB  conda-forge\n",
            "    pandas-1.1.0               |   py36h831f99a_0        10.5 MB  conda-forge\n",
            "    pcre-8.44                  |       he1b5a44_0         261 KB  conda-forge\n",
            "    pillow-7.2.0               |   py36h8328e55_1         670 KB  conda-forge\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.19.1             |   py36h4779a57_3          77 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.6             |          1_cp36m           4 KB  conda-forge\n",
            "    pytz-2020.1                |     pyh9f0ad1d_0         227 KB  conda-forge\n",
            "    rdkit-2020.03.5            |   py36h203deb4_0        24.8 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.11         |       h516909a_0         920 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.5                 |       h6597ccf_2         712 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       100.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.72.0-py36h9de70de_0\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.72.0-h7b93d67_2\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_3\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-h3fc0475_1005\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h1056068_1002\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.2-he06d7ca_0\n",
            "  glib               pkgs/main/linux-64::glib-2.65.0-h3eb4bd4_0\n",
            "  icu                conda-forge/linux-64::icu-67.1-he1b5a44_0\n",
            "  jpeg               conda-forge/linux-64::jpeg-9d-h516909a_0\n",
            "  lcms2              conda-forge/linux-64::lcms2-2.11-hbd6801e_0\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-17_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-17_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.5.0-hdf63c60_15\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.16-h516909a_0\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-17_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.10-pthreads_hb3c22a3_4\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_2\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc7e4089_6\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.1.0-h516909a_3\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-h68273f3_2\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.9.2-he1b5a44_3\n",
            "  numpy              conda-forge/linux-64::numpy-1.19.1-py36h3849536_2\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.1.0-py36h831f99a_0\n",
            "  pcre               conda-forge/linux-64::pcre-8.44-he1b5a44_0\n",
            "  pillow             conda-forge/linux-64::pillow-7.2.0-py36h8328e55_1\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.19.1-py36h4779a57_3\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
            "  pytz               conda-forge/noarch::pytz-2020.1-pyh9f0ad1d_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2020.03.5-py36h203deb4_0\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.11-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.5-h6597ccf_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  conda                       pkgs/main::conda-4.8.4-py36_0 --> conda-forge::conda-4.8.4-py36h9f0ad1d_2\n",
            "  openssl              pkgs/main::openssl-1.1.1g-h7b6447c_0 --> conda-forge::openssl-1.1.1g-h516909a_1\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2020.6.24-0 --> conda-forge::ca-certificates-2020.6.20-hecda079_0\n",
            "  certifi               pkgs/main::certifi-2020.6.20-py36_0 --> conda-forge::certifi-2020.6.20-py36h9f0ad1d_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "real\t0m43.035s\n",
            "user\t0m36.679s\n",
            "sys\t0m4.561s\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit\n",
            "    - pytorch\n",
            "    - torchvision\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2020.6.24  |                0         125 KB\n",
            "    certifi-2020.6.20          |           py36_0         156 KB\n",
            "    cudatoolkit-10.1.243       |       h6bb024c_0       347.4 MB\n",
            "    intel-openmp-2020.1        |              217         780 KB\n",
            "    mkl-2020.1                 |              217       129.0 MB\n",
            "    ninja-1.10.0               |   py36hfd86e86_0         1.4 MB\n",
            "    pytorch-1.4.0              |py3.6_cuda10.1.243_cudnn7.6.3_0       432.9 MB  pytorch\n",
            "    torchvision-0.5.0          |       py36_cu101         9.1 MB  pytorch\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       920.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0\n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2020.1-217\n",
            "  mkl                pkgs/main/linux-64::mkl-2020.1-217\n",
            "  ninja              pkgs/main/linux-64::ninja-1.10.0-py36hfd86e86_0\n",
            "  pytorch            pytorch/linux-64::pytorch-1.4.0-py3.6_cuda10.1.243_cudnn7.6.3_0\n",
            "  torchvision        pytorch/linux-64::torchvision-0.5.0-py36_cu101\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.6.2~ --> pkgs/main::ca-certificates-2020.6.24-0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2020.6.20-py36h9~ --> pkgs/main::certifi-2020.6.20-py36_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "intel-openmp-2020.1  | 780 KB    | : 100% 1.0/1 [00:00<00:00,  8.27it/s]\n",
            "ninja-1.10.0         | 1.4 MB    | : 100% 1.0/1 [00:00<00:00, 12.73it/s]\n",
            "ca-certificates-2020 | 125 KB    | : 100% 1.0/1 [00:00<00:00, 19.94it/s]\n",
            "cudatoolkit-10.1.243 | 347.4 MB  | : 100% 1.0/1 [00:09<00:00,  9.49s/it]               \n",
            "pytorch-1.4.0        | 432.9 MB  | : 100% 1.0/1 [01:32<00:00, 92.53s/it]               \n",
            "torchvision-0.5.0    | 9.1 MB    | : 100% 1.0/1 [00:04<00:00,  4.56s/it]\n",
            "certifi-2020.6.20    | 156 KB    | : 100% 1.0/1 [00:00<00:00, 15.04it/s]\n",
            "mkl-2020.1           | 129.0 MB  | : 100% 1.0/1 [00:05<00:00,  5.01s/it]               \n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "real\t2m45.582s\n",
            "user\t1m18.442s\n",
            "sys\t0m9.187s\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - ipykernel\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    backcall-0.2.0             |             py_0          15 KB\n",
            "    decorator-4.4.2            |             py_0          14 KB\n",
            "    ipykernel-5.3.4            |   py36h5ca1d4c_0         181 KB\n",
            "    ipython-7.16.1             |   py36h5ca1d4c_0         999 KB\n",
            "    ipython_genutils-0.2.0     |           py36_0          39 KB\n",
            "    jedi-0.17.1                |           py36_0         921 KB\n",
            "    jupyter_client-6.1.6       |             py_0          84 KB\n",
            "    jupyter_core-4.6.3         |           py36_0          71 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    parso-0.7.0                |             py_0          72 KB\n",
            "    pexpect-4.8.0              |           py36_0          82 KB\n",
            "    pickleshare-0.7.5          |           py36_0          13 KB\n",
            "    prompt-toolkit-3.0.5       |             py_0         245 KB\n",
            "    ptyprocess-0.6.0           |           py36_0          23 KB\n",
            "    pygments-2.6.1             |             py_0         654 KB\n",
            "    pyzmq-19.0.1               |   py36he6710b0_1         460 KB\n",
            "    tornado-6.0.4              |   py36h7b6447c_1         597 KB\n",
            "    traitlets-4.3.3            |           py36_0         140 KB\n",
            "    wcwidth-0.2.5              |             py_0          29 KB\n",
            "    zeromq-4.3.2               |       he6710b0_2         510 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.3 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-py_0\n",
            "  decorator          pkgs/main/noarch::decorator-4.4.2-py_0\n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-5.3.4-py36h5ca1d4c_0\n",
            "  ipython            pkgs/main/linux-64::ipython-7.16.1-py36h5ca1d4c_0\n",
            "  ipython_genutils   pkgs/main/linux-64::ipython_genutils-0.2.0-py36_0\n",
            "  jedi               pkgs/main/linux-64::jedi-0.17.1-py36_0\n",
            "  jupyter_client     pkgs/main/noarch::jupyter_client-6.1.6-py_0\n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.6.3-py36_0\n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0\n",
            "  parso              pkgs/main/noarch::parso-0.7.0-py_0\n",
            "  pexpect            pkgs/main/linux-64::pexpect-4.8.0-py36_0\n",
            "  pickleshare        pkgs/main/linux-64::pickleshare-0.7.5-py36_0\n",
            "  prompt-toolkit     pkgs/main/noarch::prompt-toolkit-3.0.5-py_0\n",
            "  ptyprocess         pkgs/main/linux-64::ptyprocess-0.6.0-py36_0\n",
            "  pygments           pkgs/main/noarch::pygments-2.6.1-py_0\n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-19.0.1-py36he6710b0_1\n",
            "  tornado            pkgs/main/linux-64::tornado-6.0.4-py36h7b6447c_1\n",
            "  traitlets          pkgs/main/linux-64::traitlets-4.3.3-py36_0\n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-py_0\n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.2-he6710b0_2\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  conda              conda-forge::conda-4.8.4-py36h9f0ad1d~ --> pkgs/main::conda-4.8.4-py36_0\n",
            "  openssl            conda-forge::openssl-1.1.1g-h516909a_1 --> pkgs/main::openssl-1.1.1g-h7b6447c_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ipykernel-5.3.4      | 181 KB    | : 100% 1.0/1 [00:00<00:00,  9.61it/s]\n",
            "backcall-0.2.0       | 15 KB     | : 100% 1.0/1 [00:00<00:00, 14.61it/s]\n",
            "jedi-0.17.1          | 921 KB    | : 100% 1.0/1 [00:00<00:00,  3.78it/s]\n",
            "tornado-6.0.4        | 597 KB    | : 100% 1.0/1 [00:00<00:00, 13.29it/s]\n",
            "jupyter_core-4.6.3   | 71 KB     | : 100% 1.0/1 [00:00<00:00, 15.66it/s]\n",
            "ipython_genutils-0.2 | 39 KB     | : 100% 1.0/1 [00:00<00:00, 20.55it/s]\n",
            "ipython-7.16.1       | 999 KB    | : 100% 1.0/1 [00:00<00:00,  6.96it/s]\n",
            "ptyprocess-0.6.0     | 23 KB     | : 100% 1.0/1 [00:00<00:00, 20.49it/s]\n",
            "libsodium-1.0.18     | 244 KB    | : 100% 1.0/1 [00:00<00:00, 15.63it/s]\n",
            "decorator-4.4.2      | 14 KB     | : 100% 1.0/1 [00:00<00:00, 20.48it/s]\n",
            "pexpect-4.8.0        | 82 KB     | : 100% 1.0/1 [00:00<00:00, 18.55it/s]\n",
            "jupyter_client-6.1.6 | 84 KB     | : 100% 1.0/1 [00:00<00:00, 17.63it/s]\n",
            "traitlets-4.3.3      | 140 KB    | : 100% 1.0/1 [00:00<00:00, 19.15it/s]\n",
            "pyzmq-19.0.1         | 460 KB    | : 100% 1.0/1 [00:00<00:00, 11.39it/s]\n",
            "prompt-toolkit-3.0.5 | 245 KB    | : 100% 1.0/1 [00:00<00:00, 13.80it/s]\n",
            "pygments-2.6.1       | 654 KB    | : 100% 1.0/1 [00:00<00:00, 12.76it/s]\n",
            "wcwidth-0.2.5        | 29 KB     | : 100% 1.0/1 [00:00<00:00, 21.74it/s]\n",
            "zeromq-4.3.2         | 510 KB    | : 100% 1.0/1 [00:00<00:00, 13.18it/s]\n",
            "parso-0.7.0          | 72 KB     | : 100% 1.0/1 [00:00<00:00, 21.95it/s]\n",
            "pickleshare-0.7.5    | 13 KB     | : 100% 1.0/1 [00:00<00:00,  4.12it/s]\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "real\t0m28.541s\n",
            "user\t0m24.492s\n",
            "sys\t0m2.551s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mah0YwIEtwF5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "bc91e647-c8fd-47f9-bce6-a6d374369d11"
      },
      "source": [
        "##Testing rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "\n",
        "Chem.MolFromSmiles('c1ccccc1')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<rdkit.Chem.rdchem.Mol at 0x7f41954ec760>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wg37MoxrK2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "754c0067-7492-4203-8c66-bf36f472da20"
      },
      "source": [
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
            "Collecting torch-scatter==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.4.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (10.6 MB)\n",
            "Installing collected packages: torch-scatter\n",
            "  Attempting uninstall: torch-scatter\n",
            "    Found existing installation: torch-scatter 2.0.4\n",
            "    Uninstalling torch-scatter-2.0.4:\n",
            "      Successfully uninstalled torch-scatter-2.0.4\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
            "Collecting torch-sparse==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.4.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (15.2 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from torch-sparse==latest+cu101) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/site-packages (from scipy->torch-sparse==latest+cu101) (1.19.1)\n",
            "Installing collected packages: torch-sparse\n",
            "  Attempting uninstall: torch-sparse\n",
            "    Found existing installation: torch-sparse 0.6.1\n",
            "    Uninstalling torch-sparse-0.6.1:\n",
            "      Successfully uninstalled torch-sparse-0.6.1\n",
            "Successfully installed torch-sparse-0.6.1\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
            "Collecting torch-cluster==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.4.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (14.5 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from torch-cluster==latest+cu101) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/site-packages (from scipy->torch-cluster==latest+cu101) (1.19.1)\n",
            "Installing collected packages: torch-cluster\n",
            "  Attempting uninstall: torch-cluster\n",
            "    Found existing installation: torch-cluster 1.5.4\n",
            "    Uninstalling torch-cluster-1.5.4:\n",
            "      Successfully uninstalled torch-cluster-1.5.4\n",
            "Successfully installed torch-cluster-1.5.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
            "Collecting torch-spline-conv==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.4.0/torch_spline_conv-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (5.1 MB)\n",
            "Installing collected packages: torch-spline-conv\n",
            "  Attempting uninstall: torch-spline-conv\n",
            "    Found existing installation: torch-spline-conv 1.2.0\n",
            "    Uninstalling torch-spline-conv-1.2.0:\n",
            "      Successfully uninstalled torch-spline-conv-1.2.0\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from torch-geometric) (1.19.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/site-packages (from torch-geometric) (0.23.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/site-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.6/site-packages (from torch-geometric) (3.20.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from torch-geometric) (1.5.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/site-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/site-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/site-packages (from torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/site-packages (from torch-geometric) (0.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from torch-geometric) (2.24.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (from torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from torch-geometric) (4.48.2)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/site-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn->torch-geometric) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/site-packages (from scikit-learn->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/site-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/site-packages (from ase->torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/site-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/site-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/site-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.6/site-packages (from numba->torch-geometric) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from numba->torch-geometric) (49.6.0.post20200814)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->torch-geometric) (1.25.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas->torch-geometric) (2020.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/site-packages (from matplotlib>=2.0.0->ase->torch-geometric) (7.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFpbVH8yYgg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.6/site-packages\"))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8uxaIv5i02b",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "\n",
        "</font></h1><p></p>\n",
        "در اینجا \n",
        "preprocess\n",
        "انجام شده است. ورودی صفر متناظر با \n",
        "davis\n",
        "و ورودی یک متناظر با \n",
        "kiba \n",
        "است.\n",
        "در صورتی که هر دو لازم است\n",
        "all \n",
        "باید زده شود\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuLqigwut2e1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8e3ff4bb-feac-48f5-84bf-996189cc5bda"
      },
      "source": [
        "!python preprocess_data.py 0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert data from DeepDTA for  davis\n",
            "train, validation, test for  davis created.\n",
            "preparing  davis_train.pt in pytorch format!\n",
            "Pre-processed data data/processed/davis_train.pt not found, doing pre-processing...\n",
            "preparing  davis_test.pt in pytorch format!\n",
            "Pre-processed data data/processed/davis_test.pt not found, doing pre-processing...\n",
            "preparing  davis_validation.pt in pytorch format!\n",
            "Pre-processed data data/processed/davis_validation.pt not found, doing pre-processing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgDw3snljNgy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "۴و ۵ .\n",
        "</font></h1><p></p>\n",
        "استفاده از بخش آخر به عنوان داده \n",
        "validation\n",
        "در بخش \n",
        "preprocess\n",
        "انجام شد. با توجه به آنکه فرآیند آموزش بسیار طولانی بود، از همان هایپرپارامترهای مقاله\n",
        "GraphDTA\n",
        "استفاده کردیم. \n",
        "\n",
        "مدل را روی ۱۰۰ ایپاک آموزش دادیم اما همانطور که از لاگ مشخص است مدل هنوز جای یادگیری دارد اما به زمان‌گیر بودن بسیار و کمبود وقت به همین ۱۰۰ ایپاک بسنده کردیم. در حال حاضر مدلی که در صدمین ایپاک به آن رسیدیم ، روی داده تست در \n",
        "davis\n",
        " به دو شاخص\n",
        "MSE = 0.2900\n",
        "و\n",
        "CI = 0.8578\n",
        "رسیده است. همچنین بهترین مدلی \n",
        "(مدلی که بهترین لاس را روی ولیدیشن بدست آورده است)\n",
        "دو شاخص \n",
        "MSE = 0.2919\n",
        "و\n",
        "CI = 0.8679\n",
        "را دارد.\n",
        "به نظر می‌آید که اگر مدل بیشتر یاد می‌گرفت به معیارهای بهتری هم می‌رسید.\n",
        "در مقاله \n",
        "DeepDTA\n",
        "داریم:\n",
        "CI = 0.878\n",
        "و\n",
        "MSE = 0.261\n",
        "که روی ۱۰۰ ایپاک آموزش دیده شده است. با توجه به نتایج \n",
        "GraphDTA\n",
        "که\n",
        "به نتایج \n",
        "CI = 0.893\n",
        "و\n",
        "MSE = 0.229\n",
        "روی \n",
        "davis\n",
        " رسیده است، اگر مدل را بیشتر آموزش می‌دادیم احتمالا به همان نتایج می‌رسیدیم.\n",
        "آموزش را تنها روی داده \n",
        "davis\n",
        "انجام دادیم چرا که انجام آموزش روی هر دو دیتاست در ادامه بخش‌ها بسیار بسیار زمان‌بر و دور از دسترس بود.\n",
        "\n",
        " ۵ : \n",
        " این کار را با ترشولد گفته شده انجام دادیم و معیارهای دسته‌بندی دو کلاسه را حساب کردیم.\n",
        "\n",
        " Test acc: 0.946707<br>\n",
        " Test sensitivity: 0.560976 <br> \n",
        " Test specifity: 0.981087\n",
        "\n",
        " Test f1: 0.632737\n",
        " \n",
        "همانطور که مشخص است معیار \n",
        "sensitivity\n",
        "نسبت به \n",
        "specifity\n",
        "بسیار کمتر است که نشان‌دهنده این است که بایاس خاصی نسبت به کلاس صفر داریم و هر دو کلاس را به خوبی یاد نگرفته‌ایم در قسمت‌های بعدی سعی می‌کنیم این مشکل را مرتفع کنیم\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWrz8z3FpN68",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "\n",
        "</font></h1><p></p>\n",
        "ورودی اول متناظر با دیتا است. صفر \n",
        "davis\n",
        "است و یک \n",
        "kiba.\n",
        "\n",
        "ورودی دوم متناظر با این است که روی دیتا\n",
        "upsample \n",
        "اجرا شود یا نه. این مورد در بخش ششم پروژه تاثیرگذار است\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk-uR5oHvnRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d489e0e7-fbc0-4772-df66-06e199a846f5"
      },
      "source": [
        "!python regression_training.py 0 0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "davis\n",
            "Pre-processed data found: data/processed/davis_train.pt, loading ...\n",
            "Pre-processed data found: data/processed/davis_validation.pt, loading ...\n",
            "\tValidation mse decreased (inf --> 0.846375) at epoch: 1. CI:0.629246 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  8\n",
            "Epoch: 1 Training Loss: 2.230428  val acc: 0.924336  val sensitivity: 0.007958  val specifity: 0.998921  val f1: 0.015584\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  274\n",
            "Epoch: 2 Training Loss: 0.791210  val acc: 0.874825  val sensitivity: 0.031830  val specifity: 0.943437  val f1: 0.036866\n",
            "\tValidation mse decreased (0.846375 --> 0.556503) at epoch: 3. CI:0.781914 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  13\n",
            "Epoch: 3 Training Loss: 0.746636  val acc: 0.927331  val sensitivity: 0.034483  val specifity: 1.000000  val f1: 0.066667\n",
            "\tValidation mse decreased (0.556503 --> 0.537836) at epoch: 4. CI:0.751056 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  45\n",
            "Epoch: 4 Training Loss: 0.657613  val acc: 0.929726  val sensitivity: 0.092838  val specifity: 0.997841  val f1: 0.165877\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  33\n",
            "Epoch: 5 Training Loss: 0.676046  val acc: 0.928529  val sensitivity: 0.068966  val specifity: 0.998489  val f1: 0.126829\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  33\n",
            "Epoch: 6 Training Loss: 0.620539  val acc: 0.931324  val sensitivity: 0.087533  val specifity: 1.000000  val f1: 0.160976\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  34\n",
            "Epoch: 7 Training Loss: 0.600154  val acc: 0.930725  val sensitivity: 0.084881  val specifity: 0.999568  val f1: 0.155718\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  79\n",
            "Epoch: 8 Training Loss: 0.560384  val acc: 0.934917  val sensitivity: 0.172414  val specifity: 0.996978  val f1: 0.285088\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  80\n",
            "Epoch: 9 Training Loss: 0.541584  val acc: 0.936714  val sensitivity: 0.185676  val specifity: 0.997841  val f1: 0.306346\n",
            "\tValidation mse decreased (0.537836 --> 0.386022) at epoch: 10. CI:0.808144 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  169\n",
            "Epoch: 10 Training Loss: 0.523777  val acc: 0.943701  val sensitivity: 0.350133  val specifity: 0.992012  val f1: 0.483516\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  55\n",
            "Epoch: 11 Training Loss: 0.513820  val acc: 0.934119  val sensitivity: 0.135279  val specifity: 0.999136  val f1: 0.236111\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  127\n",
            "Epoch: 12 Training Loss: 0.463747  val acc: 0.939309  val sensitivity: 0.265252  val specifity: 0.994171  val f1: 0.396825\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  78\n",
            "Epoch: 13 Training Loss: 0.472706  val acc: 0.936714  val sensitivity: 0.183024  val specifity: 0.998057  val f1: 0.303297\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  131\n",
            "Epoch: 14 Training Loss: 0.456005  val acc: 0.943701  val sensitivity: 0.299735  val specifity: 0.996114  val f1: 0.444882\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  160\n",
            "Epoch: 15 Training Loss: 0.434749  val acc: 0.943901  val sensitivity: 0.339523  val specifity: 0.993092  val f1: 0.476723\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  99\n",
            "Epoch: 16 Training Loss: 0.430935  val acc: 0.941306  val sensitivity: 0.241379  val specifity: 0.998273  val f1: 0.382353\n",
            "\tValidation mse decreased (0.386022 --> 0.359723) at epoch: 17. CI:0.818924 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  145\n",
            "Epoch: 17 Training Loss: 0.404155  val acc: 0.944899  val sensitivity: 0.326260  val specifity: 0.995250  val f1: 0.471264\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  153\n",
            "Epoch: 18 Training Loss: 0.421470  val acc: 0.942903  val sensitivity: 0.323607  val specifity: 0.993307  val f1: 0.460377\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  91\n",
            "Epoch: 19 Training Loss: 0.414137  val acc: 0.940108  val sensitivity: 0.222812  val specifity: 0.998489  val f1: 0.358974\n",
            "\tValidation mse decreased (0.359723 --> 0.329392) at epoch: 20. CI:0.831107 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  265\n",
            "Epoch: 20 Training Loss: 0.402766  val acc: 0.948093  val sensitivity: 0.506631  val specifity: 0.984024  val f1: 0.595016\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  162\n",
            "Epoch: 21 Training Loss: 0.422671  val acc: 0.945897  val sensitivity: 0.355438  val specifity: 0.993955  val f1: 0.497217\n",
            "\tValidation mse decreased (0.329392 --> 0.318462) at epoch: 22. CI:0.835179 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  230\n",
            "Epoch: 22 Training Loss: 0.379965  val acc: 0.946696  val sensitivity: 0.450928  val specifity: 0.987047  val f1: 0.560132\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  168\n",
            "Epoch: 23 Training Loss: 0.394364  val acc: 0.947495  val sensitivity: 0.374005  val specifity: 0.994171  val f1: 0.517431\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  127\n",
            "Epoch: 24 Training Loss: 0.356280  val acc: 0.943302  val sensitivity: 0.291777  val specifity: 0.996330  val f1: 0.436508\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  174\n",
            "Epoch: 25 Training Loss: 0.356154  val acc: 0.947095  val sensitivity: 0.379310  val specifity: 0.993307  val f1: 0.519056\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  156\n",
            "Epoch: 26 Training Loss: 0.355955  val acc: 0.945498  val sensitivity: 0.344828  val specifity: 0.994387  val f1: 0.487805\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  163\n",
            "Epoch: 27 Training Loss: 0.341039  val acc: 0.948093  val sensitivity: 0.371353  val specifity: 0.995035  val f1: 0.518519\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  180\n",
            "Epoch: 28 Training Loss: 0.340217  val acc: 0.946696  val sensitivity: 0.384615  val specifity: 0.992444  val f1: 0.520646\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  112\n",
            "Epoch: 29 Training Loss: 0.332120  val acc: 0.943102  val sensitivity: 0.270557  val specifity: 0.997841  val f1: 0.417178\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  132\n",
            "Epoch: 30 Training Loss: 0.328193  val acc: 0.944300  val sensitivity: 0.305040  val specifity: 0.996330  val f1: 0.451866\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  116\n",
            "Epoch: 31 Training Loss: 0.335521  val acc: 0.944300  val sensitivity: 0.283820  val specifity: 0.998057  val f1: 0.434077\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  247\n",
            "Epoch: 32 Training Loss: 0.320008  val acc: 0.952486  val sensitivity: 0.511936  val specifity: 0.988342  val f1: 0.618590\n",
            "\tValidation mse decreased (0.318462 --> 0.317672) at epoch: 33. CI:0.835470 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  236\n",
            "Epoch: 33 Training Loss: 0.322653  val acc: 0.949092  val sensitivity: 0.474801  val specifity: 0.987694  val f1: 0.584013\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  164\n",
            "Epoch: 34 Training Loss: 0.349546  val acc: 0.945897  val sensitivity: 0.358090  val specifity: 0.993739  val f1: 0.499076\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  237\n",
            "Epoch: 35 Training Loss: 0.304192  val acc: 0.950888  val sensitivity: 0.488064  val specifity: 0.988558  val f1: 0.599349\n",
            "\tValidation mse decreased (0.317672 --> 0.291950) at epoch: 36. CI:0.845450 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  237\n",
            "Epoch: 36 Training Loss: 0.311970  val acc: 0.952486  val sensitivity: 0.498674  val specifity: 0.989421  val f1: 0.612378\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  179\n",
            "Epoch: 37 Training Loss: 0.303200  val acc: 0.949691  val sensitivity: 0.403183  val specifity: 0.994171  val f1: 0.546763\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  166\n",
            "Epoch: 38 Training Loss: 0.297577  val acc: 0.949890  val sensitivity: 0.387268  val specifity: 0.995682  val f1: 0.537753\n",
            "\tValidation mse decreased (0.291950 --> 0.280793) at epoch: 39. CI:0.848451 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  218\n",
            "Epoch: 39 Training Loss: 0.278631  val acc: 0.951088  val sensitivity: 0.464191  val specifity: 0.990717  val f1: 0.588235\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  216\n",
            "Epoch: 40 Training Loss: 0.288660  val acc: 0.951088  val sensitivity: 0.461538  val specifity: 0.990933  val f1: 0.586847\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  205\n",
            "Epoch: 41 Training Loss: 0.302009  val acc: 0.949291  val sensitivity: 0.435013  val specifity: 0.991149  val f1: 0.563574\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  130\n",
            "Epoch: 42 Training Loss: 0.294107  val acc: 0.945498  val sensitivity: 0.310345  val specifity: 0.997193  val f1: 0.461538\n",
            "\tValidation mse decreased (0.280793 --> 0.275474) at epoch: 43. CI:0.858068 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  281\n",
            "Epoch: 43 Training Loss: 0.276334  val acc: 0.952885  val sensitivity: 0.559682  val specifity: 0.984888  val f1: 0.641337\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  253\n",
            "Epoch: 44 Training Loss: 0.299282  val acc: 0.950090  val sensitivity: 0.503979  val specifity: 0.986399  val f1: 0.603175\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  179\n",
            "Epoch: 45 Training Loss: 0.308955  val acc: 0.948892  val sensitivity: 0.397878  val specifity: 0.993739  val f1: 0.539568\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  246\n",
            "Epoch: 46 Training Loss: 0.280411  val acc: 0.954282  val sensitivity: 0.522546  val specifity: 0.989421  val f1: 0.632424\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  192\n",
            "Epoch: 47 Training Loss: 0.275210  val acc: 0.951887  val sensitivity: 0.435013  val specifity: 0.993955  val f1: 0.576450\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  207\n",
            "Epoch: 48 Training Loss: 0.262114  val acc: 0.952486  val sensitivity: 0.458886  val specifity: 0.992660  val f1: 0.592466\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  188\n",
            "Epoch: 49 Training Loss: 0.260624  val acc: 0.946696  val sensitivity: 0.395225  val specifity: 0.991580  val f1: 0.527434\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  230\n",
            "Epoch: 50 Training Loss: 0.251376  val acc: 0.951487  val sensitivity: 0.482759  val specifity: 0.989637  val f1: 0.599671\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  233\n",
            "Epoch: 51 Training Loss: 0.250709  val acc: 0.951687  val sensitivity: 0.488064  val specifity: 0.989421  val f1: 0.603279\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  196\n",
            "Epoch: 52 Training Loss: 0.252226  val acc: 0.949890  val sensitivity: 0.427056  val specifity: 0.992444  val f1: 0.561955\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  291\n",
            "Epoch: 53 Training Loss: 0.262608  val acc: 0.952486  val sensitivity: 0.570292  val specifity: 0.983592  val f1: 0.643713\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  267\n",
            "Epoch: 54 Training Loss: 0.263591  val acc: 0.951687  val sensitivity: 0.533156  val specifity: 0.985751  val f1: 0.624224\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  142\n",
            "Epoch: 55 Training Loss: 0.250128  val acc: 0.945099  val sensitivity: 0.323607  val specifity: 0.995682  val f1: 0.470135\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  210\n",
            "Epoch: 56 Training Loss: 0.261925  val acc: 0.952685  val sensitivity: 0.464191  val specifity: 0.992444  val f1: 0.596252\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  243\n",
            "Epoch: 57 Training Loss: 0.242848  val acc: 0.953284  val sensitivity: 0.511936  val specifity: 0.989206  val f1: 0.622581\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  190\n",
            "Epoch: 58 Training Loss: 0.268836  val acc: 0.951088  val sensitivity: 0.427056  val specifity: 0.993739  val f1: 0.567901\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  177\n",
            "Epoch: 59 Training Loss: 0.253245  val acc: 0.949691  val sensitivity: 0.400531  val specifity: 0.994387  val f1: 0.545126\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  221\n",
            "Epoch: 60 Training Loss: 0.235985  val acc: 0.952885  val sensitivity: 0.480106  val specifity: 0.991364  val f1: 0.605351\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  168\n",
            "Epoch: 61 Training Loss: 0.239067  val acc: 0.949092  val sensitivity: 0.384615  val specifity: 0.995035  val f1: 0.532110\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  211\n",
            "Epoch: 62 Training Loss: 0.231165  val acc: 0.952486  val sensitivity: 0.464191  val specifity: 0.992228  val f1: 0.595238\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  238\n",
            "Epoch: 63 Training Loss: 0.236564  val acc: 0.951887  val sensitivity: 0.496021  val specifity: 0.988990  val f1: 0.608130\n",
            "\tValidation mse decreased (0.275474 --> 0.266448) at epoch: 64. CI:0.861806 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  265\n",
            "Epoch: 64 Training Loss: 0.240778  val acc: 0.951288  val sensitivity: 0.527851  val specifity: 0.985751  val f1: 0.619938\n",
            "\tValidation mse decreased (0.266448 --> 0.260366) at epoch: 65. CI:0.863197 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  273\n",
            "Epoch: 65 Training Loss: 0.232021  val acc: 0.952885  val sensitivity: 0.549072  val specifity: 0.985751  val f1: 0.636923\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  190\n",
            "Epoch: 66 Training Loss: 0.238952  val acc: 0.949491  val sensitivity: 0.416446  val specifity: 0.992876  val f1: 0.553792\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  186\n",
            "Epoch: 67 Training Loss: 0.231942  val acc: 0.947894  val sensitivity: 0.400531  val specifity: 0.992444  val f1: 0.536412\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  220\n",
            "Epoch: 68 Training Loss: 0.223684  val acc: 0.951088  val sensitivity: 0.466844  val specifity: 0.990501  val f1: 0.589615\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  217\n",
            "Epoch: 69 Training Loss: 0.235198  val acc: 0.951288  val sensitivity: 0.464191  val specifity: 0.990933  val f1: 0.589226\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  220\n",
            "Epoch: 70 Training Loss: 0.230419  val acc: 0.953084  val sensitivity: 0.480106  val specifity: 0.991580  val f1: 0.606365\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  241\n",
            "Epoch: 71 Training Loss: 0.230184  val acc: 0.950888  val sensitivity: 0.493369  val specifity: 0.988126  val f1: 0.601942\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  211\n",
            "Epoch: 72 Training Loss: 0.227986  val acc: 0.949691  val sensitivity: 0.445623  val specifity: 0.990717  val f1: 0.571429\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  235\n",
            "Epoch: 73 Training Loss: 0.220490  val acc: 0.950888  val sensitivity: 0.485411  val specifity: 0.988774  val f1: 0.598039\n",
            "\tValidation mse decreased (0.260366 --> 0.258303) at epoch: 74. CI:0.857977 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  259\n",
            "Epoch: 74 Training Loss: 0.220405  val acc: 0.952885  val sensitivity: 0.530504  val specifity: 0.987263  val f1: 0.628931\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  164\n",
            "Epoch: 75 Training Loss: 0.220772  val acc: 0.951088  val sensitivity: 0.392573  val specifity: 0.996546  val f1: 0.547135\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  239\n",
            "Epoch: 76 Training Loss: 0.233345  val acc: 0.953284  val sensitivity: 0.506631  val specifity: 0.989637  val f1: 0.620130\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  189\n",
            "Epoch: 77 Training Loss: 0.210945  val acc: 0.950888  val sensitivity: 0.424403  val specifity: 0.993739  val f1: 0.565371\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  276\n",
            "Epoch: 78 Training Loss: 0.218305  val acc: 0.952286  val sensitivity: 0.549072  val specifity: 0.985104  val f1: 0.633997\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  238\n",
            "Epoch: 79 Training Loss: 0.218432  val acc: 0.949092  val sensitivity: 0.477454  val specifity: 0.987478  val f1: 0.585366\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  253\n",
            "Epoch: 80 Training Loss: 0.217056  val acc: 0.951288  val sensitivity: 0.511936  val specifity: 0.987047  val f1: 0.612698\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  254\n",
            "Epoch: 81 Training Loss: 0.227597  val acc: 0.951088  val sensitivity: 0.511936  val specifity: 0.986831  val f1: 0.611727\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  198\n",
            "Epoch: 82 Training Loss: 0.223594  val acc: 0.949890  val sensitivity: 0.429708  val specifity: 0.992228  val f1: 0.563478\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  202\n",
            "Epoch: 83 Training Loss: 0.210989  val acc: 0.953084  val sensitivity: 0.456233  val specifity: 0.993523  val f1: 0.594128\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  272\n",
            "Epoch: 84 Training Loss: 0.202456  val acc: 0.953484  val sensitivity: 0.551724  val specifity: 0.986183  val f1: 0.640986\n",
            "\tValidation mse decreased (0.258303 --> 0.257010) at epoch: 85. CI:0.866614 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  283\n",
            "Epoch: 85 Training Loss: 0.213432  val acc: 0.952086  val sensitivity: 0.557029  val specifity: 0.984240  val f1: 0.636364\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  187\n",
            "Epoch: 86 Training Loss: 0.224443  val acc: 0.950090  val sensitivity: 0.416446  val specifity: 0.993523  val f1: 0.556738\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  228\n",
            "Epoch: 87 Training Loss: 0.226026  val acc: 0.951487  val sensitivity: 0.480106  val specifity: 0.989853  val f1: 0.598347\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  208\n",
            "Epoch: 88 Training Loss: 0.211821  val acc: 0.951088  val sensitivity: 0.450928  val specifity: 0.991796  val f1: 0.581197\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  234\n",
            "Epoch: 89 Training Loss: 0.202939  val acc: 0.949491  val sensitivity: 0.474801  val specifity: 0.988126  val f1: 0.585925\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  202\n",
            "Epoch: 90 Training Loss: 0.207980  val acc: 0.951088  val sensitivity: 0.442971  val specifity: 0.992444  val f1: 0.576857\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  348\n",
            "Epoch: 91 Training Loss: 0.199899  val acc: 0.952286  val sensitivity: 0.644562  val specifity: 0.977332  val f1: 0.670345\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  264\n",
            "Epoch: 92 Training Loss: 0.219530  val acc: 0.951887  val sensitivity: 0.530504  val specifity: 0.986183  val f1: 0.624025\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  186\n",
            "Epoch: 93 Training Loss: 0.201883  val acc: 0.945897  val sensitivity: 0.387268  val specifity: 0.991364  val f1: 0.518650\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  284\n",
            "Epoch: 94 Training Loss: 0.208777  val acc: 0.946696  val sensitivity: 0.522546  val specifity: 0.981218  val f1: 0.596067\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  292\n",
            "Epoch: 95 Training Loss: 0.216392  val acc: 0.943502  val sensitivity: 0.511936  val specifity: 0.978627  val f1: 0.576981\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  252\n",
            "Epoch: 96 Training Loss: 0.204876  val acc: 0.951487  val sensitivity: 0.511936  val specifity: 0.987263  val f1: 0.613672\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  184\n",
            "Epoch: 97 Training Loss: 0.206653  val acc: 0.947095  val sensitivity: 0.392573  val specifity: 0.992228  val f1: 0.527629\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  216\n",
            "Epoch: 98 Training Loss: 0.216238  val acc: 0.951887  val sensitivity: 0.466844  val specifity: 0.991364  val f1: 0.593592\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  215\n",
            "Epoch: 99 Training Loss: 0.206224  val acc: 0.951687  val sensitivity: 0.464191  val specifity: 0.991364  val f1: 0.591216\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  254\n",
            "Epoch: 100 Training Loss: 0.202874  val acc: 0.951088  val sensitivity: 0.511936  val specifity: 0.986831  val f1: 0.611727\n",
            "------------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------------\n",
            "Test\n",
            "Pre-processed data found: data/processed/davis_test.pt, loading ...\n",
            "Test mse :  0.290038\n",
            "Test ci :  0.8578968495426785\n",
            "Existed numbers of class 1:  410\n",
            "Predicted numbers of class 1:  269\n",
            "Test acc: 0.944311  Test sensitivity: 0.487805  Test specifity: 0.985000  Test f1: 0.589102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT2JZ-W6h9fl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1dc3441b-6393-41ff-90a3-4d4b8540908f"
      },
      "source": [
        "!python regression_test.py 0 0"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "davis\n",
            "Pre-processed data found: data/processed/davis_test.pt, loading ...\n",
            "Test mse :  0.29197463\n",
            "Test ci :  0.8679270466516771\n",
            "Existed numbers of class 1:  410\n",
            "Predicted numbers of class 1:  317\n",
            "Test acc: 0.946707  Test sensitivity: 0.560976  Test specifity: 0.981087  Test f1: 0.632737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-as9BnvUo68E",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "۶\n",
        "</font></h1><p></p>\n",
        "در هیستوگرام کشیده شده مشخص است که تعداد برچسب‌های زیر ۷ بسیار بیشتر است. این در حالت کلاسیفیکیشن با ترشولد ۷ بدین معنا است که یک عدم توازن شدید میان دو کلاس مثبت و منفی در داده آموزش وجود دارد  و با توجه به لاس \n",
        "mse\n",
        "این شرایط باعث می‌شود که مدل وقتی یک کلاس را هم یاد گرفته است باز به \n",
        "loss\n",
        "خوبی برسد . در نتیجه این موضوع مدل بایاس خاصی نسبت به کلاس منفی پیدا می‌کند و در نتیجه معیاری مثل \n",
        "sensitivity\n",
        "افت می‌کند\n",
        "\n",
        "راه‌حل شایعی که برای حل مشکل عدم توازن در دادگان استفاده می‌شود،\n",
        "upsample\n",
        "یا افزایش تعداد نمونه‌های عضو کلاس اقلیت است. یعنی از نمونه‌های موجود به صورت تصادفی نمونه می‌گیریم و به دیتاست اضافه می‌کنیم. به نوعی داریم وزن اهمیت آن‌ها را افزایش می‌دهیم. \n",
        "اینکه تعداد نمونه‌های دسته مثبت را در نهایت چقدر بیشتر کنیم هایپرپارامتر مسئله است که می‌تواند با \n",
        "tuning\n",
        "مقدار بهینه آن بدست آید. در اینجا تعداد را مساوی با تعداد دسته منفی نمی‌کنیم تا تعداد داده‌های تکراری خیلی زیاد نشود و ۰.۹ تعداد دسته منفی را در نظر گرفتیم.\n",
        "دقت می‌کنیم این کار را بعد از تقسیم دیتا به ترین و ولیدیشن انجام می‌دهیم. چرا که در غیر اینصورت داده ولیدیشن ممکن است داده تکراری از داده ترین داشته باشد.\n",
        "\n",
        "نتایج روی آخرین مددل در صدمین \n",
        "epoch:\n",
        "Test acc: 0.943912 \n",
        "\n",
        "Test sensitivity: 0.578049  \n",
        "\n",
        "Test specifity: 0.976522\n",
        "\n",
        "  Test f1: 0.627815\n",
        "\n",
        "  نتایج روی بهترین مدل سیو شده:\n",
        "\n",
        "  Test acc: 0.944311 \n",
        "\n",
        "   Test sensitivity: 0.612195\n",
        "    \n",
        "   Test specifity: 0.973913  \n",
        "\n",
        "   Test f1: 0.642766\n",
        "\n",
        "   همانطور که دیده می‌شود\n",
        "   f1\n",
        "   در این حالت بهبود داشت. با عوض کردن هایپرپارامترهایی مانندد تعداد\n",
        "   upsample\n",
        "   می‌توان انتظار داشت به نتایج بهتری رسید. همچنین مدل سیو شده با معیار بهترین لاس روی ولیدیشن سیو شده است . اگر معیار را \n",
        "   f1\n",
        "   می‌گذاشتیم ،می‌توان انتظار داشت که به نتایج بهتری نیز برسد. \n",
        "\n",
        "   در این حالت هم مدل همچنان روی داده مثبت ضعیف عمل می‌کند، اما نشانه‌هایی از این دارد که با تغییر چند چیز و بیشتر بودن \n",
        "   epoch\n",
        "   می‌توان به حالت‌های بهتری رسید\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR4nkxr6qawb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e6c48a5a-9562-4a51-f86f-988737810a3a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "dataset = 'davis'\n",
        "df = pd.read_csv('data/' + dataset + '_train.csv')\n",
        "Y =  list(df['affinity'])\n",
        "plt.hist(Y)\n",
        "plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acrkOmsYxP_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "605f1965-09e9-4185-ef13-3676b722d445"
      },
      "source": [
        "!python upsample.py 0"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing  davis_balanced_train.pt in pytorch format!\n",
            "Pre-processed data data/processed/davis_balanced_train.pt not found, doing pre-processing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sN8VB_1x0Vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "337257f4-40d7-4cf0-8ca8-19371eb54585"
      },
      "source": [
        "!python regression_training.py 0 1"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "davis\n",
            "Pre-processed data found: data/processed/davis_balanced_train.pt, loading ...\n",
            "Pre-processed data found: data/processed/davis_validation.pt, loading ...\n",
            "\tValidation mse decreased (inf --> 1.375620) at epoch: 1. CI:0.676395 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  754\n",
            "Epoch: 1 Training Loss: 2.597658  val acc: 0.844879  val sensitivity: 0.469496  val specifity: 0.875432  val f1: 0.312997\n",
            "\tValidation mse decreased (1.375620 --> 0.640045) at epoch: 2. CI:0.777408 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  262\n",
            "Epoch: 2 Training Loss: 1.207347  val acc: 0.921142  val sensitivity: 0.323607  val specifity: 0.969775  val f1: 0.381847\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  204\n",
            "Epoch: 3 Training Loss: 1.000577  val acc: 0.931923  val sensitivity: 0.318302  val specifity: 0.981865  val f1: 0.413081\n",
            "\tValidation mse decreased (0.640045 --> 0.563915) at epoch: 4. CI:0.798129 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  190\n",
            "Epoch: 4 Training Loss: 0.860088  val acc: 0.938710  val sensitivity: 0.344828  val specifity: 0.987047  val f1: 0.458554\n",
            "\tValidation mse decreased (0.563915 --> 0.561260) at epoch: 5. CI:0.807652 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  238\n",
            "Epoch: 5 Training Loss: 0.726621  val acc: 0.936714  val sensitivity: 0.395225  val specifity: 0.980786  val f1: 0.484553\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  129\n",
            "Epoch: 6 Training Loss: 0.671781  val acc: 0.940108  val sensitivity: 0.273210  val specifity: 0.994387  val f1: 0.407115\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  113\n",
            "Epoch: 7 Training Loss: 0.597384  val acc: 0.939709  val sensitivity: 0.249337  val specifity: 0.995898  val f1: 0.383673\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  122\n",
            "Epoch: 8 Training Loss: 0.549810  val acc: 0.941505  val sensitivity: 0.273210  val specifity: 0.995898  val f1: 0.412826\n",
            "\tValidation mse decreased (0.561260 --> 0.488097) at epoch: 9. CI:0.805460 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  207\n",
            "Epoch: 9 Training Loss: 0.509271  val acc: 0.944101  val sensitivity: 0.403183  val specifity: 0.988126  val f1: 0.520548\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  201\n",
            "Epoch: 10 Training Loss: 0.485671  val acc: 0.944500  val sensitivity: 0.397878  val specifity: 0.988990  val f1: 0.519031\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  191\n",
            "Epoch: 11 Training Loss: 0.461764  val acc: 0.942104  val sensitivity: 0.368700  val specifity: 0.988774  val f1: 0.489437\n",
            "\tValidation mse decreased (0.488097 --> 0.448362) at epoch: 12. CI:0.827123 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  217\n",
            "Epoch: 12 Training Loss: 0.432659  val acc: 0.946496  val sensitivity: 0.432361  val specifity: 0.988342  val f1: 0.548822\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  196\n",
            "Epoch: 13 Training Loss: 0.411328  val acc: 0.943502  val sensitivity: 0.384615  val specifity: 0.988990  val f1: 0.506108\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  171\n",
            "Epoch: 14 Training Loss: 0.443954  val acc: 0.941705  val sensitivity: 0.339523  val specifity: 0.990717  val f1: 0.467153\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  139\n",
            "Epoch: 15 Training Loss: 0.416468  val acc: 0.943701  val sensitivity: 0.310345  val specifity: 0.995250  val f1: 0.453488\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  249\n",
            "Epoch: 16 Training Loss: 0.417214  val acc: 0.950489  val sensitivity: 0.501326  val specifity: 0.987047  val f1: 0.603834\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  116\n",
            "Epoch: 17 Training Loss: 0.387528  val acc: 0.939110  val sensitivity: 0.249337  val specifity: 0.995250  val f1: 0.381339\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  159\n",
            "Epoch: 18 Training Loss: 0.388829  val acc: 0.943302  val sensitivity: 0.334218  val specifity: 0.992876  val f1: 0.470149\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  194\n",
            "Epoch: 19 Training Loss: 0.378002  val acc: 0.945099  val sensitivity: 0.392573  val specifity: 0.990069  val f1: 0.518389\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  205\n",
            "Epoch: 20 Training Loss: 0.353197  val acc: 0.944899  val sensitivity: 0.405836  val specifity: 0.988774  val f1: 0.525773\n",
            "\tValidation mse decreased (0.448362 --> 0.364044) at epoch: 21. CI:0.820710 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  262\n",
            "Epoch: 21 Training Loss: 0.346780  val acc: 0.946696  val sensitivity: 0.493369  val specifity: 0.983592  val f1: 0.582160\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  185\n",
            "Epoch: 22 Training Loss: 0.344217  val acc: 0.944101  val sensitivity: 0.374005  val specifity: 0.990501  val f1: 0.501779\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  221\n",
            "Epoch: 23 Training Loss: 0.354587  val acc: 0.944101  val sensitivity: 0.421751  val specifity: 0.986615  val f1: 0.531773\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  179\n",
            "Epoch: 24 Training Loss: 0.334640  val acc: 0.944899  val sensitivity: 0.371353  val specifity: 0.991580  val f1: 0.503597\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  219\n",
            "Epoch: 25 Training Loss: 0.339440  val acc: 0.945298  val sensitivity: 0.427056  val specifity: 0.987478  val f1: 0.540268\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  270\n",
            "Epoch: 26 Training Loss: 0.320706  val acc: 0.945498  val sensitivity: 0.496021  val specifity: 0.982081  val f1: 0.578053\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  225\n",
            "Epoch: 27 Training Loss: 0.333722  val acc: 0.946097  val sensitivity: 0.440318  val specifity: 0.987263  val f1: 0.551495\n",
            "\tValidation mse decreased (0.364044 --> 0.337487) at epoch: 28. CI:0.837526 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  327\n",
            "Epoch: 28 Training Loss: 0.319052  val acc: 0.948493  val sensitivity: 0.591512  val specifity: 0.977547  val f1: 0.633523\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  145\n",
            "Epoch: 29 Training Loss: 0.314892  val acc: 0.943302  val sensitivity: 0.315650  val specifity: 0.994387  val f1: 0.455939\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  186\n",
            "Epoch: 30 Training Loss: 0.317920  val acc: 0.943502  val sensitivity: 0.371353  val specifity: 0.990069  val f1: 0.497336\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  242\n",
            "Epoch: 31 Training Loss: 0.309059  val acc: 0.945498  val sensitivity: 0.458886  val specifity: 0.985104  val f1: 0.558966\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  239\n",
            "Epoch: 32 Training Loss: 0.303804  val acc: 0.946896  val sensitivity: 0.464191  val specifity: 0.986183  val f1: 0.568182\n",
            "\tValidation mse decreased (0.337487 --> 0.326008) at epoch: 33. CI:0.834235 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  291\n",
            "Epoch: 33 Training Loss: 0.313790  val acc: 0.947694  val sensitivity: 0.538462  val specifity: 0.981002  val f1: 0.607784\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  191\n",
            "Epoch: 34 Training Loss: 0.309974  val acc: 0.946097  val sensitivity: 0.395225  val specifity: 0.990933  val f1: 0.524648\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  262\n",
            "Epoch: 35 Training Loss: 0.297744  val acc: 0.948293  val sensitivity: 0.503979  val specifity: 0.984456  val f1: 0.594679\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  231\n",
            "Epoch: 36 Training Loss: 0.304525  val acc: 0.946896  val sensitivity: 0.453581  val specifity: 0.987047  val f1: 0.562500\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  216\n",
            "Epoch: 37 Training Loss: 0.296902  val acc: 0.945498  val sensitivity: 0.424403  val specifity: 0.987910  val f1: 0.539629\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  250\n",
            "Epoch: 38 Training Loss: 0.293020  val acc: 0.947894  val sensitivity: 0.485411  val specifity: 0.985535  val f1: 0.583732\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  274\n",
            "Epoch: 39 Training Loss: 0.274974  val acc: 0.944300  val sensitivity: 0.493369  val specifity: 0.981002  val f1: 0.571429\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  223\n",
            "Epoch: 40 Training Loss: 0.292199  val acc: 0.947694  val sensitivity: 0.448276  val specifity: 0.988342  val f1: 0.563333\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  270\n",
            "Epoch: 41 Training Loss: 0.284596  val acc: 0.946696  val sensitivity: 0.503979  val specifity: 0.982729  val f1: 0.587326\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  247\n",
            "Epoch: 42 Training Loss: 0.278745  val acc: 0.949291  val sensitivity: 0.490716  val specifity: 0.986615  val f1: 0.592949\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  208\n",
            "Epoch: 43 Training Loss: 0.277453  val acc: 0.946297  val sensitivity: 0.419098  val specifity: 0.989206  val f1: 0.540171\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  250\n",
            "Epoch: 44 Training Loss: 0.286181  val acc: 0.947894  val sensitivity: 0.485411  val specifity: 0.985535  val f1: 0.583732\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  288\n",
            "Epoch: 45 Training Loss: 0.270720  val acc: 0.947495  val sensitivity: 0.533156  val specifity: 0.981218  val f1: 0.604511\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  195\n",
            "Epoch: 46 Training Loss: 0.268544  val acc: 0.945698  val sensitivity: 0.397878  val specifity: 0.990285  val f1: 0.524476\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  235\n",
            "Epoch: 47 Training Loss: 0.266316  val acc: 0.947694  val sensitivity: 0.464191  val specifity: 0.987047  val f1: 0.571895\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  194\n",
            "Epoch: 48 Training Loss: 0.254284  val acc: 0.945897  val sensitivity: 0.397878  val specifity: 0.990501  val f1: 0.525394\n",
            "\tValidation mse decreased (0.326008 --> 0.323900) at epoch: 49. CI:0.854055 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  269\n",
            "Epoch: 49 Training Loss: 0.259730  val acc: 0.946097  val sensitivity: 0.498674  val specifity: 0.982513  val f1: 0.582043\n",
            "\tValidation mse decreased (0.323900 --> 0.311155) at epoch: 50. CI:0.846584 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  351\n",
            "Epoch: 50 Training Loss: 0.266929  val acc: 0.945698  val sensitivity: 0.604775  val specifity: 0.973446  val f1: 0.626374\n",
            "\tValidation mse decreased (0.311155 --> 0.301652) at epoch: 51. CI:0.853300 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  314\n",
            "Epoch: 51 Training Loss: 0.287652  val acc: 0.949890  val sensitivity: 0.583554  val specifity: 0.979706  val f1: 0.636758\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  241\n",
            "Epoch: 52 Training Loss: 0.260913  val acc: 0.948892  val sensitivity: 0.480106  val specifity: 0.987047  val f1: 0.585761\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  215\n",
            "Epoch: 53 Training Loss: 0.254678  val acc: 0.946896  val sensitivity: 0.432361  val specifity: 0.988774  val f1: 0.550676\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  248\n",
            "Epoch: 54 Training Loss: 0.248547  val acc: 0.948293  val sensitivity: 0.485411  val specifity: 0.985967  val f1: 0.585600\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  270\n",
            "Epoch: 55 Training Loss: 0.249848  val acc: 0.948293  val sensitivity: 0.514589  val specifity: 0.983592  val f1: 0.599691\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  328\n",
            "Epoch: 56 Training Loss: 0.255551  val acc: 0.947095  val sensitivity: 0.583554  val specifity: 0.976684  val f1: 0.624113\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  238\n",
            "Epoch: 57 Training Loss: 0.252858  val acc: 0.947894  val sensitivity: 0.469496  val specifity: 0.986831  val f1: 0.575610\n",
            "\tValidation mse decreased (0.301652 --> 0.295383) at epoch: 58. CI:0.851383 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  356\n",
            "Epoch: 58 Training Loss: 0.247622  val acc: 0.947495  val sensitivity: 0.623342  val specifity: 0.973877  val f1: 0.641201\n",
            "\tValidation mse decreased (0.295383 --> 0.291940) at epoch: 59. CI:0.838355 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  269\n",
            "Epoch: 59 Training Loss: 0.248588  val acc: 0.948493  val sensitivity: 0.514589  val specifity: 0.983808  val f1: 0.600619\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  253\n",
            "Epoch: 60 Training Loss: 0.254904  val acc: 0.947694  val sensitivity: 0.488064  val specifity: 0.985104  val f1: 0.584127\n",
            "\tValidation mse decreased (0.291940 --> 0.289546) at epoch: 61. CI:0.845181 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  303\n",
            "Epoch: 61 Training Loss: 0.235932  val acc: 0.947295  val sensitivity: 0.551724  val specifity: 0.979491  val f1: 0.611765\n",
            "\tValidation mse decreased (0.289546 --> 0.289409) at epoch: 62. CI:0.851139 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  352\n",
            "Epoch: 62 Training Loss: 0.239402  val acc: 0.949491  val sensitivity: 0.631300  val specifity: 0.975389  val f1: 0.652949\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  258\n",
            "Epoch: 63 Training Loss: 0.245151  val acc: 0.949890  val sensitivity: 0.509284  val specifity: 0.985751  val f1: 0.604724\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  291\n",
            "Epoch: 64 Training Loss: 0.239953  val acc: 0.949291  val sensitivity: 0.549072  val specifity: 0.981865  val f1: 0.619760\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  253\n",
            "Epoch: 65 Training Loss: 0.241438  val acc: 0.948493  val sensitivity: 0.493369  val specifity: 0.985535  val f1: 0.590476\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  396\n",
            "Epoch: 66 Training Loss: 0.241132  val acc: 0.945099  val sensitivity: 0.660477  val specifity: 0.968264  val f1: 0.644243\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  291\n",
            "Epoch: 67 Training Loss: 0.256485  val acc: 0.950090  val sensitivity: 0.554377  val specifity: 0.982297  val f1: 0.625749\n",
            "\tValidation mse decreased (0.289409 --> 0.283581) at epoch: 68. CI:0.855477 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  350\n",
            "Epoch: 68 Training Loss: 0.234396  val acc: 0.947495  val sensitivity: 0.615385  val specifity: 0.974525  val f1: 0.638239\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  390\n",
            "Epoch: 69 Training Loss: 0.240603  val acc: 0.948293  val sensitivity: 0.673740  val specifity: 0.970639  val f1: 0.662321\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  332\n",
            "Epoch: 70 Training Loss: 0.252150  val acc: 0.950289  val sensitivity: 0.610080  val specifity: 0.977979  val f1: 0.648801\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  269\n",
            "Epoch: 71 Training Loss: 0.233174  val acc: 0.949691  val sensitivity: 0.522546  val specifity: 0.984456  val f1: 0.609907\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  264\n",
            "Epoch: 72 Training Loss: 0.241043  val acc: 0.947894  val sensitivity: 0.503979  val specifity: 0.984024  val f1: 0.592824\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  256\n",
            "Epoch: 73 Training Loss: 0.226013  val acc: 0.950289  val sensitivity: 0.509284  val specifity: 0.986183  val f1: 0.606635\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  374\n",
            "Epoch: 74 Training Loss: 0.229137  val acc: 0.947894  val sensitivity: 0.649867  val specifity: 0.972150  val f1: 0.652463\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  222\n",
            "Epoch: 75 Training Loss: 0.233870  val acc: 0.947894  val sensitivity: 0.448276  val specifity: 0.988558  val f1: 0.564274\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  270\n",
            "Epoch: 76 Training Loss: 0.231685  val acc: 0.949092  val sensitivity: 0.519894  val specifity: 0.984024  val f1: 0.605873\n",
            "\tValidation mse decreased (0.283581 --> 0.280434) at epoch: 77. CI:0.857226 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  348\n",
            "Epoch: 77 Training Loss: 0.223053  val acc: 0.950689  val sensitivity: 0.633952  val specifity: 0.976468  val f1: 0.659310\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  303\n",
            "Epoch: 78 Training Loss: 0.233778  val acc: 0.947295  val sensitivity: 0.551724  val specifity: 0.979491  val f1: 0.611765\n",
            "\tValidation mse decreased (0.280434 --> 0.279602) at epoch: 79. CI:0.853358 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  376\n",
            "Epoch: 79 Training Loss: 0.230206  val acc: 0.946696  val sensitivity: 0.644562  val specifity: 0.971287  val f1: 0.645418\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  279\n",
            "Epoch: 80 Training Loss: 0.232668  val acc: 0.949291  val sensitivity: 0.533156  val specifity: 0.983161  val f1: 0.612805\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  396\n",
            "Epoch: 81 Training Loss: 0.223124  val acc: 0.945099  val sensitivity: 0.660477  val specifity: 0.968264  val f1: 0.644243\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  283\n",
            "Epoch: 82 Training Loss: 0.227962  val acc: 0.948093  val sensitivity: 0.530504  val specifity: 0.982081  val f1: 0.606061\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  261\n",
            "Epoch: 83 Training Loss: 0.234448  val acc: 0.949691  val sensitivity: 0.511936  val specifity: 0.985320  val f1: 0.605016\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  396\n",
            "Epoch: 84 Training Loss: 0.225993  val acc: 0.947095  val sensitivity: 0.673740  val specifity: 0.969344  val f1: 0.657180\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  293\n",
            "Epoch: 85 Training Loss: 0.239830  val acc: 0.952486  val sensitivity: 0.572944  val specifity: 0.983377  val f1: 0.644776\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  299\n",
            "Epoch: 86 Training Loss: 0.225034  val acc: 0.950090  val sensitivity: 0.564987  val specifity: 0.981434  val f1: 0.630178\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  255\n",
            "Epoch: 87 Training Loss: 0.223541  val acc: 0.950489  val sensitivity: 0.509284  val specifity: 0.986399  val f1: 0.607595\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  290\n",
            "Epoch: 88 Training Loss: 0.218413  val acc: 0.950289  val sensitivity: 0.554377  val specifity: 0.982513  val f1: 0.626687\n",
            "\tValidation mse decreased (0.279602 --> 0.278744) at epoch: 89. CI:0.852716 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  357\n",
            "Epoch: 89 Training Loss: 0.213849  val acc: 0.950489  val sensitivity: 0.644562  val specifity: 0.975389  val f1: 0.662125\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  254\n",
            "Epoch: 90 Training Loss: 0.224853  val acc: 0.949092  val sensitivity: 0.498674  val specifity: 0.985751  val f1: 0.595880\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  292\n",
            "Epoch: 91 Training Loss: 0.208638  val acc: 0.949092  val sensitivity: 0.549072  val specifity: 0.981649  val f1: 0.618834\n",
            "\tValidation mse decreased (0.278744 --> 0.276210) at epoch: 92. CI:0.857901 Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  340\n",
            "Epoch: 92 Training Loss: 0.222547  val acc: 0.948692  val sensitivity: 0.610080  val specifity: 0.976252  val f1: 0.641562\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  361\n",
            "Epoch: 93 Training Loss: 0.226601  val acc: 0.947694  val sensitivity: 0.631300  val specifity: 0.973446  val f1: 0.644986\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  274\n",
            "Epoch: 94 Training Loss: 0.209434  val acc: 0.948293  val sensitivity: 0.519894  val specifity: 0.983161  val f1: 0.602151\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  282\n",
            "Epoch: 95 Training Loss: 0.209261  val acc: 0.949092  val sensitivity: 0.535809  val specifity: 0.982729  val f1: 0.613050\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  306\n",
            "Epoch: 96 Training Loss: 0.212599  val acc: 0.949092  val sensitivity: 0.567639  val specifity: 0.980138  val f1: 0.626647\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  246\n",
            "Epoch: 97 Training Loss: 0.220163  val acc: 0.941106  val sensitivity: 0.435013  val specifity: 0.982297  val f1: 0.526485\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  305\n",
            "Epoch: 98 Training Loss: 0.219613  val acc: 0.950090  val sensitivity: 0.572944  val specifity: 0.980786  val f1: 0.633431\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  278\n",
            "Epoch: 99 Training Loss: 0.212683  val acc: 0.949890  val sensitivity: 0.535809  val specifity: 0.983592  val f1: 0.616794\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  330\n",
            "Epoch: 100 Training Loss: 0.204195  val acc: 0.947095  val sensitivity: 0.586207  val specifity: 0.976468  val f1: 0.625177\n",
            "------------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------------\n",
            "------------------------------------------------------------------------------------------------\n",
            "Test\n",
            "Pre-processed data found: data/processed/davis_test.pt, loading ...\n",
            "Test mse :  0.31156465\n",
            "Test ci :  0.8546688510319349\n",
            "Existed numbers of class 1:  410\n",
            "Predicted numbers of class 1:  345\n",
            "Test acc: 0.943912  Test sensitivity: 0.578049  Test specifity: 0.976522  Test f1: 0.627815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWKofh8jCo4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a9f781d5-d673-48e2-cbcc-f10cc2c371fb"
      },
      "source": [
        "!python regression_test.py 0 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "davis\n",
            "Pre-processed data found: data/processed/davis_test.pt, loading ...\n",
            "Test mse :  0.31207755\n",
            "Test ci :  0.857008793872301\n",
            "Existed numbers of class 1:  410\n",
            "Predicted numbers of class 1:  371\n",
            "Test acc: 0.944311  Test sensitivity: 0.612195  Test specifity: 0.973913  Test f1: 0.642766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8UbIHO_zVx5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "۷و ۸\n",
        "</font></h1><p></p>\n",
        "مدل \n",
        "upsample\n",
        "شده را انتخاب می‌کنیم.\n",
        "\n",
        "ابتدا باید در نظر گرفت که در روش‌های گفته شده، گرادیان از کلاس درست به عقب می‌رود. حال با توجه به آنکه مدلی که یاد گرفته بودیم یک مدل رگرشن بود، ابتدا آن مدل را \n",
        "freeze \n",
        "کردیم. سپس نوورن آخر را از مدل حذف کردیم و یک لایه جدید با دو نورون اضافه کردیم که\n",
        "logit\n",
        "برای هر کلاس یاد بگیرد و سپس این مدل را روی داده آموزش\n",
        "upsample\n",
        "آموزش دادیم که تنها وزن‌های لایه آخر یاد گرفته شوند و سپس بتوانیم تفسیرپذیری را انجام دهیم. با معیار \n",
        "Cross Entropy\n",
        "شبکه را آموزش دادیم و چون این معیار می‌تواند برای هر دسته یک وزن اختصاص دهد، برای دسته مثبت وزن بیشتری قائل شدیم. \n",
        "\n",
        "حال به توضیح روش‌ها می‌پردازیم:\n",
        "\n",
        "Saliency Maps\n",
        ": \n",
        "در این روش می‌خواهیم ببینیم اگر یک ورودی را فیکس کنیم و همچنین دسته مربوط به آن ورودی را هم فیکس کنیم، شبکه به کدام یک از ویژگی‌های ورودی توجه بیشتر نشان داده است. در مقاله گفته شده است که در ابتدا یک حالت معمولی مثل پرسپترون را در نظر بگیرید. در این حالت واضح است فیچرهایی از ورودی بیشتر مورد توجه هستند که وز‌ن‌های متناظر با آن‌ها بیشتر باشد. عبارت ریاضی در این حالت بدین صورت است:\n",
        "\\begin{equation*}\n",
        "Out_c(Input) = w^TInput\n",
        "\\end{equation*}\n",
        "اما در یک شبکه عصبی، رابطه ورودی به خروجی یک تابع به شدت پیچیده و غیرخطی است و بدین سادگی نمی‌توان استدلال کرد. اما از حالت ساده می‌توان ایده گرفت . بدین صورت که تابع خروجی را در نقطه \n",
        "I_0\n",
        "با سری تیلور تقریب زد. یعنی در حالت تقریبی از شبکه عصبی می‌توان نوشت:\n",
        "\\begin{equation*}\n",
        "Out_c(Input) = \\frac{\\partial{Out_c}}{\\partial Input}|_{I_0} Input\n",
        "\\end{equation*}\n",
        "که عبارت مشتق، همان عبارتی است که در \n",
        "backprop\n",
        "بدست می‌آوریم. پس تنها کافی است مشتق را نسبت به ورودی در \n",
        "backprop \n",
        "بدست آوریم.\n",
        "\n",
        "Guided Back Propagation:\n",
        "یکی از روش‌هایی که سابق بر این مقاله برای نمایش دادن فیچرهای مهم ورودی استفاده می‌شده است، استفاده از لایه‌های \n",
        "deconvolution\n",
        "در مدل بوده است. به طور خلاصه\n",
        "deconvolution\n",
        "سعی در \n",
        "invert\n",
        "کردن \n",
        "convolution\n",
        "دارد . این روش بدون استفاده از \n",
        "max pooling\n",
        "پایدار نبوده است.\n",
        "در این مقاله سعی شده است که از ترکیب ایده‌ موجود در \n",
        "saliency\n",
        "و\n",
        "deconv\n",
        "استفاده کند. بدین صورت که در برخورد با ماژول‌های غیر از تابع اکتیویشن همانند حالت قبل عمل می‌کند اما در برخورد با \n",
        "relu\n",
        "از ترکیبی از قانون \n",
        "backprop\n",
        "موجود در \n",
        "deconv\n",
        "و\n",
        "حالت عادی مشتق از \n",
        "relu\n",
        "استفاده می‌کند.\n",
        "یعنی هم جاهایی که \n",
        "backprop\n",
        "صفر می‌گذارد را صفر می‌کند و هم بخشی که \n",
        "devonv\n",
        "صفر می‌کند را صفر می‌گذارد.\n",
        ". \n",
        "شکلی در مقاله وجود دارد که این موضوع را گرافیکی نشان می‌دهد.\n",
        "<a href=\"https://drive.google.com/file/d/1I4kARmUygpK-ijlPZ_odBHyYAYOMfz2m/view?usp=sharing\">\n",
        "اینجا\n",
        "</a>\n",
        "همچنین رابطه آپدیت مشتق در این حالت در همین شکل آمده است. این کار به صورت عملی دیده شده است که \n",
        "stable\n",
        "تر است.\n",
        "\n",
        "LRP :\n",
        "روش‌های قبلی کاملا براساس گرادیان بودند و در روش\n",
        "guided backprop\n",
        "دیده شده است که تغییر جزئی در این روند \n",
        "backprop\n",
        "به بهبود می‌انجامد. در واقع مشکل این است که گرادیان می‌تواند به شدت نویزی باشد و در نتیجه نتایج همواره قابل اعتنا نباشد. \n",
        "در نتیجه ایده در اینجا این است که به جای گرادیان، یک امتیازی را به سمت عقب منتشر کنیم که آن امتیاز را در فرآیند انتشار به عقب با یک سری \n",
        "constrait\n",
        "ای که می‌گذاریم به عقب برانیم که مشکلات گرادیان را حل کند. برای این کار از ایده‌ای مشابه با ایده قانون کیرشهف در مدارهای الکتریکی استفاده کرده‌اند. بدین صورت که امتیازی که به یک نورون وارد می‌شود، باید با مجموع امتیازهای خروجی از آن نورون به نورن‌های لایه قبل برابر باشد. یعنی داشته باشیم:\n",
        "\\begin{equation*}\n",
        "f(x) = .... = \\sum_{d \\in l+1} R_d^{l+1} = \\sum_{d \\in l} R_d^{l} = .... = \\sum_{d } R_d^{1}\n",
        "\\end{equation*}\n",
        "که\n",
        "R\n",
        "همان امتیازی است که قرار است به عقب باز نشر شود.\n",
        " کار باعث می‌شود که امتیازی که به عقب نشر می‌شود پایدارتر از گرادیان باشد. حال پیدا کردن رابطه‌ای که در این \n",
        "constraint\n",
        "صدق بکند و در عین حال به خوبی بتواند فیچرهای مناسب را پیدا کند می‌تواند چالش‌برانگیز باشد.  رابطه‌ای که در نهایت مقاله به عنوان رابطه مناسب یاد می‌کند عبارت است از:\n",
        "\\begin{equation*}\n",
        "R_j = \\sum_k \\frac{a_j w_{jk}}{\\sum_{0,j}a_j w_{jk}}R_k\n",
        "\\end{equation*}\n",
        "که \n",
        "a\n",
        "اکتیویشن\n",
        "و\n",
        "w\n",
        "بردار وزن میاد دو نورون \n",
        "j\n",
        "و\n",
        "k\n",
        "می‌باشد که در دو لایه متوالی قرار دارند. همانطور که می‌بینید ضریب به صورت نرمالایز نوشته شده است که در رابطه قانون کیرشهف برقرار شود. با اعمال این رابطه از آخر به اول می‌توان فیچرهای مورد نظر را بدست آورد.\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXGUnQ-B8_IV",
        "colab_type": "text"
      },
      "source": [
        "به علت کمبود وقت مدل را روی ۵۰ ایپاک ترین کردیم. این مدل قرار است تنها وزن‌های لایه آخر مربوط به دسته‌بندی را یاد بگیرد."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DzO4fOzwoeF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce2d5cea-d66b-4fba-970f-2b2dff5f7bdb"
      },
      "source": [
        "!python classify_interpret.py 0"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "davis\n",
            "Pre-processed data found: data/processed/davis_balanced_train.pt, loading ...\n",
            "Pre-processed data found: data/processed/davis_validation.pt, loading ...\n",
            "\tValidation f1 increased (0.000000 --> 0.350211) at epoch: 1. Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  97\n",
            "Epoch: 1 Training Loss: 1.152047  val acc: 0.938511  val sensitivity: 0.220159  val specifity: 0.996978  val f1: 0.350211\n",
            "\tValidation f1 increased (0.350211 --> 0.505338) at epoch: 2. Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  185\n",
            "Epoch: 2 Training Loss: 0.637019  val acc: 0.944500  val sensitivity: 0.376658  val specifity: 0.990717  val f1: 0.505338\n",
            "\tValidation f1 increased (0.505338 --> 0.523397) at epoch: 3. Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  200\n",
            "Epoch: 3 Training Loss: 0.491411  val acc: 0.945099  val sensitivity: 0.400531  val specifity: 0.989421  val f1: 0.523397\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  127\n",
            "Epoch: 4 Training Loss: 0.420217  val acc: 0.941705  val sensitivity: 0.281167  val specifity: 0.995466  val f1: 0.420635\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  113\n",
            "Epoch: 5 Training Loss: 0.367895  val acc: 0.939309  val sensitivity: 0.246684  val specifity: 0.995682  val f1: 0.379592\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  107\n",
            "Epoch: 6 Training Loss: 0.333692  val acc: 0.938910  val sensitivity: 0.236074  val specifity: 0.996114  val f1: 0.367769\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  78\n",
            "Epoch: 7 Training Loss: 0.322293  val acc: 0.935516  val sensitivity: 0.175066  val specifity: 0.997409  val f1: 0.290110\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  156\n",
            "Epoch: 8 Training Loss: 0.317655  val acc: 0.945099  val sensitivity: 0.342175  val specifity: 0.994171  val f1: 0.484053\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  136\n",
            "Epoch: 9 Training Loss: 0.312700  val acc: 0.942304  val sensitivity: 0.297082  val specifity: 0.994819  val f1: 0.436647\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  116\n",
            "Epoch: 10 Training Loss: 0.308597  val acc: 0.939509  val sensitivity: 0.251989  val specifity: 0.995466  val f1: 0.385396\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  133\n",
            "Epoch: 11 Training Loss: 0.304608  val acc: 0.941705  val sensitivity: 0.289125  val specifity: 0.994819  val f1: 0.427451\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  125\n",
            "Epoch: 12 Training Loss: 0.304660  val acc: 0.940108  val sensitivity: 0.267905  val specifity: 0.994819  val f1: 0.402390\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  176\n",
            "Epoch: 13 Training Loss: 0.309630  val acc: 0.946297  val sensitivity: 0.376658  val specifity: 0.992660  val f1: 0.513562\n",
            "\tValidation f1 increased (0.523397 --> 0.525939) at epoch: 14. Saving model ...\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  182\n",
            "Epoch: 14 Training Loss: 0.304253  val acc: 0.947095  val sensitivity: 0.389920  val specifity: 0.992444  val f1: 0.525939\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  89\n",
            "Epoch: 15 Training Loss: 0.307369  val acc: 0.936514  val sensitivity: 0.196286  val specifity: 0.996762  val f1: 0.317597\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  172\n",
            "Epoch: 16 Training Loss: 0.308293  val acc: 0.945498  val sensitivity: 0.366048  val specifity: 0.992660  val f1: 0.502732\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  61\n",
            "Epoch: 17 Training Loss: 0.320384  val acc: 0.933719  val sensitivity: 0.140584  val specifity: 0.998273  val f1: 0.242009\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  120\n",
            "Epoch: 18 Training Loss: 0.307624  val acc: 0.939908  val sensitivity: 0.259947  val specifity: 0.995250  val f1: 0.394366\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  114\n",
            "Epoch: 19 Training Loss: 0.308145  val acc: 0.939110  val sensitivity: 0.246684  val specifity: 0.995466  val f1: 0.378819\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  122\n",
            "Epoch: 20 Training Loss: 0.306540  val acc: 0.940307  val sensitivity: 0.265252  val specifity: 0.995250  val f1: 0.400802\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  102\n",
            "Epoch: 21 Training Loss: 0.306438  val acc: 0.937912  val sensitivity: 0.222812  val specifity: 0.996114  val f1: 0.350731\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  129\n",
            "Epoch: 22 Training Loss: 0.309265  val acc: 0.940906  val sensitivity: 0.278515  val specifity: 0.994819  val f1: 0.415020\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  132\n",
            "Epoch: 23 Training Loss: 0.303493  val acc: 0.941505  val sensitivity: 0.286472  val specifity: 0.994819  val f1: 0.424361\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  108\n",
            "Epoch: 24 Training Loss: 0.305833  val acc: 0.938311  val sensitivity: 0.233422  val specifity: 0.995682  val f1: 0.362887\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  132\n",
            "Epoch: 25 Training Loss: 0.303897  val acc: 0.941505  val sensitivity: 0.286472  val specifity: 0.994819  val f1: 0.424361\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  187\n",
            "Epoch: 26 Training Loss: 0.303896  val acc: 0.946496  val sensitivity: 0.392573  val specifity: 0.991580  val f1: 0.524823\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  139\n",
            "Epoch: 27 Training Loss: 0.306775  val acc: 0.942503  val sensitivity: 0.302387  val specifity: 0.994603  val f1: 0.441860\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  154\n",
            "Epoch: 28 Training Loss: 0.301868  val acc: 0.943502  val sensitivity: 0.328912  val specifity: 0.993523  val f1: 0.467043\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  141\n",
            "Epoch: 29 Training Loss: 0.304470  val acc: 0.942903  val sensitivity: 0.307692  val specifity: 0.994603  val f1: 0.447876\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  151\n",
            "Epoch: 30 Training Loss: 0.302769  val acc: 0.943701  val sensitivity: 0.326260  val specifity: 0.993955  val f1: 0.465909\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  126\n",
            "Epoch: 31 Training Loss: 0.305266  val acc: 0.940307  val sensitivity: 0.270557  val specifity: 0.994819  val f1: 0.405567\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  123\n",
            "Epoch: 32 Training Loss: 0.304984  val acc: 0.940108  val sensitivity: 0.265252  val specifity: 0.995035  val f1: 0.400000\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  73\n",
            "Epoch: 33 Training Loss: 0.303272  val acc: 0.934917  val sensitivity: 0.164456  val specifity: 0.997625  val f1: 0.275556\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  125\n",
            "Epoch: 34 Training Loss: 0.300965  val acc: 0.940906  val sensitivity: 0.273210  val specifity: 0.995250  val f1: 0.410359\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  153\n",
            "Epoch: 35 Training Loss: 0.305323  val acc: 0.944101  val sensitivity: 0.331565  val specifity: 0.993955  val f1: 0.471698\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  100\n",
            "Epoch: 36 Training Loss: 0.303552  val acc: 0.937912  val sensitivity: 0.220159  val specifity: 0.996330  val f1: 0.348008\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  66\n",
            "Epoch: 37 Training Loss: 0.304438  val acc: 0.934318  val sensitivity: 0.151194  val specifity: 0.998057  val f1: 0.257336\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  105\n",
            "Epoch: 38 Training Loss: 0.301037  val acc: 0.938111  val sensitivity: 0.228117  val specifity: 0.995898  val f1: 0.356846\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  112\n",
            "Epoch: 39 Training Loss: 0.303623  val acc: 0.939110  val sensitivity: 0.244032  val specifity: 0.995682  val f1: 0.376278\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  176\n",
            "Epoch: 40 Training Loss: 0.303812  val acc: 0.946696  val sensitivity: 0.379310  val specifity: 0.992876  val f1: 0.517179\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  82\n",
            "Epoch: 41 Training Loss: 0.304338  val acc: 0.935915  val sensitivity: 0.183024  val specifity: 0.997193  val f1: 0.300654\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  127\n",
            "Epoch: 42 Training Loss: 0.305808  val acc: 0.940507  val sensitivity: 0.273210  val specifity: 0.994819  val f1: 0.408730\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  119\n",
            "Epoch: 43 Training Loss: 0.301781  val acc: 0.940108  val sensitivity: 0.259947  val specifity: 0.995466  val f1: 0.395161\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  129\n",
            "Epoch: 44 Training Loss: 0.304277  val acc: 0.940906  val sensitivity: 0.278515  val specifity: 0.994819  val f1: 0.415020\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  138\n",
            "Epoch: 45 Training Loss: 0.301970  val acc: 0.942304  val sensitivity: 0.299735  val specifity: 0.994603  val f1: 0.438835\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  97\n",
            "Epoch: 46 Training Loss: 0.296964  val acc: 0.937313  val sensitivity: 0.212202  val specifity: 0.996330  val f1: 0.337553\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  108\n",
            "Epoch: 47 Training Loss: 0.298415  val acc: 0.938710  val sensitivity: 0.236074  val specifity: 0.995898  val f1: 0.367010\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  60\n",
            "Epoch: 48 Training Loss: 0.297700  val acc: 0.933520  val sensitivity: 0.137931  val specifity: 0.998273  val f1: 0.237986\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  76\n",
            "Epoch: 49 Training Loss: 0.299818  val acc: 0.935516  val sensitivity: 0.172414  val specifity: 0.997625  val f1: 0.286976\n",
            "Existed numbers of class 1:  377\n",
            "Predicted numbers of class 1:  147\n",
            "Epoch: 50 Training Loss: 0.303538  val acc: 0.943701  val sensitivity: 0.320955  val specifity: 0.994387  val f1: 0.461832\n",
            "Pre-processed data found: data/processed/davis_test.pt, loading ...\n",
            "Existed numbers of class 1:  410\n",
            "Predicted numbers of class 1:  207\n",
            "Test acc: 0.937525  Test sensitivity: 0.370732  Test specifity: 0.988043  Test f1: 0.492707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsvvcqwzHZNj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=4>\n",
        "<h1><font  id=\"section3\" color=#FF7500 size=6>\n",
        "۹\n",
        "</font></h1><p></p>\n",
        "دو روش \n",
        "saliency\n",
        "و\n",
        "guided backprop\n",
        "را با کمک از\n",
        "<a href=\"https://github.com/osu-xai/pytorch-saliency/tree/master/saliency\">\n",
        "اینجا\n",
        "</a>\n",
        "پیاده‌سازی کردیم. گرادیان را در این حالت خاص مدل ما، نسبت به امبدینگ حاصل از پروتئین می‌توان بدست آورد چرا که در پای‌تورچ رد شدن گرادیان از امبدینگ ممکن نیست. همچنین گرادیان نسبت به دارو ، نسبت به فیچرهایی بدست آمد که به نودهای گراف داده بودیم و آن‌ها را در \n",
        "preprocess\n",
        "استخراج کردیم. حاصل آن‌ها یک بردار ۷۸ بعدی برای هر نود شده است.\n",
        "\n",
        "پس یعنی به ازای هر ورودی پروتئین یک بردار گرادیان ۱۲۸ \n",
        "بعدی به ازای هر حرف آن داریم. و برای دارو یک بردار ۷۸ \n",
        "بعدی به ازای هر نود موجود در گراف داریم. حال نیاز داریم که این بردارها را به نوعی\n",
        "aggregate\n",
        "کنیم تا یک عدد حاصل شود. برای این منظور همان‌طور که ددر پیاتزا گفته شده بودد از \n",
        "max\n",
        "استفاده کردیم.\n",
        "گزینه‌هایی مثل \n",
        "sum\n",
        "و\n",
        "avg\n",
        "هم می‌توانستند امتحان شوند و نتایج مقایسه شود. \n",
        "\n",
        "حال بعد از این کار به ازای هر حرف یا نود یک عدد داریم. حال نیاز به معیاری داشتیم که بهترین فیچرها را انتخاب کنیم. ساده‌ترین گزینه این است که همواره \n",
        "n\n",
        "عدد ماکس را انتخاب کنیم که اصلا کار خوبی نیست. به همین منظور در ابتدا در نظر گرفه شد که آن‌هایی که از میانگین بیشتر هستند را انتخاب کنیم که دیده شدد این کار خیلی کار خوبی نیست چون می‌تواند تعداد بسیار بالایی را شامل شود. در نتیجه در نظر گرفته شد که از میانگین به اندازه یک یا دو انحراف معیار فاصله بگیریم و فیچرهای بزرگتر از آن را انتخاب کنیم. یک انحراف معیار به نظر گزینه مناسبی برای این منظور آمد\n",
        ".\n",
        "با پیدا کردن فیچرهای مهم، مقادیر آن‌ها را عوض کردیم و صفر گذاشتیم . یکبار برای دارو و یکبار برای پروتئین. نتایج نشان می‌دهد که روش \n",
        "guided backprop\n",
        "بسیار بهتر عمل کرده است چرا که با عوض کردن فیچرهایی که او گفته است، تعداد بسیار بیشتری کلاسشان عوض شده است.\n",
        "تعداد کل پیش‌بینی درست یک از میان\n",
        "۴۱۰ \n",
        "یک موجود۱۵۲ بوده است. حال در \n",
        "saliency\n",
        "داریم:\n",
        "\n",
        "Number of changed by manipulating protein :  1  drug:  113\n",
        "\n",
        "در \n",
        "guided backprop\n",
        "داریم:\n",
        "Number of changed by manipulating protein :  3  drug:  132\n",
        "\n",
        "که نشان می‌دهد \n",
        "guided backprop\n",
        "بسیار بهتر عمل کرده است.\n",
        "یک نکته  مهمی که وجود دارد این است که عوض کردن پروتئین‌ها تقریبا هیچ تاثیری در عوض کردن کلاس نداشته است که این در صورت درست بودن نتیجه خیلی مهمی درباره مدل به ما می‌دهد که در پیش‌بینی، پروتئين‌ها خیلی تاثیرگذار نبوده‌اند . ددر نتیجه با این نتیجه می‌توان رفت و مدل را تغییر داد و بهبود بخشید.\n",
        "\n",
        "نمودار توزیع تعداد حروف انتخابی هم کشیده شده است که عکس آن به صورت فایل قرار دادده شدده است.\n",
        "\n",
        "\n",
        "یک نکته که باید رعایت شود این است که هنگام \n",
        "تفسیر باید در \n",
        "model.py\n",
        "خط \n",
        "embedded_xt.requires_grad = True\n",
        "در فروارد اجرا شود اما در یادگیری معمولی مدل، چون این یک \n",
        "leaf\n",
        "نیست ، حتما باید کامنت شود.\n",
        "\n",
        "نتایج\n",
        "</font><p></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxBQWszr0Pf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "df6ed034-95cc-4dea-cf08-77e390e8934f"
      },
      "source": [
        "!python interpretibility.py"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "Pre-processed data found: data/processed/davis_test.pt, loading ...\n",
            "True class numbers:  410\n",
            "True Positive numbers:  152\n",
            "Mehtod:  SaliencyMap\n",
            "Number of changed by manipulating protein :  1  drug:  113\n",
            "Dist of target: [5, 5, 5, 5, 6, 3, 4, 6, 6, 4, 6, 5, 5, 6, 4, 4, 6, 5, 4, 5, 5, 6, 6, 5, 5, 4, 5, 6, 6, 6, 7, 5, 5, 5, 5, 6, 7, 5, 6, 6, 5, 6, 5, 6, 4, 6, 8, 5, 4, 7, 6, 6, 6, 6, 7, 5, 5, 6, 5, 5, 6, 5, 7, 5, 5, 5, 7, 6, 5, 4, 6, 4, 4, 4, 5, 6, 7, 5, 4, 6, 8, 7, 6, 6, 6, 5, 5, 7, 5, 5, 5, 4, 4, 5, 6, 8, 7, 6, 5, 6, 3, 2, 5, 5, 4, 5, 8, 4, 7, 7, 3, 5, 6, 7, 4, 5, 5, 5, 4, 5, 4, 6, 7, 7, 4, 6, 5, 6, 5, 5, 4, 4, 5, 6, 3, 4, 5, 5, 5, 6, 5, 6, 6, 4, 6, 7, 5, 5, 5, 6, 6, 5]\n",
            "Dist of drug: [6, 6, 9, 5, 4, 6, 5, 6, 6, 4, 6, 9, 6, 6, 6, 4, 5, 6, 3, 9, 6, 4, 5, 7, 5, 9, 3, 6, 8, 7, 4, 6, 5, 6, 6, 6, 4, 5, 5, 5, 5, 8, 6, 4, 4, 7, 9, 6, 8, 8, 5, 5, 7, 5, 8, 8, 5, 8, 6, 6, 7, 2, 8, 5, 4, 6, 5, 3, 5, 6, 4, 5, 4, 5, 6, 5, 5, 6, 4, 6, 7, 5, 5, 7, 4, 2, 7, 6, 6, 5, 5, 4, 3, 2, 8, 7, 8, 6, 6, 7, 6, 6, 6, 5, 9, 6, 9, 5, 7, 5, 6, 5, 5, 7, 7, 6, 8, 7, 5, 5, 7, 8, 5, 4, 7, 4, 5, 5, 4, 8, 5, 8, 5, 4, 7, 7, 6, 4, 6, 6, 5, 7, 4, 6, 6, 4, 5, 6, 4, 8, 8, 5]\n",
            "Mehtod:  GuidedBackProp\n",
            "Number of changed by manipulating protein :  3  drug:  132\n",
            "Dist of target: [5, 5, 6, 4, 4, 5, 4, 5, 4, 6, 5, 4, 6, 5, 5, 4, 6, 4, 7, 6, 6, 5, 6, 5, 6, 6, 4, 6, 7, 6, 6, 5, 3, 6, 5, 7, 6, 5, 4, 3, 5, 7, 6, 4, 7, 6, 5, 5, 4, 4, 7, 7, 4, 5, 5, 7, 5, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 4, 6, 4, 4, 6, 6, 5, 5, 6, 5, 6, 6, 5, 6, 7, 6, 5, 6, 4, 4, 6, 5, 5, 6, 7, 6, 7, 5, 5, 6, 6, 6, 6, 7, 6, 6, 7, 5, 5, 6, 6, 4, 6, 5, 6, 6, 4, 6, 6, 5, 6, 5, 6, 6, 5, 6, 7, 5, 6, 5, 5, 7, 5, 5, 7, 5, 5, 6, 5, 5, 4, 6, 4, 6, 4, 5, 6, 6, 4, 7, 5, 7, 6, 4]\n",
            "Dist of drug: [7, 5, 6, 4, 5, 5, 5, 5, 7, 5, 5, 5, 6, 5, 4, 5, 3, 6, 4, 6, 5, 5, 10, 5, 5, 7, 5, 5, 7, 8, 6, 5, 3, 7, 5, 5, 5, 5, 5, 3, 5, 7, 6, 5, 10, 10, 6, 5, 7, 5, 5, 5, 5, 5, 6, 4, 7, 6, 6, 5, 7, 3, 9, 5, 3, 5, 5, 6, 5, 9, 5, 4, 4, 5, 5, 6, 6, 9, 7, 10, 7, 5, 5, 7, 3, 3, 4, 5, 5, 5, 5, 5, 4, 3, 7, 4, 9, 10, 5, 8, 9, 5, 5, 6, 10, 4, 6, 3, 9, 5, 9, 5, 6, 4, 7, 7, 4, 6, 8, 5, 7, 6, 5, 5, 5, 7, 7, 6, 4, 4, 5, 6, 5, 3, 8, 7, 5, 6, 7, 6, 3, 7, 5, 6, 6, 5, 6, 6, 7, 7, 6, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StMA_GOLVycR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}